{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e607ca26",
   "metadata": {},
   "source": [
    "<img src='images/gesis.png' style='height: 50px; float: left'>\n",
    "<img src='images/social_comquant.png' style='height: 50px; float: left; margin-left: 40px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70bc56f",
   "metadata": {},
   "source": [
    "## Introduction to Computational Social Science methods with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71218328",
   "metadata": {},
   "source": [
    "# Session 2: Data handling and visualization\n",
    "\n",
    "**Data** has two components: content and structure. Plain text data is unstructured, but its content can also be represented in a structured way. Data representations reside in a continuum of structuration. The rectangular table (also called dataframe or spreadsheet) is the most frequent data format in the social sciences because it is perfectly suited to manage the highly structured (survey) data that social scientists have typically worked on. Hierarchical data formats like JSON and HTML are semi-structured, and text data is unstructured (<a href='#destination5'>Weidmann 2022</a>, ch. 3). The practice of Computational Social Science primarly revolves around **Digital Behavioral Data**, the traces of behavior left by uses of or harnessed by digital technology. The so-called data life cycle consists of three steps: data collection, processing, and analysis. In this process, data changes its face from a raw state to a state in which it is ready for analysis. **Data processing** subsumes the steps in which this transformation takes place (<a href='#destination5'>Weidmann 2022</a>, ch. 1).\n",
    "\n",
    "**Data management** refers to the practices by which we stay in control of data as a resource. Data is best managed with a focus on practical questions. Since data processing is about bringing data into a form that permits answering those questions, it is strategically advantageous also to focus data management on data processing. Computational data processing workflows are beneficial because they fully document the many steps from data collection to data analysis, they are convenient (like your favorite spreadsheet software could never be), they are replicable (nowadays in high demand by scholarly journals), they can be scaled up (necessary for [big data](https://en.wikipedia.org/wiki/Big_data)), and they offer the needed flexibility in the face of semi-structured or unstructured data sources (<a href='#destination5'>Weidmann 2022</a>, p. 7–9).\n",
    "\n",
    "The **R** language and software environment for statistical computing and graphics is very popular in the social sciences, also because it provides the [Tidyverse](https://www.tidyverse.org/), a collection of mutually adapted packages for tabular data structures, their manipulation (*e.g.*, merging, aggregating), and producing appealing graphics (<a href='#destination5'>Weidmann 2022</a>, ch. 7). We argue that **Python** does not need to hide behind R in this regard. The [Pandas](https://pandas.pydata.org/) library for managing tables truly is a \"fast, powerful, flexible and easy to use open source data analysis and manipulation tool\" that, when combined with the [Seaborn](https://seaborn.pydata.org/) statistical data visualization library, leaves nothing to be desired.\n",
    "\n",
    "Pandas can also be used in a way that mimics the functionality of **relational databases**. These are systems where the columns of a table are split into multiple tables such that redundancies are eliminated. Relational databases are often used in research when the data is either large in volume or rich in content because they ensure consistency and speed up data processing (<a href='#destination5'>Weidmann 2022</a>, part 3). The public [TweetsKB](https://data.gesis.org/tweetskb/) corpus of annotated tweets (<a href='#destination4'>Fafalios *et al.*, 2018</a>), as well as its offspring, the [TweetsCOV19](https://data.gesis.org/tweetscov19/) corpus (<a href='#destination3'>Dimitrov *et al.*, 2020</a>), are examples where the data is explicitly modeled relationally and can serve as illustrations of meaningful data management.\n",
    "\n",
    "<div class='alert alert-block alert-success'>\n",
    "<b>In this session</b>, \n",
    "\n",
    "you will learn how to manage your data, keep it tidy, and visualize it while keeping a focus on your research questions. In subsession **2.1**, we will have a deep look at the 2-dimensional table as the fundamental data structure we will work with throughout all sessions. You will experience how you can use the Pandas library to handle tables and mimic a relational database in such a way that your data gets ready for analysis. You will see what it means that relational databases eliminate redundancy and ensure consistency. The TweetsCOV19 dataset will function as an example that will shine up repeatedly in this and subsequent sessions. In subsession **2.2**, we will introduce the NumPy and SciPy libraries. NumPy allows working with n-dimensional tables called arrays, which are typically needed in the data processing. SciPy enables you to efficiently process and analyze huge matrices (*i.e.*, 2-dimensional numerical tables) with many zeros or missing values (which is often the case). Finally, in subsession **2.3**, you will learn how to use the Matplotlib and Seaborn libraries to explore data visually.\n",
    "</div>\n",
    "\n",
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Caution</b>\n",
    "\n",
    "This Jupyter Notebook demonstrates a workflow that consists of a **sequence of processing steps**. In this process, tables are created, changed, and deleted. Hence, the notebook must be executed from top to bottom. Going back up from a certain code cell and trying to execute a cell that precedes it may not work.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc363a2",
   "metadata": {},
   "source": [
    "## 2.1. Managing data with Pandas\n",
    "\n",
    "<img src='images/pandas.png' style='height: 100px; float: right; margin-left: 10px'>\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/) is Python's package for data management and processing using 2-dimensional tables. It allows you to work with any kind of observational or statistical data set, including matrices. Column entries can be heterogeneous (*i.e.*, a single column can contain text, numerical values, or even lists). Pandas is also well-equipped to handle time series data, as we will see. We start with some illustrative toy examples before entering the almost-big-data world using the TweetsCOV19 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94fdc35",
   "metadata": {},
   "source": [
    "### 2.1.1. Toy examples\n",
    "\n",
    "#### Data and structure\n",
    "\n",
    "In subsections 3.2 to 3.4, <a href='#destination5'>Weidmann 2022</a> discusses data, data processing, and the benefit of relational databases using toy examples and the R language. Here, we adapt these examples to Python. Keep in mind that data = content + structure. Furthermore,  consider the following two pieces of data. They have (almost) the same content but a different structure. `sdb` represents unstructured data, `tdb` - is structured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ef5241",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdb = 'Switzerland is a country with 8.3 million inhabitants, and its capital is Bern. Another country is Austria; its capital is Vienna and the population is 8.7 million.'\n",
    "sdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e92be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c59cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdb = pd.DataFrame(data=[['Switzerland', 8.3, 'Bern'], ['Austria', 8.7, 'Vienna']], columns=['country', 'population', 'capital'])\n",
    "tdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb194bbc",
   "metadata": {},
   "source": [
    "`tdb` is a Pandas table called a [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). The columns of a DataFrame are called [Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.html). A DataFrame contains labeled axes (rows and columns). Axis 0 (the rows) is called the **index**, and its labels consist of integers from $0$ to $n-1$ by default, where $n$ is the number of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af92f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdb.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067bbf8a",
   "metadata": {},
   "source": [
    "When no names are given, the **columns** (axis 1) are also labeled in such a way, but using text labels makes the table much more readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4bc142",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdb.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e6728e",
   "metadata": {},
   "source": [
    "Similarly the index can be a list of text labels or unordered integers.\n",
    "\n",
    "Rows, columns, or cells can be extracted by specifying their [`loc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html)ations (labels). For example, to extract the capital of the second row (Austria):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a408a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdb.loc[1, 'capital']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac8cb80",
   "metadata": {},
   "source": [
    "Filtering data. For example, selecting all rows where the country is Switzerland:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdb[tdb['country'] == 'Switzerland']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467b88a8",
   "metadata": {},
   "source": [
    "To add a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0cd947",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdb['area'] = [41, 83]\n",
    "tdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948eecd4",
   "metadata": {},
   "source": [
    "The values in the 'area' column are of the integer data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5584cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdb['area'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dd0e97",
   "metadata": {},
   "source": [
    "To add a new row, we must first create a DataFrame containing the new row. Then we can [`concat`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)enate the two dataframes on the index axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3950f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = ['Liechtenstein', 0.038 , 'Vaduz', 0.16]\n",
    "tdb_new_row = pd.DataFrame(data=[new_row], columns=tdb.columns)\n",
    "tdb = pd.concat(objs=[tdb, tdb_new_row], axis=0)\n",
    "tdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c4597e",
   "metadata": {},
   "source": [
    "Note that the 'area' column is now a continuous numerical variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f06b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdb['area'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae726ffc",
   "metadata": {},
   "source": [
    "But also note that concatenation results in the old indexes being used (the third row also has index 0). To reset the index and drop the old values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24c270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdb = tdb.reset_index(drop=True)\n",
    "tdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5048d",
   "metadata": {},
   "source": [
    "To remove the column 'area'  we can [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) it (use `axis=1`) and put the result `inplace` of the original table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdb.drop(labels=['area'], axis=1, inplace=True)\n",
    "tdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2419fc7",
   "metadata": {},
   "source": [
    "Alternatively, you could have used `del` which also works for whole dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2ba9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del tdb['area']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665cb0bc",
   "metadata": {},
   "source": [
    "Rows can only be removed with the `drop()` method by using `axis=0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdb.drop(labels=[2], axis=0, inplace=True)\n",
    "tdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636974db",
   "metadata": {},
   "source": [
    "#### Wide vs. long structure\n",
    "\n",
    "A rule in data management states that tables should grow long, not wide. Consider this `bad_table` of two countries' population sizes in three years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3418a9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_table = pd.DataFrame(data=[['Switzerland', 4.7, 5.3, 6.2], ['Austria', 6.9, 7.1, 7.5]], columns=['country', 'pop1950', 'pop1960', 'pop1970'])\n",
    "bad_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eda0ab",
   "metadata": {},
   "source": [
    "Though appealing to the eye, this table is computationally bad, as can be demonstrated by trying to compute the average population size over all countries and years. It is relatively easy to compute the mean for each year by selecting the corresponding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c9eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_table[['pop1950', 'pop1960', 'pop1970']].mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77aa666",
   "metadata": {},
   "source": [
    "But computing the overall mean requires selecting columns and taking the mean of the year means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0de912",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_table[['pop1950', 'pop1960', 'pop1970']].mean().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85ce79",
   "metadata": {},
   "source": [
    "A `good_table` is long, not wide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f2b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_table = pd.DataFrame(data=[['Switzerland', 1950, 4.7], ['Switzerland', 1960, 5.3], ['Switzerland', 1970, 6.2], ['Austria', 1950, 6.9], ['Austria', 1960, 7.1], ['Austria', 1970, 7.5]], columns=['country', 'year', 'population'])\n",
    "good_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e7b90d",
   "metadata": {},
   "source": [
    "Computing the overall mean is a simple operation on one column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10716fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_table['population'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ef65d",
   "metadata": {},
   "source": [
    "and the year means can be obtained via aggregation (using the [`groupby()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) method) without having to specify any year columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e8cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_table.groupby('year').mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5011168",
   "metadata": {},
   "source": [
    "#### Multiple tables\n",
    "\n",
    "What does it mean that relational databases eliminate redundancies and ensure consistency? Consider a copy of the `good_table` with an additional column that contains a country's capital (copying makes sure that any changes made to `good_table2` do not affect the original `good_table`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb4364",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_table2 = good_table.copy()\n",
    "good_table2.loc[0:2, 'capital'] = 'Bern'\n",
    "good_table2.loc[3:5, 'capital'] = 'Vienna'\n",
    "good_table2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d5309c",
   "metadata": {},
   "source": [
    "Clearly, this table contains redundant information because country-capital pairs are always the same. This data format also potentially yields a consistency problem. If, for example, one wants to refer to capital names not in English but in the respective national language, one must replace each occurrence of 'Vienna' (English) with 'Wien' (German). But if you miss a single occurrence, your table becomes inconsistent. You can evade both problems if you split `good_table2` into two tables: one containing the population sizes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a25da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "populations = good_table2[['country', 'year', 'population']].copy()\n",
    "populations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962ea1b",
   "metadata": {},
   "source": [
    "and one containing the capitals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c939946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals = good_table2[['country', 'capital']].drop_duplicates().reset_index(drop=True)\n",
    "capitals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd3e190",
   "metadata": {},
   "source": [
    "This way, you have eliminated all redundancies and ensured consistency because you need to replace 'Vienna' with 'Wien' in one place only.\n",
    "\n",
    "Next, we will take Pandas and relational database thinking to the next level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdb8705",
   "metadata": {},
   "source": [
    "### 2.1.2. TweetsCOV19\n",
    "\n",
    "[Twitter](https://en.wikipedia.org/wiki/Twitter) is a microblogging service that is very influential among politicians and journalists. Though stagnating over the past years, the number of monthly active users was 238 million in the second quarter of 2022 (<a href='#destination6'>Wikipedia, 2022</a>). Since January 2013, researchers at [L3S](https://www.l3s.de/) and [GESIS](https://www.gesis.org/) have been collecting a 1% random sample of all Twitter transactions (tweets), detecting sentiments, and extracting named entities, user mentions, hashtags, as well as URLs, and making those publicly available as the [TweetsKB](https://data.gesis.org/tweetskb/) corpus (<a href='#destination4'>Fafalios *et al.*, 2018</a>). By August 2022, the corpus had grown to about 3 billion tweets. In the following, we will store the content of a small fraction of those tweets, one month of the [TweertsCOV19](https://data.gesis.org/tweetscov19/) corpus (<a href='#destination3'>Dimitrov *et al.*, 2020</a>), in multiple Pandas dataframes that make reference to the TweetsKB data structure.\n",
    "\n",
    "#### Ontologies in practice\n",
    "\n",
    "|<img src='images/model.png' style='float: none; width: 640px'>|\n",
    "|:--|\n",
    "|<em style='float: center'>**Figure 1**: Data structure used to build the TweetsKB corpus ([source](https://data.gesis.org/tweetskb/#Data-model))</em>|\n",
    "\n",
    "The data structure depicted in ***figure 1*** used to build the TweetsKB corpus is relational and uses several standardized ontologies. **Relational** means that each piece of content belongs to a class, and classes have properties that can either describe a class attribute or link to another class. We will shortly see that classes are candidates for tables. Classes and properties are drawn from **ontologies** which are vocabularies for modeling data and, in our particular tweets case, online community data. These vocabularies are developed and maintained by the [Semantic Web](https://en.wikipedia.org/wiki/Semantic_Web) research community, aiming to make internet data machine-readable.\n",
    "\n",
    "|<img src='images/TweetsKB_model_example.jpg' style='float: none; width: 640px'>|\n",
    "|:--|\n",
    "|<em style='float: center'>**Figure 2**: Example of how a tweet is encoded using the data structure</em>|\n",
    "\n",
    "***Figure 2*** is an example of how a tweet is encoded using this abstract data structure. In other words, the figure depicts how the content of a tweet is modeled as machine-readable data. Starting with the central element, a **tweet** is modeled as belonging to the [Post](https://www.w3.org/Submission/sioc-spec/#term_Post) class, which is defined as an \"article or message that can be posted to a Forum\" in the [SIOC](http://sioc-project.org/) ontology. A tweet has a [has_creator](https://www.w3.org/Submission/sioc-spec/#term_has_creator) property which relates a tweet to a user. A **user** is modeled as belonging to the [User](https://www.w3.org/Submission/sioc-spec/#term_User) class (in figure 1, the class is called \"UserAccount\"), which is defined as a \"User account in an online community site.\" The \"tweet1\" instance of Post as well as the \"usr1\" instance of User have [id](http://rdfs.org/sioc/spec/#term_id) properties that link to the actual [literal](https://www.w3.org/TR/rdf-schema/#ch_literal) values of the **tweet id** (<span style='font-family:Courier'>9565121266</span>) and (encrypted) **user name** (<span style='font-family:Courier'>2356912</span>) variables. Below, we will create separate Pandas tables for the Post and User classes, among others. Just like in the above example of populations and capitals, this is how redundancy is eliminated.\n",
    "\n",
    "Starting from such an understanding of separate tables for tweets and users, we can discuss which one some of the other variables belong to which come with the data. The **timestamp**, **number of retweets** (number of users that forward the tweet), and **number of favorites** (number of users that like the tweet) clearly are attributes of tweets. In the example of *figure 1*, \"tweet1\" is liked by $12$ users, a statistic that is modeled using the [InteractionCounter](https://schema.org/InteractionCounter) for the [LikeAction](https://schema.org/LikeAction) of the [Schema.org](https://schema.org/) vocabulary. The **number of followers** (number of other users that follow a user) and **number of friends** (number of other users a user follows) seem to be attributes of users at first glance. But since they are measured at the time of tweet creation, they are better also attributed to tweets. While the Twitter API delivers these variables, the following variables have been obtained by the corpus creators by processing the tweet content. The sentiment or emotional content of a tweet is modeled by using the [Onyx](https://www.gsi.upm.es/ontologies/onyx/) ontology, which is \"designed to annotate and describe the emotions expressed by user-generated content\". The [SentiStrength](http://sentistrength.wlv.ac.uk/) algorithm results in **positive sentiment** (1 means low and 5 means high) and **negative sentiment** (-1 means low and -5 means high) variables. Though the sentiment expresses the mind state of a user, it is expressed in language and is, hence, a tweet attribute.\n",
    "\n",
    "The dataset producers have also annotated tweets by extracting four different kinds of **facts** (communicative symbols) from tweet texts: named entities (universally recognized semantic concepts), user **mentions** (words starting with <span style='font-family:Courier'>@</span>), **hashtags** (words starting with <span style='font-family:Courier'>#</span>), and **URLs** (addresses of web pages). Since URLs are often too detailed, we will also extract the **TLDs** (top-level domains) from URLs. To identify **named entities**, the [FEL](https://github.com/yahoo/FEL) algorithm matches parts of the tweet **text** to Wikipedia pages as universally identifiable resources and provides a **confidence** score to what extent the match is trustworthy (0 means high and -3 means low confidence). In the example of *figure 2*, the text snippet \"<span style='font-family:Courier'>Federer</span>\" has been matched to the Wikipedia resource [Roger_Federer](https://de.wikipedia.org/wiki/Roger_Federer) with an average confidence of $-1.54$.\n",
    "\n",
    "|<img src='images/TweetsCOV19_ext_erd.png' style='float: none; width: 640px'>|\n",
    "|:--|\n",
    "|<em style='float: center'>**Figure 3**: Entity relationship diagram to organize Pandas tables</em>|\n",
    "\n",
    "It is clear that named entities, mentions, hashtags, URLs, and TLDs cannot be tweet attributes, as that would create an immense amount of redundancy. Hence, each of these five facts gets its own table. ***Figure 3*** shows the entity relationship diagram into which we will transform the tweets data. The diagram in *figure 3* mirrors the TweetsKB data structure.We will construct the tables shown in the figure by following the rules of [database normalization](https://en.wikipedia.org/wiki/Database_normalization), which we have introduced in section 2.1.1, in the most basic way. Entities are classes in the above sense and are not to be confused with named entities. We will create **entity tables** for the seven entities discussed so far: `tweets`, `users`, `named_entities`, `mentions`, `hashtags`, `utls`, and `tlds`. Each table has a primary key (PK) that uniquely identifies the entity instances in a table. We will use the index of Pandas dataframes as primary keys. Six of those tables have a column called 'tweets', which is the number of tweets a user has created or the number of times a fact has been selected in a tweet.\n",
    "\n",
    "In addition, we will create five **relationship tables** that put tweets into a relationship to facts (named entities, mentions, hashtags, URLs, and TLDs). Relationship tables are depicted using dashed lines in *figure 3*. They just contain entity identifiers (indices) that are now called foreign keys (FK). We will shortly see that relationship tables can be directly used in data analysis. One of the five tables is an exception: The `tweets_named_entities` table has two more columns – the text that was used to name the named entity and the confidence score – because these are true attributes of the relationship between tweets and named entities. Finally, users and tweets are linked in the tweets table via the 'user_idx' column because a tweet is created by one and only one user.\n",
    "\n",
    "#### Structuring TweertsCOV19\n",
    "\n",
    "We will be working with the May 2020 dump of the TweertsCOV19 corpus. Download this [file](https://zenodo.org/record/4593502/files/TweetsCOV19_052020.tsv.gz) and put it into the '../data/TweetsCOV19' folder. The [description](https://data.gesis.org/tweetscov19/#Dataset) of the dataset says that each row contains variables of a tweet instance, there are twelve variables (columns), and variables are separated by a tab character ('\\t'). In other words, the data is delivered as a table. Furthermore, the description says that sentiment scores, named entity metadata, etc. are concatenated. In other words, the delivered table is wide in selected columns. Our job will be to transform this table into the multiple tables of *figure 3*. Before reading the full data, it is a good idea to look at the first rows to check if the file contains column names and if there are any peculiarities. Using UTF-8 encoding is recommended since it allows for coding all the different characters used in tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b011e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "head = pd.read_csv(\n",
    "    filepath_or_buffer = '../data/TweetsCOV19/TweetsCOV19_052020.tsv.gz', \n",
    "    sep = '\\t', \n",
    "    nrows = 5, \n",
    "    encoding = 'utf-8'\n",
    ")\n",
    "head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a88ad",
   "metadata": {},
   "source": [
    "Get the number of rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed239d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "head.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e337b48",
   "metadata": {},
   "source": [
    "Knowing that the file does not contain column names and that the separator indeed creates twelve columns, we can read the whole file.\n",
    "\n",
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Caution</b>\n",
    "\n",
    "The file we are about to load is almost 200 MB large in compressed format. When loaded into memory as a dataframe, it consumes almost 1 GB. Since we are about to create many new tables from it, which all require significant amounts of memory, you can quickly reach the limits of the machine you are working on. In fact, if we work with the whole file and run the notebook all until the end, it will consume 4.2 GB. This is too much if, for example, you are executing this notebook on mybinder.org, which gives you 2 GB of memory.\n",
    "</div>\n",
    "\n",
    "To reduce the memory load, take a sample from the already-sampled file. Setting the `seed()` of the random library will create results that are exactly reproducible. `p` is the sample fraction to load. It is set to 25% to use less than 2 GB of memory. Increase it if you have more memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c086303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5510919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "p = .25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b351c8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\n",
    "    filepath_or_buffer = '../data/TweetsCOV19/TweetsCOV19_052020.tsv.gz', \n",
    "    sep = '\\t', \n",
    "    header = None, \n",
    "    skiprows = lambda i: i > 0 and random.random() > p, \n",
    "    quoting = 3, \n",
    "    encoding = 'utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90db525",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Caution</b>\n",
    "\n",
    "Setting the `quoting` parameter of the [`read_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function to the value `3` means that no quoting symbols (*e.g.*, quotation marks) are used to enclose the content of cells in columns. This allows that the respective symbol can be a cell content. In the TweetsCOV19 dataset, some hashtags actually contain quotation marks. Not setting the parameter to `3` would result in a wrong reading of the file.\n",
    "</div>\n",
    "\n",
    "Since the table does not have column names, we use those from the data [description](https://data.gesis.org/tweetscov19/#Dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.columns = ['tweet_id', 'user', 'timestamp', 'followers', 'friends', 'retweets', 'favorites', 'named_entities', 'sentiments', 'mentions', 'hashtags', 'urls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69d4b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb462d",
   "metadata": {},
   "source": [
    "There are 480 thousand tweets (1.9 million for the full sample). Look at the last five columns to see how multiple annotations are stored in single cells.\n",
    "\n",
    "#### Creating the `users` table\n",
    "\n",
    "Besides the 'user' name, the `users` table should also contain the number of tweets the user has created as well as the maximum number of followers and friends. Aggregate the data using `groupby()` with the `size()` method to count the number of rows (i.e., the number of tweets)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b502ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = tweets.groupby(by='user').size().reset_index(name='tweets')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97c5870",
   "metadata": {},
   "source": [
    "and then with the `.max()` method to get the maximum number of followers and friends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d1b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ff = tweets.groupby(by='user')[['followers', 'friends']].max().reset_index()\n",
    "users_ff.columns = ['user', 'followers_max', 'friends_max']\n",
    "users_ff.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf5052a",
   "metadata": {},
   "source": [
    "Since these dataframes are both ordered alphabetically and have the same length, we can simply add the two columns from `users_ff` to the `users` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc67fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "users[['followers_max', 'friends_max']] = users_ff[['followers_max', 'friends_max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30db09e",
   "metadata": {},
   "source": [
    "Sort the dataframe descendingly by the number of tweets, maximum number of followers, and maximum number of friends (in that order):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b3a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users.sort_values(by=['tweets', 'followers_max', 'friends_max'], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43475042",
   "metadata": {},
   "source": [
    "Finally, reorder the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf44ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users[['user', 'tweets', 'followers_max', 'friends_max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc56347d",
   "metadata": {},
   "source": [
    "The index will function as a unique user identifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90409559",
   "metadata": {},
   "outputs": [],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e58cd",
   "metadata": {},
   "source": [
    "Note that 'user' names are encrypted for privacy reasons in the original dataset. There are 350 thousand distinct users (1.1 million in the full dataset), and the most active one has created 1,989 tweets (in the full dataset). Indeed, it is not an error that some users have tens of millions of followers [and more](https://en.wikipedia.org/wiki/List_of_most-followed_Twitter_accounts). An interesting observation is that the most active users also have many followers.\n",
    "\n",
    "The index values of this table are unique identifiers for the users in the dataset (the primary keys). The effect of sorting is that the most active users have small index values, which aids computational purposes, as you will see. To not waste memory, it is good practice to delete dataframes we do not need anymore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf106d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del users_ff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fad0b8",
   "metadata": {},
   "source": [
    "#### Creating the `tweets` table\n",
    "\n",
    "We will proceed by refining the existing `tweets` table. Sorting tweets by date and time is straightforward. For handling such data, Pandas provides the 'datetime' data type. It is perfectly suited for handling time series data as it allows many ways to manipulate dates and times. For now, we will simply transform the 'timestamp' values from 'string' [`to_datetime()`](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html). Set the `format` of the original string to spare Pandas figuring it out itself (and save time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b0335",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['timestamp'] = pd.to_datetime(tweets['timestamp'], format='%a %b %d %X %z %Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128be749",
   "metadata": {},
   "source": [
    "Knowing that the timezone is Coordinated Universal Time (UTC), remove it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d16c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets['timestamp'].dt.tz)\n",
    "tweets['timestamp'] = tweets['timestamp'].dt.tz_localize(tz=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47269a2",
   "metadata": {},
   "source": [
    "Then remove tweets from April 2020 (since they are not complete) and sort the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[tweets['timestamp'] >= '2020-05-01']\n",
    "tweets = tweets.sort_values(by=['timestamp']).reset_index(drop=True)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78616f2",
   "metadata": {},
   "source": [
    "After sorting, the index is stable and acts as a unique tweet identifier.\n",
    "\n",
    "The first change the table needs is to replace the 'user' name with the 'user_idx' index from the `users` table. Since we will repeat this operation for other tables, we define an `add_index()` function. Following best Python practice, what it does is described in the function itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd11ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_index(source, target, entity):\n",
    "    '''\n",
    "    Inserts the index of a source dataframe into a target dataframe as a column.\n",
    "    \n",
    "    Parameters:\n",
    "        source : Pandas DataFrame\n",
    "            Dataframe whose index is to be inserted.\n",
    "        target : Pandas DataFrame\n",
    "            Dataframe into which the index is inserted.\n",
    "        entity : String\n",
    "            Name of the entity that is identified by the index. Will be given an '_idx' suffix and then inserted into the target dataframe.\n",
    "    \n",
    "    Returns:\n",
    "        The target dataframe with the inserted column.\n",
    "    '''\n",
    "    _ = source.copy()\n",
    "    _[entity + '_idx'] = _.index\n",
    "    df = pd.merge(left=target, right=_[[entity + '_idx', entity]], on=entity)\n",
    "    del df[entity]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00140e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = add_index(source=users, target=tweets, entity='user')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea744bb6",
   "metadata": {},
   "source": [
    "After reordering the columns, the 'user_idx' column is at the right position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb95f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[['tweet_id', 'user_idx', 'timestamp', 'followers', 'friends', 'retweets', 'favorites', 'named_entities', 'sentiments', 'mentions', 'hashtags', 'urls']]\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b377769d",
   "metadata": {},
   "source": [
    "#### Creating the `named_entities` and `tweets_named_entities` tables\n",
    "\n",
    "Next, we process the facts. In general, we proceed by, first, extracting relationship tables from the `tweets` table and, second, deriving the entity tables from the relationship tables. We start with the most complicated case of **named entities**. The 'named_entities' column of the `tweets` table contains ';'-separated 3-tuples, each of which contains ':'-separated values for 'text', 'named_entity', and 'confidence'. In the process of normalization, the first step is to transform cell content into lists of 3-tuples. Again, we define a custom `to_list()` function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff4298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(cell, pat):\n",
    "    '''\n",
    "    Function to be applied to individual cells of a dataframe column. Transforms concatenated cell content into a list.\n",
    "    \n",
    "    Parameters:\n",
    "        pat : String\n",
    "            Pattern that separates the cell values.\n",
    "    \n",
    "    Returns:\n",
    "        The cell will automatically be overwritten by a potentially empty list.\n",
    "    '''\n",
    "    if cell == 'null;' or type(cell) == float:\n",
    "        cell = ['']\n",
    "    else:\n",
    "        cell = cell.split(pat)\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba583d2",
   "metadata": {},
   "source": [
    "that we can now `apply` cell by cell to the 'named_entities' column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e784dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['named_entities'] = tweets['named_entities'].apply(to_list, pat=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f4546",
   "metadata": {},
   "source": [
    "An example cell with a list of multiple 3-tuples now looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64f5c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['named_entities'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01bb7ab",
   "metadata": {},
   "source": [
    "Next, we create the `tweets_named_entities` relationship table. The reason for having created a list of 3-tuples is that a wide table – the subtable with just the 'named_entities' column – can be easily and quickly transformed into a long table using the [`explode()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287af264",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_named_entities = tweets[['named_entities']].explode(column='named_entities')\n",
    "del tweets['named_entities']\n",
    "tweets_named_entities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf28b9d7",
   "metadata": {},
   "source": [
    "Remove rows without 3-tuples (concatenation artifacts) and split the 3-tuples into three columns by splitting the string at ':':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb1bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_named_entities = tweets_named_entities[tweets_named_entities['named_entities'] != '']\n",
    "tweets_named_entities = tweets_named_entities['named_entities'].str.split(pat=':', expand=True)\n",
    "tweets_named_entities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54449eb9",
   "metadata": {},
   "source": [
    "At this point, the index consists of the unique tweet identifiers because we used `explode()` on a `tweets` subtable.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "<b>Insight: Data quality</b>\n",
    "\n",
    "Part of what we call \"getting a feeling for your data\" is actually to look at it. For example, here is the right stage in the data processing pipeline to check how well the named-entity-recognition algorithm worked because the `tweets_named_entities` table still contains the 'text' from which a named entity was recognized as well as the name of the 'named_entity'. Pandas is actually not the best tool to read tables as it only displays up to 50 rows at a time. You can cycle through your columns using `loc[]`, but it is more convenient to use a spreadsheet editor for the task. Therefore, we export the distinct rows of the `tweets_named_entities` table to an Excel file. The first column is the tweet index, and you can look up the 'tweet_id' via `tweets.loc[<tweet index without angle brackets>, :]`. You can read the actual tweet of any user (*e.g.*, with the 'tweet id' $1262933787131904001$) by visiting https://twitter.com/anyuser/status/1262933787131904001.\n",
    "\n",
    "Going through the rows, you will notice that the text snippet <span style='font-family:Courier'>democrat</span> is matched to the [Democratic_Party_(United_States)](https://en.wikipedia.org/wiki/Democratic_Party_(United_States)) named entity with a confidence of -1.81. Even though the matching is mostly right in our particular context, the algorithm will also mismatch a lot of discourse about democracy itself. The problem also exists the other way around. The text <span style='font-family:Courier'>dems</span>, which is frequently used to name the Democratic Party of the US, is matched to [Defensively_equipped_merchant_ship](https://en.wikipedia.org/wiki/Defensively_equipped_merchant_ship) with a slightly weaker confidence of -2.03. The idea is to use this score to filter out bad matchings. However, setting the filter to -2.00 would result in missing the correct matching of <span style='font-family:Courier'>us democratic party</span>, which has a score of -2.06. As you go through the spreadsheet, you will notice more mistakes. All these point to the general problem of fully automated data processing.\n",
    "</div>\n",
    "\n",
    "Before the Excel file is saved, the target directory is created if it does not exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005fd03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'results'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a121d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_named_entities.drop_duplicates().to_excel(\n",
    "    excel_writer = 'results/tweets_named_entities_dropped_duplicates.xlsx', \n",
    "    engine = 'xlsxwriter', \n",
    "    encoding = 'utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539022ea",
   "metadata": {},
   "source": [
    "Continuing with creating `tweets_named_entities`, if we now reset the index without dropping the old one, the tweet index values will be added as the first column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23bde26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_named_entities = tweets_named_entities.reset_index(drop=False)\n",
    "tweets_named_entities.columns = ['tweet_idx', 'text', 'named_entity', 'confidence']\n",
    "tweets_named_entities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c01f3",
   "metadata": {},
   "source": [
    "Drop duplicate relationships because we are not interested in multiple occurrences of a named entity in one tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_named_entities = tweets_named_entities.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b691cdb9",
   "metadata": {},
   "source": [
    "The `named_entities` entity table with the desired 'tweets' column is created from the `tweets_named_entities` relationship table, again using a dedicated function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f325d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entity_table(relationship_table, entity):\n",
    "    '''\n",
    "    Creates an entity table from a relationship table via aggregation.\n",
    "    \n",
    "    Parameters:\n",
    "        relationship_table : Pandas DataFrame\n",
    "            Dataframe that contains tweet entity relationships.\n",
    "        entity : String\n",
    "            Name of the entity column in the relationship table that contains the entities.\n",
    "    \n",
    "    Returns:\n",
    "        An entity table sorted descendingly by an additional 'tweets' column giving the number of tweets that selected an entity.\n",
    "    '''\n",
    "    df = relationship_table.groupby(entity).size().reset_index(name='tweets')\n",
    "    df = df.sort_values(by=['tweets', entity], ascending=[False, True]).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afefc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities = create_entity_table(relationship_table=tweets_named_entities, entity='named_entity')\n",
    "named_entities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b533a1f",
   "metadata": {},
   "source": [
    "Given this entity table, add its index to the corresponding `tweets_named_entities` relationship table, reorder the columns to obtain the desired design, and change the data type of the 'confidence' scores from string to a rounded float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5731433",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_named_entities = add_index(source=named_entities, target=tweets_named_entities, entity='named_entity')\n",
    "tweets_named_entities = tweets_named_entities[['tweet_idx', 'named_entity_idx', 'text', 'confidence']]\n",
    "tweets_named_entities['confidence'] = tweets_named_entities['confidence'].astype(float).round(4)\n",
    "tweets_named_entities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f0df1",
   "metadata": {},
   "source": [
    "#### Creating the other relationship and entity tables\n",
    "\n",
    "The data of the remaining facts (mentions, hashtags, URLs, and TLDs) is easier to normalize because the relationship tables only contain foreign keys. Without having to process metadata from named entity recognition, we can create the relationship tables via a general function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6357d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relationship_table(entity, to_lower_case, drop_duplicates=True, source=tweets):\n",
    "    '''\n",
    "    Creates a relationship table for a given entity from the `tweets` table.\n",
    "    \n",
    "    Parameters:\n",
    "        source : Pandas DataFrame, default `tweets`\n",
    "            Table that contains the entity column.\n",
    "        entity : String\n",
    "            Name of the entity column in the source table that contains the entities. The column must contain an object data type list of entity names.\n",
    "        to_lower_case : Boolean\n",
    "            Whether entity names should be reduced to lower case.\n",
    "        drop_duplicates : Boolean, default True\n",
    "            Whether duplicate relationships should be removed.\n",
    "    \n",
    "    Returns:\n",
    "        A relationship table linking tweet indices to entity names.\n",
    "    '''\n",
    "    df = source[[entity + 's']].explode(column=entity + 's')\n",
    "    df = df[df[entity + 's'] != '']\n",
    "    df = df.reset_index()\n",
    "    df.columns = ['tweet_idx', entity]\n",
    "    if to_lower_case == True:\n",
    "        df[entity] = df[entity].str.lower()\n",
    "    if drop_duplicates == True:\n",
    "        df = df.drop_duplicates().reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6eb86",
   "metadata": {},
   "source": [
    "The processing pipeline is the same for all four facts:\n",
    "\n",
    "1. Transform the entity column in the `tweets` table to a list\n",
    "2. Create the relationship table from the entity column in the `tweets` table, dropping duplicate rows by default\n",
    "3. Delete the entity column in the `tweets` table\n",
    "4. Create the entity table from the relationship table\n",
    "5. Add the entity index to the relationship table\n",
    "\n",
    "In the case of **mentions**, in step 2, we must transform all capital (upper case) characters to lower case. This is because user names on Twitter are not case-sensitive. In other words, when a user named \"realDonaldTrump\" already exists, no new user will be allowed with the name \"realdonaldtrump\". Since user mentions are extracted as words starting with <span style='font-family:Courier'>@</span>, but tweet creators often use upper and lower cases as they wish, not transforming upper to lower case would result in the same mentioned user getting more than a single unique identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b340aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['mentions'] = tweets['mentions'].apply(to_list, pat=' ') # Step 1\n",
    "tweets_mentions = create_relationship_table(entity='mention', to_lower_case=True) # Step 2\n",
    "del tweets['mentions'] # Step 3\n",
    "mentions = create_entity_table(relationship_table=tweets_mentions, entity='mention') # Step 4\n",
    "tweets_mentions = add_index(source=mentions, target=tweets_mentions, entity='mention') # Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e9c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d39d13",
   "metadata": {},
   "source": [
    "Donald Trump is the most mentioned user by far, followed by the prime minister of India. Narendra Modi. both with his official and private accounts.\n",
    "\n",
    "In the case of **hashtags**, do the same transformation from upper to lower case to prevent synonymous hashtags getting from different indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a591a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['hashtags'] = tweets['hashtags'].apply(to_list, pat=' ') # Step 1\n",
    "tweets_hashtags = create_relationship_table(entity='hashtag', to_lower_case=True) # Step 2\n",
    "del tweets['hashtags'] # Step 3\n",
    "hashtags = create_entity_table(relationship_table=tweets_hashtags, entity='hashtag') # Step 4\n",
    "tweets_hashtags = add_index(source=hashtags, target=tweets_hashtags, entity='hashtag') # Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864f1b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4588629d",
   "metadata": {},
   "source": [
    "**URLs** are case-sensitive. Hence, set `to_lower_case=False` in step 2. To create the tables related to **TLDs**, postpone the step 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c22dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['urls'] = tweets['urls'].apply(to_list, pat=':-:') # Step 1\n",
    "tweets_urls = create_relationship_table(entity='url', to_lower_case=False) # Step 2\n",
    "del tweets['urls'] # Step 3\n",
    "urls = create_entity_table(relationship_table=tweets_urls, entity='url') # Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432675f8",
   "metadata": {},
   "source": [
    "At this point, `tweets_urls` has all the information to create the TLD-related tables. Create the ``tweets_tlds`` as a copy of `tweets_urls` and extract the TLDs (pseudo step 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c36f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tlds = tweets_urls.copy()\n",
    "tweets_tlds['tld'] = tweets_tlds['url'].str[8:].str.split(pat='/').str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b0a8d",
   "metadata": {},
   "source": [
    "Now, finish step 5 for URLs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6884056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_urls = add_index(source=urls, target=tweets_urls, entity='url') # Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b4398f",
   "metadata": {},
   "source": [
    "and steps 4 and 5 for TLDs (step 3 is not necessary since no such column ever existed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db434588",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlds = create_entity_table(relationship_table=tweets_tlds, entity='tld') # Step 4\n",
    "tweets_tlds = add_index(source=tlds, target=tweets_tlds, entity='tld') # Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ff51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99af14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e1eba4",
   "metadata": {},
   "source": [
    "As expected, the detail of URLs hides which TLDs are most popular.\n",
    "\n",
    "#### Sentiment data\n",
    "\n",
    "Positive and negative sentiment scores are stored as a string in `tweets['sentiments']`. To construct the desired three columns from it, we only have to split and expand the string (line 1), and transform the scores into integers (lines 2–3), create the average score (line 4), ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f299a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sentiments = tweets['sentiments'].str.split(pat=' ', expand=True)\n",
    "tweets_sentiments[0] = tweets_sentiments[0].astype('int')\n",
    "tweets_sentiments[1] = tweets_sentiments[1].astype('int')\n",
    "tweets_sentiments[2] = tweets_sentiments[[0, 1]].mean(axis=1)\n",
    "tweets_sentiments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39308999",
   "metadata": {},
   "source": [
    "and append these columns to the `tweets` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b617724",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[['sentiment_pos', 'sentiment_neg', 'sentiment_avg']] = tweets_sentiments\n",
    "del tweets['sentiments']\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f3dc1",
   "metadata": {},
   "source": [
    "As the data is structured now, it is not fully normalized because the `tweets` table is still wide regarding sentiment scores. In a way, it is like the `bad_table` in the toy example above, which did not allow us to compute an average easily. It is worth discussing whether or not it is necessary to fully normalize the data and store the three sentiment scores in a relationship table and the sentiment category in yet another table.For most tasks, working with the three columns in the `tweets` table will certainly be easier. However, as we will see in the following subsection, it is beneficial to normalize our data all the way through. With `tweets_sentiments`, we already have the first step for the relationship table. Use the [`melt()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.melt.html) function, which makes this wide subtable long ([`ignore_index=False`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.melt.html) keeps the tweet index as a column). With the few steps in the following cell, we have already completed steps 2, 3, and 5 from the pipeline:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455e7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sentiments = pd.melt(tweets_sentiments, ignore_index=False).reset_index()\n",
    "tweets_sentiments.columns = ['tweet_idx', 'sentiment_idx', 'score']\n",
    "tweets_sentiments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39958df",
   "metadata": {},
   "source": [
    "The only thing left to do is to create the `sentiments` table, which contains the labels for the 'sentiment_idx' in `tweets_sentiments`. We have to make this manually as we have not introduced these labels yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = pd.DataFrame(data=['positive', 'negative', 'average'], columns=['sentiment'])\n",
    "sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912b73de",
   "metadata": {},
   "source": [
    "### 2.1.3. Saving the tables to file(s)\n",
    "\n",
    "#### Multiple TSV files\n",
    "\n",
    "Now that we have a set of linked tables, how do we store them? A simple way is to save each table to a file with tab-separated values (TSV) using [`to_csv(sep='\\t')`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d1550",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../data/TweetsCOV19/TweetsCOV19_tables'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b296e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets.to_csv(path_or_buf='../data/TweetsCOV19/TweetsCOV19_tables/tweets.tsv', sep='\\t', index=False, encoding='utf-8')\n",
    "#users.to_csv(path_or_buf='../data/TweetsCOV19/TweetsCOV19_tables/users.tsv', sep='\\t', index=False, encoding='utf-8')\n",
    "#tweets_named_entities.to_csv(path_or_buf='../data/TweetsCOV19/TweetsCOV19_tables/tweets_named_entities.tsv', sep='\\t', index=False, encoding='utf-8')\n",
    "#tweets_mentions.to_csv(path_or_buf='../data/TweetsCOV19/TweetsCOV19_tables/tweets_mentions.tsv', sep='\\t', index=False, encoding='utf-8')\n",
    "#tweets_hashtags.to_csv(path_or_buf='../data/TweetsCOV19/TweetsCOV19_tables/tweets_hashtags.tsv', sep='\\t', index=False, encoding='utf-8')\n",
    "#tweets_urls.to_csv(path_or_buf='../data/TweetsCOV19/TweetsCOV19_tables/tweets_urls.tsv', sep='\\t', index=False, encoding='utf-8')\n",
    "#tweets_tlds.to_csv(path_or_buf='../data/TweetsCOV19/TweetsCOV19_tables/tweets_tlds.tsv', sep='\\t', index=False, encoding='utf-8')\n",
    "#named_entities.to_csv(path_or_buf='../data/TweetsCOV19/TweetsCOV19_tables/named_entities.tsv', sep='\\t', index=False, encoding='utf-8')\n",
    "#mentions.to_csv(path_or_buf='../data/TweetsCOV19/TweetsCOV19_tables/mentions.tsv', sep='\\t', index=False, encoding='utf-8')\n",
    "#hashtags.to_csv(path_or_buf='../data/TweetsCOV19/TweetsCOV19_tables/hashtags.tsv', sep='\\t', index=False, encoding='utf-8')\n",
    "#urls.to_csv(path_or_buf='../data/TweetsCOV19/TweetsCOV19_tables/urls.tsv', sep='\\t', index=False, encoding='utf-8')\n",
    "#tlds.to_csv(path_or_buf='../data/TweetsCOV19/TweetsCOV19_tables/tlds.tsv', sep='\\t', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d741121",
   "metadata": {},
   "source": [
    "Note that, in the above cell, we set `index=False`. That means the index is not written to file.\n",
    "This is not a problem because it is a row counter starting with 0 in any table. Such an index is automatically created if `index_col=None` when reading these files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d1a584",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\n",
    "    filepath_or_buffer = '../data/TweetsCOV19/TweetsCOV19_tables/tweets.tsv', \n",
    "    sep = '\\t', \n",
    "    index_col = None, \n",
    "    encoding = 'utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e55cc",
   "metadata": {},
   "source": [
    "#### SQL\n",
    "\n",
    "One benefit of a relational database is that it can be stored in one file. Hence, another possibility is to take the next step on the relational database path and store all tables in an SQL database. [SQLAlchemy](https://www.sqlalchemy.org/) has emerged as the standard Python library for working with databases in SQL-like ways. To store the twelve tables from *figure 3*, all you need to do before exporting the tables is to create a so-called \"engine\" that manages connections through which you create, modify, and query the database. In this case, we create an SQLite database (other SQL dialects are possible):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "sqlalchemy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b657382",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine(url='sqlite:///../data/TweetsCOV19/TweetsCOV19.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dec0572",
   "metadata": {},
   "source": [
    "Use the [`to_sql()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html) method, configured to use the engine just created, to add the tables to the database (the following lines just add the first five rows of each table; to add all rows remove `.head()`; a table is replaced if it already exists):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96454b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head().to_sql(name='tweets', con=engine, if_exists='replace', index=False)\n",
    "users.head().to_sql(name='users', con=engine, if_exists='replace', index=False)\n",
    "tweets_named_entities.head().to_sql(name='tweets_named_entities', con=engine, if_exists='replace', index=False)\n",
    "tweets_mentions.head().to_sql(name='tweets_mentions', con=engine, if_exists='replace', index=False)\n",
    "tweets_hashtags.head().to_sql(name='tweets_hashtags', con=engine, if_exists='replace', index=False)\n",
    "tweets_urls.head().to_sql(name='tweets_urls', con=engine, if_exists='replace', index=False)\n",
    "tweets_tlds.head().to_sql(name='tweets_tlds', con=engine, if_exists='replace', index=False)\n",
    "named_entities.head().to_sql(name='named_entities', con=engine, if_exists='replace', index=False)\n",
    "mentions.head().to_sql(name='mentions', con=engine, if_exists='replace', index=False)\n",
    "hashtags.head().to_sql(name='hashtags', con=engine, if_exists='replace', index=False)\n",
    "urls.head().to_sql(name='urls', con=engine, if_exists='replace', index=False)\n",
    "tlds.head().to_sql(name='tlds', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4f921",
   "metadata": {},
   "source": [
    "To read a dataframe from the database, also use the engine, but now in the [`read_sql()`](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16afcf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(sql='tweets', con=engine).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24570d",
   "metadata": {},
   "source": [
    "#### Excel\n",
    "\n",
    "It is also possible to store all tables as sheets in an Excel file. However, the limit is one million rows and columns per sheet, so Excel is not an option for the full dataset. But this is how the tables can be exported in principle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b3d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(path='../data/TweetsCOV19/TweetsCOV19.xlsx', engine='xlsxwriter') as writer:\n",
    "    tweets.head().to_excel(writer, sheet_name='tweets')\n",
    "    users.head().to_excel(writer, sheet_name='users')\n",
    "    tweets_named_entities.head().to_excel(writer, sheet_name='tweets_named_entities')\n",
    "    tweets_mentions.head().to_excel(writer, sheet_name='tweets_mentions')\n",
    "    tweets_hashtags.head().to_excel(writer, sheet_name='tweets_hashtags')\n",
    "    tweets_urls.head().to_excel(writer, sheet_name='tweets_urls')\n",
    "    tweets_tlds.head().to_excel(writer, sheet_name='tweets_tlds')\n",
    "    named_entities.head().to_excel(writer, sheet_name='named_entities')\n",
    "    mentions.head().to_excel(writer, sheet_name='mentions')\n",
    "    hashtags.head().to_excel(writer, sheet_name='hashtags')\n",
    "    urls.head().to_excel(writer, sheet_name='urls')\n",
    "    tlds.head().to_excel(writer, sheet_name='tlds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ea2ca7",
   "metadata": {},
   "source": [
    "To read a sheet from that file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da2fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel(io='../data/TweetsCOV19/TweetsCOV19.xlsx', sheet_name='tweets', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bcffb8",
   "metadata": {},
   "source": [
    "## 2.2. Handling data with Pandas, NumPy, and SciPy\n",
    "\n",
    "In the previous subsection, you have learned how to create and save multiple tables from the raw TweetsCOV19 table that, taken together, mimic a relational database. In practice, particularly if your data is not as rich as this dataset, it may not be necessary to do that. However, even if you can live with some amount of redundancy, it certainly is a way to keep your data tidy and consistent. Having the TweetsCOV19 data in the relational database structure, we will next query it to obtain some first data descriptions. At some point, when working with the data, even Pandas will reach its limits. It is then time for other libraries to help or take over. We will see how [NumPy](https://numpy.org/) and [SciPy](https://scipy.org/), two fundamental libraries for scientific computing, can be used to store data in n-dimensional arrays and to compute the co-occurrence of hashtags in tweets using sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce206ff",
   "metadata": {},
   "source": [
    "### 2.2.1. Querying the database just created\n",
    "\n",
    "Querying is a term from the relational database world and it means to extract information from the database. **Filtering tables** is a simple query.\n",
    "\n",
    "#### Which tweet named-entity relationships can we be confident about?\n",
    "\n",
    "Recall from the discussion above that matches below a threshold (*e.g.*, -2) may be too biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8652c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_named_entities[tweets_named_entities['confidence'] >= -2.].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf58f77",
   "metadata": {},
   "source": [
    "#### Which are the most popular tweets by unpopular users?\n",
    "\n",
    "This question is open to interpretation. If we think of popular tweets as those with many retweets and unpopular users as those with few followers, then a possible answer can be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[(tweets['retweets'] >= 1000) & (tweets['followers'] <= 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9583b372",
   "metadata": {},
   "source": [
    "Such queries are useful to identify tweets for reading. For unsampled tweets, the first two are:\n",
    "\n",
    "- https://twitter.com/anyuser/status/1262661210647887872\n",
    "- https://twitter.com/anyuser/status/1265468721440514048\n",
    "\n",
    "The third was probably deleted by the user.\n",
    "\n",
    "Since we have eliminated redundancy (*i.e.*, split up data in multiple tables), we also need to link tables to answer certain questions. Linking tables goes by the name of **joining tables**. We will introduce the use of Pandas' [`join()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html) method step by step using this question:\n",
    "\n",
    "#### Which are the most popular tweets by unproductive users?\n",
    "\n",
    "The logic of joining is to link two tables on their indices or a column that exists in both tables. If no column name is specified, Pandas joins on the index by default. You can build a query step by step. Choose a dataframe from which you want to start. In our case, this is the table of popular `tweets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_tweets = tweets[tweets['retweets'] >= 1000]\n",
    "popular_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51d08b",
   "metadata": {},
   "source": [
    "The other dataframe that contains information to answer the question is the table of unproductive (*i.e.*, one tweet only) users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558f28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unproductive_users = users[users['tweets'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51345545",
   "metadata": {},
   "source": [
    "To be able to join these tables on an index, we have stored the 'user_idx' in the `tweets` – and, hence, `popular_tweets` – table. Set this column as the index of the `popular_tweets` table, join the `unproductive_users` table, and sort the result by the number of `retweets`. Adding `[[]]` in line 2 has the effect that no columns of the `unproductive_users` table are actually added to the result, and `how='inner'` in line 3 has the effect that not all popular tweets are shown but only those whose creators are unproductive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9771881",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_tweets.set_index(keys='user_idx').join(\n",
    "    other = unproductive_users[[]], \n",
    "    how = 'inner'\n",
    ").sort_values(by='retweets', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a810c68c",
   "metadata": {},
   "source": [
    "The above query only uses one join. Other queries can also require multiple joins:\n",
    "\n",
    "#### How often is a given hashtag used over time?\n",
    "\n",
    "In this example, the result must be constructed from two tables linked by a relationship table, so we need two joins. There are several ways to build the query. You can start with either table. Here, we start with the relationship table. First, join the `tweets` table with just the 'timestamp' column to the `tweets_hashtags` table  via the tweet index (lines 1–2). Second, join the `hashtags` table, filtered to only contain hashtags from `hashtag_list`, to the resulting table via the hashtag index (lines 3–6). Line 7 shows how the date can be extracted from the timestamp (in other words, how hours, minutes, and seconds can be removed). Line 9 aggregates the data and counts the number of tweets that use a given hashtag on a given day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_list = ['coronavirus', 'covid19', 'hydroxychloroquine', 'vaccine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a4e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_hashtags_long = tweets_hashtags.set_index(keys='tweet_idx').join(\n",
    "    other = tweets[['timestamp']]\n",
    ").set_index(keys='hashtag_idx').join(\n",
    "    other = hashtags[hashtags['hashtag'].isin(hashtag_list)][['hashtag']], \n",
    "    how = 'inner'\n",
    ")\n",
    "days_hashtags_long['day'] = days_hashtags_long['timestamp'].dt.date\n",
    "del days_hashtags_long['timestamp']\n",
    "days_hashtags_long = days_hashtags_long.groupby(by=['day', 'hashtag']).size().reset_index(name='tweets')\n",
    "days_hashtags_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6130e2",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "<b>Additional resources</b>\n",
    "\n",
    "We have set up the TweetsCOV19 dataset in a relational database way and even stored it as an SQL database. It is also possible to use SQL commands to retrieve information from this dataset. Consult the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html) of Pandas' `read_sql()` method and the SQLAlchemy [tutorials](https://docs.sqlalchemy.org/en/14/tutorial/index.html) as entry points. See the following code cell for instant success (it requires the engine from above).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(sql='SELECT hashtag, tweets FROM hashtags', con=engine).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f443c48",
   "metadata": {},
   "source": [
    "### 2.2.2. Using NumPy to store data in n-dimensional arrays\n",
    "\n",
    "<img src='images/numpy.png' style='height: 100px; float: right; margin-left: 10px'>\n",
    "\n",
    "Pandas offers Series and DataFrames as native data structures, but its mathematical operations rely on the corresponding NumPy data structures: vectors and matrices. NumPy is purely numerical. The basic differences between Pandas and NumPy are:\n",
    "\n",
    "- NumPy provides n-dimensional data structures called **arrays**, not just matrices (2-dimensional data structures).\n",
    "- NumPy does not allow items to be lists.\n",
    "- NumPy does not have metadata (*i.e.*, the indices of arrays are not labeled)\n",
    "\n",
    "One use of NumPy is to store data in numerical form as part of a data processing pipeline. We will now go through two examples where data is stored in 2-dimensional and 3-dimensional arrays, respectively. For the **2-dimensional example**, consider that you want to create an array where the rows are days, the columns are hashtags, and the cells give the number of times that a hashtag is used in a day. We have just created the necessary `days_hashtags_long` table in the last querying example. We can make this table wide by using the [`pivot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html) method, which imitates the corresponding procedure in the Excel spreadsheet software:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3136847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_hashtags_wide = days_hashtags_long.pivot(index='day', columns='hashtag', values='tweets')\n",
    "days_hashtags_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064bdb52",
   "metadata": {},
   "source": [
    "We can transform the table into a NumPy array to get what we want. Appending `[:5]` will only show the first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d27be",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_hashtags_wide_array = days_hashtags_wide.to_numpy()\n",
    "days_hashtags_wide_array[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a3a991",
   "metadata": {},
   "source": [
    "What we see is the dataframe stripped of its metadata.\n",
    "\n",
    "For the **3-dimensional example**, obviously, we cannot use Pandas all the way since it only supports matrices. But we can still use it to process the data before we store it in an array. Consider that you want to create an array where the first dimension is days, the second dimension is mentioned users, the third dimension is sentiment categories (positive, negative, and average), and the cells give the mean sentiment scores of tweets in which users are mentioned in a day. Now we benefit from having normalized the data all the way through because the array can naturally be populated from the normalized tables. The full information we need is stored in five tables, so we would need four joins. However, since we use all three sentiment categories (and all three indices) from the `sentiments` table, we will spare the fourth join to the `sentiments` table and use the `sentiment_idx` for the third dimension instead. Note that we do not `drop` the 'tweet_idx' in the first join (lines 1–2) because we will also need it in line 6 to prepare the third join (lines 6–8). Line 9 extracts the date. Line 11 computes the mean sentiment scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c3d127",
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_list = ['realdonaldtrump', 'who', 'breitbartnews', 'cnn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_mentions_sentiments_long = tweets_mentions.set_index(keys='tweet_idx', drop=False).join(\n",
    "    other = tweets[['timestamp']]\n",
    ").set_index('mention_idx').join(\n",
    "    other = mentions[mentions['mention'].isin(mention_list)][['mention']], \n",
    "    how = 'inner'\n",
    ").set_index(keys='tweet_idx').join(\n",
    "    other = tweets_sentiments.set_index(keys='tweet_idx')\n",
    ").reset_index(drop=True)\n",
    "days_mentions_sentiments_long['day'] = days_mentions_sentiments_long['timestamp'].dt.date\n",
    "del days_mentions_sentiments_long['timestamp']\n",
    "days_mentions_sentiments_long = days_mentions_sentiments_long.groupby(['day', 'mention', 'sentiment_idx']).mean().round(4).reset_index()\n",
    "days_mentions_sentiments_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f771ad85",
   "metadata": {},
   "source": [
    "From this table, we now create the array, and the first three columns are the three dimensions. Since NumPy indexing is purely numerical, we must represent the days and mentions in this table by identifiers that start with 0 and are contiguous (just like the 'sentiment_idx'). In other words, we must make the first two variables categorical (if you know R, you will recognize the 'category' data type). Transform the first two columns using `astype('category')`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9e984",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_mentions_sentiments_long['day'] = days_mentions_sentiments_long['day'].astype('category')\n",
    "days_mentions_sentiments_long['mention'] = days_mentions_sentiments_long['mention'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9995e66",
   "metadata": {},
   "source": [
    "Before replacing the category labels by their numerical codes, we save them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da4d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_categories = days_mentions_sentiments_long['day'].cat.categories\n",
    "day_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd241249",
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_categories = days_mentions_sentiments_long['mention'].cat.categories\n",
    "mention_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4747d03e",
   "metadata": {},
   "source": [
    "It is useful to also store the sentiment categories in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_categories = sentiments['sentiment'].tolist()\n",
    "sentiment_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55f954",
   "metadata": {},
   "source": [
    "Now we can replace the categories by their codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e0a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_mentions_sentiments_long['day'] = days_mentions_sentiments_long['day'].cat.codes\n",
    "days_mentions_sentiments_long['mention'] = days_mentions_sentiments_long['mention'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5445fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_mentions_sentiments_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cb3a54",
   "metadata": {},
   "source": [
    "We have arrived at a purely numerical table where all cells are integers. This is a data structure that NumPy can work with. To transform the table into an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a86fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_mentions_sentiments_long_array = days_mentions_sentiments_long.to_numpy()\n",
    "days_mentions_sentiments_long_array[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3c646a",
   "metadata": {},
   "source": [
    "To make this long array wide, first, create an empty container (array) with a 3-dimensional `shape` where the first dimension is as long as there are days, the second dimension is as long as there are mentions, and the third dimension is as long as there are sentiment categories. Note that `days_mentions_sentiments_long_array` contains float variables even though the first three columns were integers in the dataframe. This is because an array (of the `ndarray` class) only permits one data type and the last column contains floats. To handle this situation, we use a trick: we create a container for string variables (`dtype='object'`) because integers and floats can be encoded as strings. Initially, each cell is Not a Number (NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db147f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_mentions_sentiments_wide_array = np.empty(shape=(len(day_categories), len(mention_categories), len(sentiments)), dtype='object')\n",
    "days_mentions_sentiments_wide_array[:] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f241a2",
   "metadata": {},
   "source": [
    "Then fill this array by using the first three columns of `days_mentions_sentiments_long_array` as indices – Pandas actually took over indexing from NumPy – for the wide array and filling the cells from the fourth column. Note that we transform the first three columns from 'object' to 'int' and the last to 'float'. Appending `[:5]` now means that the matrices of mentions and sentiments for the first five days are shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62549c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_mentions_sentiments_wide_array[days_mentions_sentiments_long_array[:, 0].astype('int'), days_mentions_sentiments_long_array[:, 1].astype('int'), days_mentions_sentiments_long_array[:, 2].astype('int')] = days_mentions_sentiments_long_array[:, 3].astype('float')\n",
    "days_mentions_sentiments_wide_array[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac17b80",
   "metadata": {},
   "source": [
    "We can slice a n-dimensional array any way we want. The matrix of days and sentiments for the first mentioned user 'breitbartnews' is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cffade",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mention_categories[0])\n",
    "days_mentions_sentiments_wide_array[:, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef166de5",
   "metadata": {},
   "source": [
    "### 2.2.3. Using SciPy to handle sparse data\n",
    "\n",
    "In many cases, we want to work with matrices and arrays mathematically. For example, to compute the logarithms of each cell in a Pandas Series, we must apply a NumPy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e2186",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log10(users['followers_max'].replace(to_replace=0, value=np.nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b34e8e2",
   "metadata": {},
   "source": [
    "NumPy's [`log10()`](https://numpy.org/doc/stable/reference/generated/numpy.log10.html) is a so-called [universal function](https://numpy.org/doc/stable/reference/ufuncs.html) that goes through the vector item by item. Universal functions are unaware of **data sparsity**. Data is sparse when it contains many zeros or NaNs, depending on the context. `log10()` tries to take the logarithm of each cell in a vector even if only one out of 1 million cells is larger than 0. Unawareness of data sparsity can easily cause your computer to run out of memory, for example, when algebraic operations like matrix multiplication are performed. Matrix multiplication is required to obtain **co-occurrence matrices**, for example, of hashtags in tweets. The raw TweetsCOV19 dataset is quite sparse, as you can tell by the many 'null;' entries. Besides eliminating redundancy, our transformation into a relational database has also made the data completely dense (*i.e.*, unsparse). Zeros, like 0 retweets, actually is a piece of information.\n",
    "\n",
    "<img src='images/scipy.png' style='height: 100px; float: right; margin-left: 10px'>\n",
    "\n",
    "Pandas is not well prepared to handle sparsity. While it offers [data structures for efficiently storing sparse data](https://pandas.pydata.org/docs/user_guide/sparse.html), sparse data processing is hardly developed. This is where SciPy, the other fundamental library for scientific computing comes in. SciPy is the standard library for handling [sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html) data, and we will now see how we can obtain the hashtag co-occurrence matrix for the TweetsCOV19 dataset.\n",
    "\n",
    "To be efficient for different kinds of operations, SciPy offers different sparse matrix formats. While choosing the right format is not so important in our case, it is quite important when data gets really big. COOrdinate matrices are fast for constructing sparse matrices. To construct the occurrence matrix `TH` (T for tweets, H for hashtags), the [`coo_matrix()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html) constructor takes as input a (cells, (rows, columns)) triplet.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "<b>Insight</b>\n",
    "\n",
    "It is one of the big benefits of **relationship tables**, as we are using them here, that they only contain the contiguous identifiers which are required by matrix manipulation routines like in SciPy. In the current example, all necessary information for constructing the sparse occurrence matrix lies in the `tweets_hashtags` relationship table.\n",
    "</div>\n",
    "\n",
    "`cells` is just a vector of 1s; each hashtag is used only once in a tweet (as a result of the `create_relationship_table()` function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bca067",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = tweets_hashtags['tweet_idx']\n",
    "cols = tweets_hashtags['hashtag_idx']\n",
    "cells = [1]*len(tweets_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef91b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "TH = coo_matrix((cells, (rows, cols)), shape=(len(tweets), len(hashtags)))\n",
    "TH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc885f",
   "metadata": {},
   "source": [
    "The technical summary is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4af455",
   "metadata": {},
   "outputs": [],
   "source": [
    "TH.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1828a74",
   "metadata": {},
   "source": [
    "Easy to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c607ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948a803f",
   "metadata": {},
   "source": [
    "The index pairs of a sparse matrix can be accessed via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186daeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tweet/row indices:', TH.nonzero()[0])\n",
    "print('Hashtag/column indices:', TH.nonzero()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946cb20a",
   "metadata": {},
   "source": [
    "#### Getting the co-occurrence matrix\n",
    "\n",
    "Given an occurrence matrix $TH$ with tweet indices as rows and hashtag indices as columns, the co-occurrence matrix of hashtags co-occurring in tweets is $Co=HT\\cdot TH$ where $HT$ is the transpose of $TH$ (*i.e.*, with hashtag indices as rows and tweet indices as columns) and $\\cdot$\n",
    "means that $HT$ is [multiplied](https://en.wikipedia.org/wiki/Matrix_multiplication) by $TH$ (<a href='#destination1'>Batagelj & Cerinsěk, 2013</a>). To do fast vector operations (*e.g.*, matrix multiplication), it is recommended to transform the matrix into a Compressed Sparse Row (CSR) [`csr_matrix()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) before multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeba66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TH = TH.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e822d32",
   "metadata": {},
   "source": [
    "If there had been duplicate entries of the COO format they would have been summed during the conversion.\n",
    "\n",
    "To get the co-occurrence matrix `Co` (where `TH.T` is the transpose of `TH` and [`dot()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.dot.html) is the method for matrix multiplication):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c8d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "Co = TH.T.dot(other=TH)\n",
    "Co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f471445c",
   "metadata": {},
   "source": [
    "Note that the resulting matrix is in the Compressed Sparse Column (CSC) format.\n",
    "\n",
    "Sparse matrices can be accessed via indices like arrays. Use [`todense()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.todense.html) to display the sparse matrix in wide (but dense), not long format. The first five rows and columns show that the matrix is symmetric (*i.e.*, has redundant information in the upper and lower triangular portions) and that the diagonal contains the counts of how often a hashtag is used in all tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d97eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Co[:5, :5].todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f4cd2",
   "metadata": {},
   "source": [
    "To eliminate the redundancy, us the [`triu()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.triu.html) method to extract just the matrix's upper triangular portion, including the diagonal. This operation is fastest when the matrix is in the COO format. Hence, transform it `.tocoo()` first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd34422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import triu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ddcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Co = triu(Co.tocoo())\n",
    "Co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb4b5c7",
   "metadata": {},
   "source": [
    "In the SciPy version we are using, the diagonal cannot be removed, only set to 0 (fastest in COO format). The cell is commented out because we want to keep the diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a4a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Co.setdiag(values=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06fba2c",
   "metadata": {},
   "source": [
    "The sparse matrix can be saved as a Pandas `hashtag_cooccurrences` table by transforming the row indexes (line 3), column indexes (line 4), and cells (line 5) to Series and concatenating them (if the diagonal values have been set to 0 they still show up as rows with NaN indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5418cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_cooccurrences = pd.concat(\n",
    "    objs=[\n",
    "        pd.Series(Co.nonzero()[0]), \n",
    "        pd.Series(Co.nonzero()[1]), \n",
    "        pd.Series(Co.data)\n",
    "    ], \n",
    "    axis=1\n",
    ")\n",
    "hashtag_cooccurrences.columns = ['hashtag_idx_i', 'hashtag_idx_j', 'cooccurrence']\n",
    "hashtag_cooccurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c984f4",
   "metadata": {},
   "source": [
    "#### Getting the normalized co-occurrence matrix\n",
    "\n",
    "You can also compute normalized co-occurrence scores. The idea is to give a hashtag a smaller weight the more it shares the \"attention\" it gets with other hashtags in a tweet. For example, if three hashtags are used in a tweet, each hashtag gets an attention or relationship weight on 1/3. These weights can be obtained via row normalization (of CSR matrices). For that purpose, `TH` must be normalized and the result stored as `N`. The [scikit-learn](https://scikit-learn.org/) library's [`normalize()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) function can be used for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a1fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(TH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1fed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d971ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = normalize(TH, norm='l1', axis=1)\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64539300",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea0abc1",
   "metadata": {},
   "source": [
    "The normalized co-occurrence matrix of hashtags co-occurring in tweets is $Cn=HT\\cdot N$ (<a href='#destination1'>Batagelj & Cerinsěk, 2013</a>):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c23d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cn = TH.T.dot(other=N)\n",
    "Cn = triu(Cn.tocoo())\n",
    "#Cn.setdiag(values=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035aa46",
   "metadata": {},
   "source": [
    "Attach the normalized co-occurrence scores to the `hashtag_cooccurrences` table (line 1), remove the diagonal rows if you want (line 2), and sort the table (line 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e432451",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_cooccurrences['cooccurrence_norm'] = pd.Series(data=Cn.data.round(4))\n",
    "#hashtag_cooccurrences = hashtag_cooccurrences[hashtag_cooccurrences['cooccurrence'] > 0]\n",
    "hashtag_cooccurrences = hashtag_cooccurrences.sort_values(by=['hashtag_idx_i', 'hashtag_idx_j']).reset_index(drop=True)\n",
    "hashtag_cooccurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25161d92",
   "metadata": {},
   "source": [
    "Networks can be constructed directly from such co-occurrence tables, as we will see in [Session 7: Network analysis](). For now, we just save the table to the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00862145",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_cooccurrences.to_csv(path_or_buf='../data/TweetsCOV19/hashtag_cooccurrences.tsv', sep='\\t', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012a7cb0",
   "metadata": {},
   "source": [
    "## 2.3. Exploring the data visually\n",
    "\n",
    "<img src='images/matplotlib.png' style='height: 50px; float: right; margin-left: 10px'>\n",
    "\n",
    "We have claimed that Python does not need to hide behind R and the tidyverse. We hope that the previous subsections have demonstrated Python's appeal for the data management and processing steps. Now, you will learn how you can explore the data visually and produce publication-ready figures. [Matplotlib](https://matplotlib.org/) is Python's basic library for creating visualizations. Matplotlib gives you many options regarding kinds of plots and how to style them, but achieving what you want can be cumbersome. The [Seaborn](https://seaborn.pydata.org/) library is an easier-to-use interface \"for drawing attractive and informative statistical graphics\" that builds on Matplotlib and integrates closely with Pandas data structures.\n",
    "\n",
    "Plotting the number of tweets over time using plain Matplotlib, you will notice a downward trend and a weekly rhythm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6417fa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d522abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_over_time = tweets.groupby(tweets['timestamp'].dt.date).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6ad274",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12, 3])\n",
    "plt.plot(tweets_over_time)\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55696b7",
   "metadata": {},
   "source": [
    "<img src='images/seaborn.png' style='width: 100px; float: right; margin-left: 10px'>\n",
    "\n",
    "With `set_theme()` from Seaborn, you can set a visual style that will be used even if you do plain Matplotlib plotting. You can choose from five styles: 'darkgrid', 'whitegrid', 'dark', 'white', and 'ticks'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30062046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99e03ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72143a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12, 2])\n",
    "plt.plot(tweets_over_time)\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26903f2e",
   "metadata": {},
   "source": [
    "The TweetsCOV19 webpage shows [plots](https://data.gesis.org/tweetscov19/#Statistics) about the frequency development of selected hashtags up until April 2020. Above, we have collected the usage statistics for hashtags listed in `hashtag_list` and stored them in `days_hashtags_wide_array`. We can easily create figures for May 2020 from that 2-dimensional array. The following cell loops through the array by iterating through the `hashtag_indices` defined in line 1. Since the array that holds the y values is stripped of labels, we must take the x labels from the corresponding `days_hashtags_wide` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_indices = [0, 1]\n",
    "\n",
    "plt.figure(figsize=[12, 2])\n",
    "for hashtag_index in hashtag_indices:\n",
    "    plt.plot(days_hashtags_wide.index, days_hashtags_wide_array[:, hashtag_index], label='#' + hashtag_list[hashtag_index])\n",
    "plt.legend()\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a213b62",
   "metadata": {},
   "source": [
    "It is equally simple to plot data that lives in 3-dimensional arrays. The TweetsCOV19 webpage also shows time trends of the mean sentiment category scores of tweets in which prominent Twitter users are mentioned up until April 2020. We have retrieved the May 2020 data for mentions in `mention_list` and stored it in `days_mentions_sentiments_wide_array`. This time we make a first loop through all mentions from `mention_categories` (line 1) and a second loop through all sentiments from `sentiment_categories` (line 3). This time, we use datetime objects stored in `day_categories` as x values and draw the y values from the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6b73ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mention_index in range(len(mention_categories)):\n",
    "    plt.figure(figsize=[12, 2])\n",
    "    for sentiment_index in range(len(sentiment_categories)):\n",
    "        plt.plot(day_categories, days_mentions_sentiments_wide_array[:, mention_index, sentiment_index], label=sentiment_categories[sentiment_index])\n",
    "    plt.legend()\n",
    "    plt.title('@' + mention_categories[mention_index])\n",
    "    plt.ylabel('Sentiment')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b865695b",
   "metadata": {},
   "source": [
    "Except for @who, the average sentiment is slightly negative. This means that the language of tweets that mention these users tends to be laden with negative emotions – it does not mean that negative sentiments are voiced about the mentioned users.\n",
    "\n",
    "#### Distributions\n",
    "\n",
    "Social media data is known to often be very skewed (*i.e.*, not normally distributed). Indeed, we have already seen that some users have tens of millions of followers. For quantitative analysis, especially for the kinds of analyses performed in [Session 9: Statistics & supervised machine learning](), it is very important to know how variables are distributed. Boxplots can be a first step to assessing distributions. In the following, we are interested in the distributions of the 'followers_max' and 'friends_max' variables in the `users` table. To produce Seaborn boxplots, `melt()` the subtable with those two columns (*i.e.*, make it long), ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_friends = pd.melt(users[['followers_max', 'friends_max']])\n",
    "followers_friends.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a00a52",
   "metadata": {},
   "source": [
    "then plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe4418",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[3, 3])\n",
    "sns.boxplot(x='variable', y='value', data=followers_friends)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ade1b",
   "metadata": {},
   "source": [
    "Both variables are extremely skewed (note the logarithmic y-axis). Often such variables are transformed into their logarithm to make them behave better (add 1 before taking the log, then users with a value of 0 will keep it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb58eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_users = users[['followers_max', 'friends_max']].copy()\n",
    "log_users = np.log10(log_users[['followers_max', 'friends_max']] + 1).round(4)\n",
    "log_users.columns = ['log_followers_max', 'log_friends_max']\n",
    "log_users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f9b05e",
   "metadata": {},
   "source": [
    "Seaborn's [`histplot()`](https://seaborn.pydata.org/generated/seaborn.histplot.html) creates histograms and allows to add [kernel density estimates](https://en.wikipedia.org/wiki/Kernel_density_estimation). In line 3, we take a random sample from the data because density estimation takes quite long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6defd15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[3, 3])\n",
    "sns.histplot(\n",
    "    data=log_users.sample(n=10000, random_state=42), \n",
    "    bins=20, \n",
    "    kde=True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68f6b7c",
   "metadata": {},
   "source": [
    "Both logged variables look normally distributed. That means it is a good hypothesis that the untransformed variables are [lognormally](https://en.wikipedia.org/wiki/Log-normal_distribution) distributed. We can test this hypothesis with the [powerlaw](https://github.com/jeffalstott/powerlaw) library. First, we fit a number of candidate functions to the whole range (`xmin=1`) of the data using [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import powerlaw\n",
    "powerlaw.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f82c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_followers_max = powerlaw.Fit(data=users['followers_max'], xmin=1)\n",
    "fit_friends_max = powerlaw.Fit(data=users['friends_max'], xmin=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43381407",
   "metadata": {},
   "source": [
    "We plot two of these candidate functions, the lognormal and a power law:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553c0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[3, 3])\n",
    "fig = fit_followers_max.plot_pdf(marker='o', linestyle='', label='data')\n",
    "fit_followers_max.lognormal.plot_pdf(linestyle='--', ax=fig, label='lognormal')\n",
    "fit_followers_max.power_law.plot_pdf(linestyle='--', ax=fig, label='power_law')\n",
    "plt.legend()\n",
    "plt.xlabel('followers_max')\n",
    "plt.ylabel('PDF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d2523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[3, 3])\n",
    "fig = fit_friends_max.plot_pdf(marker='o', linestyle='', label='data')\n",
    "fit_friends_max.lognormal.plot_pdf(linestyle='--', ax=fig, label='lognormal')\n",
    "fit_friends_max.power_law.plot_pdf(linestyle='--', ax=fig, label='power_law')\n",
    "plt.legend()\n",
    "plt.xlabel('friends_max')\n",
    "plt.ylabel('PDF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99e028",
   "metadata": {},
   "source": [
    "The lognormal seems to be a better fit for the data in both cases. We can again test this using loglikelihood ratios. In both cases, the ratio (the first value in the brackets) is extremely large and significantly (the second value in the bracket) different from 0. A large significant value means that the first distribution, in these cases the lognormal distribution, is a better fit to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca26542",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_followers_max.distribution_compare('lognormal', 'power_law')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fcc5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_friends_max.distribution_compare('lognormal', 'power_law')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a9434c",
   "metadata": {},
   "source": [
    "We have found that both variables are, in fact, lognormally distributed.\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "<b>Additional resources</b>\n",
    "\n",
    "These results from fitting functions to the data mean that the variables are certainly not **power law distributions**. Knowing about power law behavior is important because, depending on their exponent, power laws do nto have characteristic sample variance or even sample mean, which is statistically problematic. To learn about the importance of power law distributions, consult <a href='#destination2'>Clauset *et al.* (2009)</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4737f29e",
   "metadata": {},
   "source": [
    "#### Bivariate relationships\n",
    "\n",
    "Identifying bivariate relationships or correlations is another part of data exploration and can be done visually. Seaborn's [`jointplot()`](https://seaborn.pydata.org/generated/seaborn.jointplot.html) function creates joint and marginal views on two variables. There are four different `kind`s of views. Here, we show histograms (line 6): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b88deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = sns.jointplot(\n",
    "    data = log_users.sample(n=10000, random_state=42), \n",
    "    x = 'log_followers_max', \n",
    "    y = 'log_friends_max', \n",
    "    kind = 'hist', \n",
    "    joint_kws = dict(bins=40), \n",
    "    marginal_kws = dict(bins=20)\n",
    ")\n",
    "plot.fig.set_figwidth(3)\n",
    "plot.fig.set_figheight(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff2600",
   "metadata": {},
   "source": [
    "If you have many variables whose relationships you want to explore, Seaborn offers the [`pairplot()`]() function. The diagonal of such a plot will be filled with the univariate distribution, and the kind of view can be set separately for the univariate (`diag_kind` parameter) and bivariate cases. This time, we plot the relationships for the four numerical variables of the `tweets` table (excluding the sentiment scores). Since they are very skewed, we log them first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c96888",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_tweets = tweets[['followers', 'friends', 'retweets', 'favorites']].copy()\n",
    "log_tweets = np.log10(log_tweets[['followers', 'friends', 'retweets', 'favorites']] + 1)\n",
    "log_tweets.columns = ['log_followers', 'log_friends', 'log_retweets', 'log_favorites']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f3949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW CHANGE THE NUMBER OF BINS IN AND OFF THE DIAGONAL?\n",
    "\n",
    "plot = sns.pairplot(\n",
    "    data = log_tweets.sample(n=10000, random_state=42), \n",
    "    height = 2, \n",
    "    kind = 'hist', \n",
    "    diag_kind = 'hist', \n",
    "    #pair_kws = dict(bins=40), \n",
    "    #diag_kws = dict(bins=20)\n",
    ")\n",
    "plot.fig.set_figwidth(6)\n",
    "plot.fig.set_figheight(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522958f8",
   "metadata": {},
   "source": [
    "This concludes Session 2: Data handling & visualization. Some of the datasets created here will be used in later sessions. Now that we have an idea of how to manage, process, and explore our data in a research pipeline, we move to the first step of the data life cycle. [Session 3: API Harvesting]() and [Session 4: Web scraping]() are dedicated to data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a71b90",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a id='destination1'></a>\n",
    "Batagelj, V. & Cerinsěk, M. (2013). On bibliographic networks. *Scientometrics* 96:845–864. https://doi.org/10.1007/s11192-012-0940-1.\n",
    "\n",
    "<a id='destination2'></a>\n",
    "Clauset, A., Shalizi, C. R., & Newman, M. E. J. (2009). \"Power-law distributions in empirical data\". *SIAM Review* 51:661–703. https://doi.org/10.1137/070710111.\n",
    "\n",
    "<a id='destination3'></a>\n",
    "Dimitrov, D., Baran, E., Fafalios, P., Yu, R., Zhu, X., Zloch, M., & Dietze, S. (2020). TweetsCOV19 - A Knowledge Base of Semantically Annotated Tweets about the COVID-19 Pandemic. In: *CIKM '20: Proceedings of the 29th ACM International Conference on Information & Knowledge Management* (p. 2991–2998). https://doi.org/10.1145/3340531.3412765.\n",
    "\n",
    "<a id='destination4'></a>\n",
    "Fafalios, P., Iosifidis, V., Ntoutsi, E., & Dietze, S. (2018). TweetsKB: A Public and Large-Scale RDF Corpus of Annotated Tweets. In: *The Semantic Web. ESWC 2018. Lecture Notes in Computer Science*, vol 10843. Springer, Cham. https://doi.org/10.1007/978-3-319-93417-4_12.\n",
    "\n",
    "<a id='destination5'></a>\n",
    "Weidmann, N. B. (2022). *Data Management for Social Scientists: From Files to Databases*. Cambridge University Press.\n",
    "\n",
    "<a id='destination6'></a>\n",
    "Wikipedia (2022). Twitter. https://en.wikipedia.org/wiki/Twitter. Retrieved 01.12.2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbb79d6",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "<b>Document information</b>\n",
    "\n",
    "Contact and main author: Haiko Lietz\n",
    "\n",
    "Contributors: Pouria Mirelmi & N. Gizem Bacaksizlar Turbic\n",
    "\n",
    "Acknowledgements: ...\n",
    "\n",
    "Version date: 18. January 2023\n",
    "\n",
    "License: ...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662b5253",
   "metadata": {},
   "source": [
    "#### Notes to be removed before publication\n",
    "\n",
    "- Reviewers: Olya & Helena\n",
    "- Finish green boxes\n",
    "- Pandas low_memory option\n",
    "- use correct docu versions\n",
    "- clean hashtags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
