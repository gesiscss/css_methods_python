{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='social_comquant.png' style='height: 60px; float: left'>\n",
    "<img src='gesis.png' style='height: 60px; float: right; margin-right: 40px'>\n",
    "<img src='isi.png' style='height: 60px; float: right; margin-right: 20px'>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Section C: Data preprocessing methods | Session 6: Natural Language Processing\n",
    "\n",
    "### Authors: N. Gizem Bacaksizlar Turbic, Haiko Lietz, Pouria Mirelmi, and ..\n",
    "\n",
    "### Date: ? October 2022\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "The field of study that focuses on the interactions between human language and computers is called natural language processing (NLP).\n",
    "\n",
    "NLP is a field of artificial intelligence in which computers analyze, understand, and derive meaning from human language in a smart and useful way. NLP systems are used exploiting the signals in our language used to predict all of the aforementioned features: people’s age (Nguyen et al., 2011; Rosenthal & McKeown, 2011), gender (Alowibdi et al., 2013; Ciot et al., 2013; Liu & Ruths, 2013), personality (Park et al., 2015), job title (Preoţiuc-Pietro et al., 2015a), income (Preoţiuc-Pietro et al., 2015b), and much more (Volkova et al., 2014, 2015).\n",
    "\n",
    "In NLP, word embeddings have been at the forefront of this progress, which has expanded to include flexible model architectures (Hovy, 2021). The most publicly visible example of this shift is probably the translation quality of services like Google Translate (Wu et al., 2016).\n",
    "\n",
    "NLP has a wide number of applications in the real world.\n",
    "\n",
    "- Sentiment Analysis\n",
    "- Speech Recognition\n",
    "- Text Classification\n",
    "- Machine Translation\n",
    "- Semantic Search\n",
    "- News/article Summarization\n",
    "- Answering Questions\n",
    "\n",
    "This session will help you understand the basic and advanced NLP concepts and show you how to implement using the most advanced and popular NLP libraries – <a href=\"https://www.nltk.org/\">`NLTK`</a>, <a href=\"https://spacy.io/\">`spaCy`</a>, <a href=\"https://pypi.org/project/gensim/\">`Gensim`</a>, and <a href=\"https://huggingface.co/\">`Huggingface`</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: Should we start with regex here? Or should it be in the second session?\n",
    "https://github.com/gesiscss/css_methods_python/blob/main/a_introduction/2_data_handling_and_visualization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfg6soCx1CND"
   },
   "source": [
    "## 1.1. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2RhrTxx1J4r",
    "outputId": "897cffeb-bb57-45d6-b96b-e3313d9ba2fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\bacaksgm\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: click in c:\\users\\bacaksgm\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\bacaksgm\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\users\\bacaksgm\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bacaksgm\\anaconda3\\lib\\site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bacaksgm\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# Lets first make sure that nltk is installed.\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bacaksgm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. spaCy\n",
    "\n",
    "<a href=\"https://spacy.io/\">`spaCy`</a> is the most trending and advanced library for implementing NLP today. It has distinct features that provide clear advantage for processing text data and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, make sure that spacy is installed and uncomment the below code line\n",
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy and create nlp object with loading the models and data for English Language\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') # if you get error, run python -m spacy download en on your Anaconda prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. gensim\n",
    "\n",
    "<a href=\"https://pypi.org/project/gensim/\">`Gensim`</a> was developed for topic modelling, which supports the NLP tasks like Word Embedding, text summarization and many others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, make sure that gensim is installed and uncomment the code line below\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Transformers\n",
    "\n",
    "It was developed by <a href=\"https://huggingface.co/\">`HuggingFace`</a> and provides state of the art models. It is an advanced library known for the transformer modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package if you haven't done so, please uncomment the code line below\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QkINdLLSGe5"
   },
   "source": [
    "# 2. Text Preprocessing methods\n",
    "\n",
    "To refer to the entire collection of documents/observations, we use the word corpus (plural corpora). The raw text data often referred to as *text corpus* has punctuations, suffices, and stop words that do not give us important information. To have more useful information for NLP tasks, Text Preprocessing involves preparing the text corpus.\n",
    "\n",
    "Let's walk through some basic steps of preprocessing of a raw text corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A text can be converted into nlp object of spaCy as it was shown in the earlier step (1.2. spaCy).\n",
    "# Convert raw text into a spaCy object\n",
    "raw_text = 'Today is a great day with learning NLP, such a powerful tool!'\n",
    "text_doc = nlp(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYJ0Y4OAj2gq"
   },
   "source": [
    "## 2.1. Word Descriptors\n",
    "\n",
    "### 2.1.1 Tokens and splitting \n",
    "\n",
    "### What is a Token?\n",
    "\n",
    "The set of all the unique terms in our data is called the vocabulary. Each element in this set is called a type. Each occurrence of a type in the data is called a token. \n",
    "\n",
    "Let's practice: Our sentence “Today is a great day with learning NLP, such a power tool!”, has 14 tokens but only 13 types (namely, 'Today', 'is', 'a', 'great', 'day', 'with', 'learning', 'NLP', ',', 'such', 'a', 'powerful', 'tool', '!'). Note that types can also include punctuation marks and multiword expressions.\n",
    "\n",
    "In other words, the words of a text document/file separated by spaces and punctuation are called as tokens.\n",
    "\n",
    "### What is a Tokenization?\n",
    "The process of extracting tokens from a text file/document is referred as tokenization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today\n",
      "is\n",
      "a\n",
      "great\n",
      "day\n",
      "with\n",
      "learning\n",
      "NLP\n",
      ",\n",
      "such\n",
      "a\n",
      "powerful\n",
      "tool\n",
      "!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can print text of the tokens by accessing token.text while using spaCy\n",
    "# printing tokens\n",
    "tokens = []\n",
    "for token in text_doc:\n",
    "    print(token.text)\n",
    "    tokens.append(token.text)\n",
    "    \n",
    "len(tokens)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today True\n",
      "is True\n",
      "a True\n",
      "great True\n",
      "day True\n",
      "with True\n",
      "learning True\n",
      "NLP True\n",
      ", False\n",
      "such True\n",
      "a True\n",
      "powerful True\n",
      "tool True\n",
      "! False\n"
     ]
    }
   ],
   "source": [
    "# What if we want to find a particular token with alphabetic characters?\n",
    "\n",
    "for token in text_doc:\n",
    "    print(token.text, token.is_alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text         Alpha Space Stop Punct\n",
      "Today      : True False False False\n",
      "is         : True False True False\n",
      "a          : True False True False\n",
      "great      : True False False False\n",
      "day        : True False False False\n",
      "with       : True False True False\n",
      "learning   : True False False False\n",
      "NLP        : True False False False\n",
      ",          : False False False True\n",
      "such       : True False True False\n",
      "a          : True False True False\n",
      "powerful   : True False False False\n",
      "tool       : True False False False\n",
      "!          : False False False True\n"
     ]
    }
   ],
   "source": [
    "# What if we want to know if the particular token is space, or a stop word or punctuation?\n",
    "print(\"Text\".ljust(10), ' ', \"Alpha\", \"Space\", \"Stop\", \"Punct\")\n",
    "for token in text_doc:\n",
    "    print(token.text.ljust(10), ':', token.is_alpha, token.is_space, token.is_stop, token.is_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try tokenization with nltk.\n",
    "nltk doesn't come fully installed, you need to use nltk.download() to use some of the missing functions, and we will also use spaCy to show similar features that these library have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jAcPU658j67u",
    "outputId": "9b0cccfc-f7f9-4d29-c10e-b09a23b066da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', '10:30', \"o'clock\", 'on', 'Monday', 'mornings', ',', 'we', 'have', 'Social', 'ComQuant', 'meetings', '.', 'Let', \"'s\", 'have', 'our', 'meeting', 'another', 'time', '.']\n",
      "\n",
      "At 10:30 o'clock on Monday mornings , we have Social ComQuant meetings . Let 's have our meeting another time .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "sentence = \"At 10:30 o'clock on Monday mornings, we have Social ComQuant meetings. Let's have our meeting another time.\"\n",
    "tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "print(tokens)\n",
    "print()\n",
    "print(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "lLQWxbNM6Ldw",
    "outputId": "3759aeb6-30f7-4209-b9dd-7ecba72153a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AT 10:30 O'CLOCK ON MONDAY MORNINGS, WE HAVE SOCIAL COMQUANT MEETINGS. LET'S HAVE OUR MEETING ANOTHER TIME.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert string to upper case characters\n",
    "sentence.upper() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ZMlnCTpP6To_",
    "outputId": "e33e6074-0bb1-4776-fb03-455e2e9c35c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"at 10:30 o'clock on monday mornings, we have social comquant meetings. let's have our meeting another time.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert string to lower case characters\n",
    "sentence.lower() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvMwNW877fZj"
   },
   "source": [
    "#### Let's remove punctuation and stop words. But, wait, what do we mean by stop words?\n",
    "Stop words are a set of commonly used words in any language. For example, in English, “the”, “is”, and “and” would easily qualify as stop words. In NLP and text mining applications, stop words are used to eliminate unimportant words, allowing applications to focus on the important words instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dRBUQU44rIQN",
    "outputId": "a1edf0b6-0b55-4e91-ffa9-16534add66e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bacaksgm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Access the built-in stop words in nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "herein\n",
      "namely\n",
      "see\n",
      "and\n",
      "can\n"
     ]
    }
   ],
   "source": [
    "# Access the built-in stop words in Spacy\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "list_stopwords=list(stopwords)\n",
    "\n",
    "# printing a fraction of the list through indexing\n",
    "for word in list_stopwords[:5]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfa4ueiY7eWK",
    "outputId": "946a1cb7-1543-46d2-f4ee-b4df22db949b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today\n",
      "great\n",
      "day\n",
      "learning\n",
      "NLP\n",
      ",\n",
      "powerful\n",
      "tool\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Filter out the stopwords\n",
    "filtered_text= [token for token in text_doc if not token.is_stop]\n",
    "\n",
    "# Count the tokens after removal of stopwords\n",
    "token_count_without_stopwords=0\n",
    "for token in filtered_text:\n",
    "    print(token)\n",
    "    token_count_without_stopwords+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today\n",
      "great\n",
      "day\n",
      "learning\n",
      "NLP\n",
      "powerful\n",
      "tool\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuations\n",
    "filtered_text=[token for token in filtered_text if not token.is_punct]\n",
    "\n",
    "token_count_without_stop_and_punct=0\n",
    "\n",
    "# Counting the new no of tokens\n",
    "for token in filtered_text:\n",
    "    print(token)\n",
    "    token_count_without_stop_and_punct += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fxyRQ7X399J"
   },
   "source": [
    "### 2.1.2. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look up a word in a dictionary, we usually just look for the base form. This dictionary base form is called the lemma.\n",
    "For instance, we might see forms like “go,” “goes,” “went,”, “gone,” or “going” and we look up dictionary in a lemmatized form, such as \"go\" (Hovy, 2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rXdJ76j4Byh",
    "outputId": "ae40be62-74a3-48b4-ad2a-0a587adaddd0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bacaksgm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', '10:30', \"o'clock\", 'on', 'Monday', 'morning', ',', 'we', 'have', 'Social', 'ComQuant', 'meeting', '.', 'Let', \"'s\", 'have', 'our', 'meeting', 'another', 'time', '.']\n"
     ]
    }
   ],
   "source": [
    "# Let's give an example with nltk\n",
    "# import nltk already imported\n",
    "# import the lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Remember our sentence with the Social ComQuant meeting.\n",
    "sentence = \"At 10:30 o'clock on Monday mornings, we have Social ComQuant meetings. Let's have our meeting another time.\"\n",
    "\n",
    "WNL = WordNetLemmatizer() # declaring an instance of our preprocessor.\n",
    "tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "\n",
    "lemmatized_tokens = []\n",
    "for t in tokens:\n",
    "    t_lemma = WNL.lemmatize(t)\n",
    "    lemmatized_tokens.append(t_lemma)\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What -----> what\n",
      "about -----> about\n",
      "going -----> go\n",
      "to -----> to\n",
      "festivals -----> festival\n",
      "? -----> ?\n",
      "You -----> you\n",
      "said -----> say\n",
      "you -----> you\n",
      "like -----> like\n",
      "dancing -----> dance\n",
      ". -----> .\n"
     ]
    }
   ],
   "source": [
    "# Let's give an example with spaCy\n",
    "new_sentence = \"What about going to festivals? You said you like dancing.\"\n",
    "\n",
    "text_doc = nlp(new_sentence)\n",
    "for token in text_doc:\n",
    "    print(token.text, '----->', token.lemma_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmSoE-YBnDek"
   },
   "source": [
    "### 2.1.3. Stemming \n",
    "\n",
    "From Hovy's book: \"Rather than reducing a word to the lemma, we strip away everything but the irreducible morphological core (the stem). For example, for a word like “anticonstitutionalism”, which can be analyzed as “anti+constitut+ion+al+ism,” we remove everything but “constitut.” The most famous and commonly used stemming tool is based on the algorithm developed by Porter (1980). For each language, it defines a number of suffixes (i.e., word endings) and the order in which they should be removed or replaced. By repeatedly applying these actions, we reduce all words to their stems. In our example, all words derive from the stem “constitut–” by attaching different endings. Again, a version of the Porter stemmer is already available in Python, in the nltk library (Loper & Bird, 2002), but we have to specify the language.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G8NdEilRnC8i",
    "outputId": "817f4d81-b30a-41d6-e9ed-e7bcdd187467"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everi weekday even , our editor guid you through the biggest stori of the day , help you discov new idea , and surpris you with moment of delight . subscrib to get thi deliv to your inbox ..\n"
     ]
    }
   ],
   "source": [
    "# import stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "sentence = \"\"\"Every weekday evening, our editors guide you through the biggest stories of the day,\n",
    "help you discover new ideas, and surprise you with moments of delight. Subscribe to get this delivered to your inbox..\"\"\"\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "\n",
    "stemmed_tokens = []\n",
    "# for loop for each token in stemmed_tokens\n",
    "for token in tokens:\n",
    "    # add the stemmed version of the token to the new list\n",
    "    stemmed_tokens.append(stemmer.stem(token))\n",
    "# join a list of tokens into one string\n",
    "stemmed_sentence = \" \".join(stemmed_tokens) \n",
    "print(stemmed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4. n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.5. Word frequency analysis (not sure if this should be here?)\n",
    "## 2.1.6 regex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Parts of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Named entities\n",
    "NER with NLTK\n",
    "\n",
    "NER with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. NLP tasks implementation\n",
    "\n",
    "Text summarization (gensim + spaCy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Huggingface’s transformers: State-of-the-art NLP\n",
    "\n",
    "Currently, Hugging face is supported by Pytorch and tensorflow 2.0. We can use transformers of Hugging Face to implement Summarization, Text Generation, Language Trasnlation, ChatBot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you haven't installed yet, please install torch first\n",
    "# !pip install torch\n",
    "import torch\n",
    "# !pip install transformers --upgrade\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sentiment Analysis with BERT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Hovy, D. (2020). Text analysis in Python for social scientists: Discovery and exploration. Cambridge University Press.\n",
    "\n",
    "Hovy, D. (2021). Text Analysis in Python for Social Scientists: Prediction and Classification. Cambridge University Press.\n",
    "\n",
    "https://www.nltk.org/book/ch01.html\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/natural-language-processing-guide/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Z0-GgEFeNJbn"
   ],
   "name": "Day 1: Introduction to Jupyter Notebook and Text Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
