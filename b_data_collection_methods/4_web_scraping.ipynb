{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26acf247",
   "metadata": {},
   "source": [
    "<img src='images/gesis.png' style='height: 50px; float: left'>\n",
    "<img src='images/social_comquant.png' style='height: 50px; float: left; margin-left: 40px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d67c10",
   "metadata": {},
   "source": [
    "## Introduction to Computational Social Science methods with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a8570",
   "metadata": {},
   "source": [
    "# Session 4: Web scraping\n",
    "\n",
    "**Data collection** is a procedure of gathering information from subjects (all relevant sources), measuring and analyzing insights for research using various techniques, such as web scraping or API harvesting. Researchers can evaluate their research questions and hypotheses on the basis of collected data. In most cases, data collection is the primary and most important step for research, irrespective of the field of study. The approach of data collection varies for different fields of study, depending on the required information.\n",
    "\n",
    "<img src=\"./images/webscrape.png\"  width=\"350\" height = \"350\" align=\"center\"/>\n",
    "\n",
    "Digital behavioral data from the internet is an absolutely massive source of data which we can access by various ways such as connecting APIs (see [Session 3: API harvesting](https://github.com/gesiscss/css_methods_python/blob/main/b_data_collection_methods/3_api_harvesting.ipynb)). In addition to APIs, web scraping opens up another way to access **Digital Behavioral Data** that it is not available in convenient CSV exports or APIs. Also, APIs are not offered by every websites, and they might not always provide every piece of information we need. Therefore, scraping can be the only solution to extract data from websites. \n",
    "\n",
    "Websites are often valuable sources of data; for example, [weather forecasts](https://www.accuweather.com/), [articles on news sites](https://www.washingtonpost.com/), and [posts on forums](https://quora.com/). To access these sorts of information on webpages, we can use web scraping. Different use cases are studied after collecting data by web scarping to understand hospitality through online reviews, compare food prices,  aggregate news and bank accounts, and build datasets which are not available otherwise (Han et al., 2021; Hillen 2019)..\n",
    "\n",
    "While web scraping is one of the common ways of collecting data from websites, a lot of websites offer APIs to access the public data that they host on their website. This is to avoid unnecessary traffic on the websites. However, even though we have access to these API, as researchers, we should not forget to respect API access rules and always read the **Terms of Use** documents before collecting data. In order to access APIs, you first need to create an account and apply to have a developer account on the platform that you want to work on. With this developer account, platforms provide you KEYS (e.g., secret, public, or access) to authenticate their system. The more practical information and hands on examples on API harvesting can be found in [Session 3: API harvesting](https://github.com/gesiscss/css_methods_python/blob/main/b_data_collection_methods/3_api_harvesting.ipynb).\n",
    "\n",
    "<div class='alert alert-block alert-success'>\n",
    "<b>In this session</b>, \n",
    "\n",
    "you will learn how to do web scraping with Python from scratch. In subsession **4.1**, we will have a deep look at the fundamentals of web scraping. You will experience how you can use the Python libraries to handle different data collections. In subsubsession **4.1.1**, you will learn about basics for the feed parsing with the feedparser library, and in subsubsession **4.1.2**, you will learn about html syntax. We move to the basic web scraping tool; the Beautiful soup library in the following subsession **4.2**. In subsession **4.3**, we will introduce the Selenium library to collect data from dynamic websites that only load content once you interact with it (scrolling, clicking, etc.) and are difficult to obtain through more traditional scraping approaches. We will work through an actual web scraping projects throughout this session, focusing on online news sites, pdf pages, and Quora. Finally, in subsession **4.4**, we will compare these libraries and talk about the challanges and data privacy approaches.\n",
    "</div>\n",
    "\n",
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Caution</b>\n",
    "\n",
    "This Jupyter Notebook demonstrates a workflow that consists of a **sequence of processing steps**. The notebook must be executed from top to bottom. Going back up from a certain code cell and trying to execute a cell that precedes it may not work.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c403de1f",
   "metadata": {},
   "source": [
    "## 4.1. Fundamentals of Web scraping\n",
    "\n",
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Gizem will add more info on the fundamentals from this realpython page and parts from Singrodia et al.2019 as well. Also, a short intro on the types of webpages will come here.\n",
    "    \n",
    "https://realpython.com/beautiful-soup-web-scraper-python/#reasons-for-web-scraping\n",
    "    \n",
    "https://medium.com/pythoneers/the-fundamentals-of-web-scraping-using-python-its-libraries-6f146b91efb4\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f0bacd",
   "metadata": {},
   "source": [
    "### 4.1.1. Feed Parsing [<a href='#destination1'>1, 2, 3</a>] <a id='destination1_'></a>\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Web_feed), a web feed (or news feed) is a data format used for providing users with frequently updated content. Content distributors syndicate a web feed, thereby allowing users to subscribe a channel to it by adding the feed resource address to a news aggregator client (also called a feed reader or a news reader). Users typically subscribe to a feed by manually entering the URL of a feed or clicking a link in a web browser or by dragging the link from the web browser to the aggregator, thus \"RSS and Atom files provide news updates from a website in a simple form for your computer.\"\n",
    "\n",
    "Here we introduce [feedparser](https://pypi.org/project/feedparser/), a powerful python package for parsing RSS feeds. By providing the RSS feed link, we can get structured information in the form of python lists and dictionaries, which could then be used to extract the desired information in a simple and efficient way.\n",
    "\n",
    "\n",
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Pouria, please change the citations to APA format, below Feed Parsing also has three references in a number format. We should integrate these references into the text not on the title. Does that sound clear? Thanks!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cac09b",
   "metadata": {},
   "source": [
    "#### Getting started with feedparser\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Hint:</b> \n",
    "    \n",
    "Before importing the libraries, we need to have the neccessary software packages and libraries installed. You can always go back to [Session 1: Setting up the computing environment](https://github.com/gesiscss/css_methods_python/blob/main/a_introduction/1_computing_environment.ipynb) to learn about how to install software packages and libraries that you need for this session.\n",
    "    \n",
    "</div>\n",
    "\n",
    "As usual, we need to import the package in the first place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8541458",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb80565f",
   "metadata": {},
   "source": [
    "#### Parsing an RSS feed URL\n",
    "To parse an RSS feed link, you can simply use the **parse()** method from the feedparser package. It takes a string as argument, which could be a URL or the address to the file locally saved on the computer. Here we use CNN RSS as an example URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23239687",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed = feedparser.parse(\"https://www.voanews.com/api/zgvmqye_o_qv\")\n",
    "\n",
    "# You can try other news websites as well:\n",
    "\n",
    "# feed = feedparser.parse(\"https://www.aljazeera.com/xml/rss/all.xml\")\n",
    "# feed = feedparser.parse(\"http://rss.cnn.com/rss/edition_europe.rss\")\n",
    "\n",
    "feed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e431dc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Hint:</b> \n",
    "You can try the following ways in order to get a website's RSS feed:\n",
    "\n",
    "- If the website is powered by Wordpress, you can do it by adding /feed/ at the end of its URL. Trying /rss/ is another option.\n",
    "<img src='images/rss_logo.png' style='height: 50px; float: right; margin-left: 50px' >\n",
    "- If you see the standard orange RSS logo, by simply clicking on it you will be taken to the website's RSS feed.\n",
    "- You can also use the page source: right click on the page and choose page source. In the new window, use ctrl+f and type in RSS. You’ll find the feed’s URL between the quotes after **href=**.\n",
    "\n",
    "The parse method fetches the feed from the provided URL, extracts the information in a systematic way and stores each piece in a structured format. At the high level, it returns a python dictionary with multiple keys and values, in which each value may contain python lists or other dictionaries. You can access the keys using the **keys()** method:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd44f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6810a7",
   "metadata": {},
   "source": [
    "Using these keys, we can access the more specific information that we want. The most common keys that can be used for extracting information are **entries** and **feed**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0a7703",
   "metadata": {},
   "source": [
    "#### Extracting the contents from the feed\n",
    "We will start with the **entries** key. We can get the list of all the posts/podcasts/entries or any other form of content the feed is serving for, from the **entries** key in the dictionary. More information on other possible keys in the returned dictionary can be found [here](https://feedparser.readthedocs.io/en/latest/reference.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = feed['entries']\n",
    "entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b524fd40",
   "metadata": {},
   "source": [
    "We can get the number of articles/entries using the **len()** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab3a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f528aa",
   "metadata": {},
   "source": [
    "#### Getting details of the entries\n",
    "We can iterate over the items of the entries list and print them to get more details on each article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fefd893",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in entries:\n",
    "    print (entry)\n",
    "    print (\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140dac73",
   "metadata": {},
   "source": [
    "As we can see, each entry in the list is a dictionary again, which has different key-value pairs like **title**, **summary**, **link**, etc. We can again use the **keys()** method in order to explore the keys of the new dictionary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c40680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entries[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105af85d",
   "metadata": {},
   "source": [
    "Now that we have all the keys associated with the entries, we can extract the specific information like title, author, and actual contents of the feed.\n",
    "Though this might not be the same for all RSS feeds, it might be very similar and a matter of using the right keyword for the associated keys in the list of dictionaries.\n",
    "\n",
    "Let's say, we want to print out the titles of all the entries in the feed, and save them to the `titles` list. We can do that by iterating over the entries list and fetching the title from the iterator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a49ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "\n",
    "for entry in entries:\n",
    "    titles.append(entry.title)\n",
    "    print (entry.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334eb28e",
   "metadata": {},
   "source": [
    "Similarly, we can get the links, summaries, publishing dates and tags of the entries using the corresponding keys in the dictionary. We will save them in lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fedfe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links = []\n",
    "summaries = []\n",
    "published = []\n",
    "tags = []\n",
    "\n",
    "for entry in entries:\n",
    "    links.append(entry.link)\n",
    "    summaries.append(entry.summary)\n",
    "    published.append(entry.published)\n",
    "    tags.append(entry.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dab9a1",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Pouria, please Add a Caution like I did before in red: Go to Session 2 if you don't know the basics of the dataframe.\n",
    "All entries should be in a dataframe and written to a csv file.\n",
    "A dataframe with entry id, title, summary, link as column titles.\n",
    "\n",
    "<b>Pouria's note:</b>\n",
    "Added it below:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09c1935",
   "metadata": {},
   "source": [
    "Now we put all of the results in a pandas dataframe and then save it to a csv file. You can find it in the `outputs` folder, which is located in our current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c379071d",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Caution</b>\n",
    "\n",
    "Pandas dataframes are used for keeping data in a well-structured manner. If you are not familiar with the basics of dataframes, check out [session 2: data handling and visualization](https://github.com/gesiscss/css_methods_python/blob/main/a_introduction/2_data_handling_and_visualization.ipynb).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55ff7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "feeds_df = pd.DataFrame([titles, published, summaries, links, tags]).transpose()\n",
    "feeds_df.columns = ['title', 'published', 'summary', 'link', 'tags']\n",
    "\n",
    "feeds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9499ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feeds_df.to_csv('./outputs/feeds_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09370b8",
   "metadata": {},
   "source": [
    "### 4.1.2. Scraping HTML content\n",
    "\n",
    "<img src='images/html.png' style='height: 90px; float: right; margin-left: 50px' >\n",
    "\n",
    "The **HyperText Markup Language** or **HTML** is the standard markup language for documents designed to be \n",
    "displayed in a web browser. It can be assisted by technologies such as Cascading Style Sheets (CSS) and scripting languages such as JavaScript. While the main content of the web pages are in the form of HTML, CSS add styling to the pages to make them look nicer and JavaScript files add interactivity to them.\n",
    "\n",
    "\n",
    "HTML code consists of a series of **elements**, and these elements tell the browser how to display the content. For collecting data from HTML web pages, it's necessary to have an idea of how this element syntax works.\n",
    "\n",
    "#### HTML Element Syntax [<a href='#destination2'>4</a>] <a id='destination2_'></a>\n",
    "\n",
    "HTML language can be applied to pieces of text to give them different meanings in a document (Is it a paragraph? Is it a bulleted list? Is it part of a table?), structure a document into logical sections (Does it have a header? Three columns of content? A navigation menu?), and embed content such as images and videos into a page. In this section we will introduce the first two of these, together with the fundamental concepts and syntax you need to know to understand HTML.\n",
    "\n",
    "To get started, we will begin with defining elements, attributes, and some other important terms. We will also explain where these fit into HTML. You will learn how HTML elements are structured, how a typical HTML page is structured, and other important basic language features.\n",
    "\n",
    "As already mentioned, HTML is a markup language that tells web browsers how to structure the web pages you visit. It can be as complicated or as simple as the web developer wants it to be. HTML consists of a series of elements, which you use to enclose, wrap, or mark up different parts of content to make it appear or act in a certain way. The enclosing tags can make content into a hyperlink to connect to another page, italicize words, and so on. For example, consider the following line of text:\n",
    "\n",
    "`My cat is very grumpy`\n",
    "\n",
    "If we wanted the text to stand by itself, we could specify that it is a paragraph by enclosing it in a paragraph (`<p>`) element:\n",
    "\n",
    "`<p>My cat is very grumpy</p>`\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "<b>Insight</b>\n",
    "\n",
    "Tags in HTML are not case-sensitive, but it's better to write all of them in lower case for the sake of consistency and readability.\n",
    "    \n",
    "</div>\n",
    "\n",
    "####  Anatomy of an HTML element\n",
    "\n",
    "Let's further explore our paragraph element mentioned above:\n",
    "\n",
    "<img src='images/html4.png' width=\"500\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "The anatomy of our element is:\n",
    "\n",
    "- **The opening tag**: This consists of the name of the element (in this example, p for paragraph), wrapped in opening and closing angle brackets. This opening tag marks where the element begins or starts to take effect. In this example, it precedes the start of the paragraph text.\n",
    "\n",
    "\n",
    "- **The content**: This is the content of the element. In this example, it is the paragraph text.\n",
    "\n",
    "\n",
    "- **The closing tag**: This is the same as the opening tag, except that it includes a forward slash before the element name. This marks where the element ends. Failing to include a closing tag is a common beginner error that can produce peculiar results.\n",
    "\n",
    "So, *the element* is the opening tag, followed by content, followed by the closing tag.\n",
    "\n",
    "**Create your first HTML element:**  Edit the `html` string below (it contains an HTML code) and get the actual rendered HTML output from `HTML()`. You can wrap the text of your choice with the tags `<em>` and `</em>`. Doing this should give the line italic text formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596764a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"<em>This is my text.</em>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c98d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a452d1",
   "metadata": {},
   "source": [
    "#### Nesting elements\n",
    "\n",
    "Elements can be placed within other elements. This is called *nesting*. If we wanted to state that our cat is **very** grumpy, we could wrap the word \"very\" in a `<strong>` element, which means that the word is to have strong(er) text formatting:\n",
    "\n",
    "`<p>My cat is <strong>very</strong> grumpy.</p>`\n",
    "\n",
    "There is a right and wrong way to do nesting. In the example above, we opened the `p` element first, then opened the `strong` element. For proper nesting, we should close the `strong` element first, before closing the `p`.\n",
    "The following is an example of the *wrong* way to do nesting:\n",
    "\n",
    "`<p>My cat is <strong>very grumpy.</p></strong>`\n",
    "\n",
    "<u>The tags have to open and close in a way that they are inside or outside one another.</u> With the kind of overlap in the example above, the browser has to guess at your intent. This kind of guessing can lead to unexpected results.\n",
    "\n",
    "#### Block versus inline elements\n",
    "\n",
    "There are two important categories of elements to know in HTML: block-level elements and inline elements.\n",
    "\n",
    "- Block-level elements form a visible block on a page. A block-level element appears on a new line following the content that precedes it. Any content that follows a block-level element also appears on a new line. Block-level elements are usually structural elements on the page. For example, a block-level element might represent headings, paragraphs, lists, navigation menus, or footers. A block-level element wouldn't be nested inside an inline element, but it might be nested inside another block-level element.\n",
    "\n",
    "\n",
    "- Inline elements are contained within block-level elements, and surround only small parts of the document's content (not entire paragraphs or groupings of content). An inline element will not cause a new line to appear in the document. It is typically used with text, for example an `<a>` element creates a hyperlink, and elements such as `<em>` or `<strong>` create emphasis.\n",
    "\n",
    "Consider the following example:\n",
    "\n",
    "`<em>first</em><em>second</em><em>third</em>`\n",
    "\n",
    "`<p>fourth</p><p>fifth</p><p>sixth</p>`\n",
    "\n",
    "`<em>` is an inline element. As you can see below, the first three elements sit on the same line, with no space in between. On the other hand, `<p>` is a block-level element. Each p element appears on a new line, with space above and below. (The spacing is due to default CSS styling that the browser applies to paragraphs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ec810",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"<em>first</em><em>second</em><em>third</em>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"<p>fourth</p><p>fifth</p><p>sixth</p>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27958786",
   "metadata": {},
   "source": [
    "#### Empty elements\n",
    "\n",
    "Not all elements follow the pattern of an opening tag, content, and a closing tag. Some elements consist of a single tag, which is typically used to insert/embed something in the document. For example, the `<img>` element embeds an image file onto a page:\n",
    "\n",
    "`<img src=\"https://raw.githubusercontent.com/mdn/beginner-html-site/gh-pages/images/firefox-icon.png\">`\n",
    "\n",
    "This would output the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<img src=\"https://raw.githubusercontent.com/mdn/beginner-html-site/gh-pages/images/firefox-icon.png\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e23fd2f",
   "metadata": {},
   "source": [
    "#### Attributes\n",
    "\n",
    "Elements can also have attributes. Attributes look like this:\n",
    "\n",
    "<img src='images/html5.png' width=\"800\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "Attributes contain extra information about the element that won't appear in the content. In this example, the `class` attribute is an identifying name used to target the element with style information.\n",
    "\n",
    "An attribute should have:\n",
    "\n",
    "- A space between it and the element name. (For an element with more than one attribute, the attributes should be separated by spaces too.)\n",
    "- The attribute name, followed by an equal sign.\n",
    "- An attribute value, wrapped with opening and closing quote marks.\n",
    "\n",
    "**Adding attributes to an element**: Another example of an element is `<a>`. This stands for *anchor*. An anchor can make the text it encloses into a hyperlink. Anchors can take a number of attributes, but several are as follows:\n",
    "\n",
    "- `href`: This attribute's value specifies the web address for the link. For example: `href=\"https://www.mozilla.org/\"`\n",
    "- `title`: The `title` attribute specifies extra information about the link, such as a description of the page that is being linked to. For example, `title=\"The Mozilla homepage\"`. This appears as a tooltip when a cursor hovers over the element.\n",
    "- `target`: The `target` attribute specifies the browsing context used to display the link. For example, `target=\"_blank\"` will display the link in a new tab. If you want to display the linked content in the current tab, just omit this attribute.\n",
    "\n",
    "You can edit the `html` string below to turn it into a link to your favorite website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '<p>A link to my <a href=\"https://www.mozilla.org/\" title=\"The Mozilla homepage\" target=\"_blank\">favorite website</a>.</p>'\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430d6efa",
   "metadata": {},
   "source": [
    "#### Anatomy of an HTML document\n",
    "\n",
    "Individual HTML elements aren't very useful on their own. Next, let's examine how individual elements combine to form an entire HTML page:\n",
    "\n",
    "```\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en-US\">\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>My test page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <p>This is my page</p>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "Here we have:\n",
    "\n",
    "1. `<!DOCTYPE html>`: The doctype. When HTML was young (1991-1992), doctypes were meant to act as links to a set of rules that the HTML page had to follow to be considered good HTML. More recently, the doctype is a historical artifact that needs to be included for everything else to work right. `<!DOCTYPE html>` is the shortest string of characters that counts as a valid doctype. That is all you need to know!\n",
    "\n",
    "\n",
    "2. `<html></html>`: The `<html>` element. This element wraps all the content on the page. It is sometimes known as the root element.\n",
    "\n",
    "\n",
    "3. `<head></head>`: The `<head>` element. This element acts as a container for everything you want to include on the HTML page, **that isn't the content** the page will show to viewers. This includes keywords and a page description that would appear in search results, CSS to style content, character set declarations, and more. You will learn more about this in the next article of the series.\n",
    "\n",
    "\n",
    "4. `<meta charset=\"utf-8\">`: The `<meta>` element. This element represents metadata that cannot be represented by other HTML meta-related elements, like `<base>`, `<link>`, `<script>`, `<style>` or `<title>`. The charset attributes sets the character set for your document to UTF-8, which includes most characters from the vast majority of human written languages. With this setting, the page can now handle any textual content it might contain. There is no reason not to set this, and it can help avoid some problems later.\n",
    "\n",
    "\n",
    "5. `<title></title>`: The `<title>` element. This sets the title of the page, which is the title that appears in the browser tab the page is loaded in. The page title is also used to describe the page when it is bookmarked.\n",
    "\n",
    "\n",
    "6. `<body></body>`: The `<body>` element. This contains all the content that displays on the page, including text, images, videos, games, playable audio tracks, or whatever else.\n",
    "\n",
    "Later in this notebook, you will get to explore HTML codes in more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b11b87",
   "metadata": {},
   "source": [
    "#### HTML Tree Structure [<a href='#destination3'>5</a>] <a id='destination3_'></a>\n",
    " \n",
    "Each HTML document can actually be referred to as a document tree. We describe the elements in the tree like we would describe a family tree. There are ancestors, descendants, parents, children and siblings.\n",
    "\n",
    "Use the sample HTML document below for the following examples. The `<head>` section of the document is omitted for brevity.\n",
    "\n",
    "```\n",
    "<body>\n",
    "\n",
    "  <div id=\"content\">\n",
    "    <h1>Heading here</h1>\n",
    "    <p>Lorem ipsum dolor sit amet.</p>\n",
    "    <p>Lorem ipsum dolor <em>sit</em> amet.</p>\n",
    "    <hr>\n",
    "  </div>\n",
    "  \n",
    "  <div id=\"nav\">\n",
    "    <ul>\n",
    "      <li>item 1</li>\n",
    "      <li>item 2</li>\n",
    "      <li>item 3</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "\n",
    "</body>\n",
    "```\n",
    "\n",
    "A diagram of the above HTML document tree would look like this:\n",
    "\n",
    "<img src='images/tree1.gif' width=\"435\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "##### Ancestor\n",
    "\n",
    "An ancestor refers to any element that is connected but further up the document tree - no matter how many levels higher.\n",
    "\n",
    "In the diagram below, the `<body>` element is the ancestor of all other elements on the page.\n",
    "\n",
    "<img src='images/tree_ancestor.gif' width=\"435\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "##### Descendant\n",
    "\n",
    "A descendant refers to any element that is connected but lower down the document tree - no matter how many levels lower.\n",
    "In the diagram below, all elements that are connected below the `<div>` element are descendants of that `<div>`.\n",
    "\n",
    "<img src='images/tree_descendant.gif' width=\"435\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "##### Parent and Child\n",
    "\n",
    "A parent is an element that is directly above and connected to an element in the document tree. In the diagram below, the `<div>` is a parent to the `<ul>`.\n",
    "\n",
    "A child is an element that is directly below and connected to an element in the document tree. In the diagram above, the `<ul>` is a child to the `<div>`.\n",
    "\n",
    "<img src='images/tree_parent.gif' width=\"435\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "##### Sibling\n",
    "\n",
    "A sibling is an element that shares the same parent with another element.\n",
    "\n",
    "In the diagram below, the `<li>`s are siblings as they all share the same parent - the `<ul>`.\n",
    "\n",
    "<img src='images/tree_siblings.gif' width=\"435\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526fc121",
   "metadata": {},
   "source": [
    "## 4.2. Scraping static HTML content with Beautiful Soup\n",
    "<img src='images/bs.png' style='height: 150px; float: right; margin-left: 0px' >\n",
    "\n",
    "[Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is a Python library that makes it easy to scrape information from web pages. It sits atop (an interactive monitor to view the load on a Linux system) an HTML or XML parser, providing Pythonic idioms for iterating, searching, and modifying the parse tree.\n",
    "\n",
    "Now that you have an idea of how HTML webpages are structured, we can start working with Beautiful Soup. We will go through some of the most important methods of it, and then you will get to write your first scraping project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f1e7c4",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Caution</b>\n",
    "    \n",
    "If you need to the package installed on your system, use `pip` (check out [Session 1](https://github.com/gesiscss/css_methods_python/blob/main/a_introduction/1_computing_environment.ipynb) for installing packages), and then import the neccessary packages as the upcoming cell shows.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b69e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1cf9bf",
   "metadata": {},
   "source": [
    "### 4.2.1. Learning basic functions\n",
    "\n",
    "[<a href='#destination4'>6, 7</a>] <a id='destination4_'></a>\n",
    "\n",
    "We will begin with an example page at http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html.\n",
    "\n",
    "The HTML source code of the page is stored in the `content` string as follows:\n",
    "\n",
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Pouria, please change the reference styling to APA and let's not have numbers for citations on the title.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c71a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"<html>\n",
    "<head>\n",
    "<title>A simple example page</title>\n",
    "</head>\n",
    "<body>\n",
    "<div>\n",
    "<p class=\"inner-text first-item\" id=\"first\">\n",
    "                First paragraph.\n",
    "            </p>\n",
    "<p class=\"inner-text\">\n",
    "                Second paragraph.\n",
    "            </p>\n",
    "</div>\n",
    "<p class=\"outer-text first-item\" id=\"second\">\n",
    "<b>\n",
    "                First outer paragraph.\n",
    "            </b>\n",
    "</p>\n",
    "<p class=\"outer-text\">\n",
    "<b>\n",
    "                Second outer paragraph.\n",
    "            </b>\n",
    "</p>\n",
    "</body>\n",
    "</html>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0168fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML (content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f585526",
   "metadata": {},
   "source": [
    "## requests \n",
    "\n",
    "You can get the same content by fetching the page through `requests`. It is a simple and useful HTTP library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "page = requests.get(\"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\")\n",
    "content = page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f5fff",
   "metadata": {},
   "source": [
    "By printing `page`, you can check to see if fetching the contents has been successful. The status code of \"200\" means you are good to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f09151",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page)\n",
    "# print(page.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50800ce3",
   "metadata": {},
   "source": [
    "#### html parser\n",
    "\n",
    "By using its HTML parser, Beautiful Soup transforms a complex HTML document into a tree of python objects, so we can manage working with it easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdffe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff2709e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364711e0",
   "metadata": {},
   "source": [
    "Using `soup.pretiffy()`, we can have a better tree overview of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a675de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434bde9f",
   "metadata": {},
   "source": [
    "Each tag can now be viewed as an object. We can also access all children objects of a tag using dots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bfe331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(soup.html.body.children)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b4868f",
   "metadata": {},
   "source": [
    "#### find() & find_all()\n",
    "\n",
    "Two  of the most important methods of Beautiful Soup are its `find` and `find_all()` methods.\n",
    "\n",
    "`find()` method finds the first occurence of a certain tag matching the given criteria. Its first argument is the tag name, so if we pass `p` as a string to it, it will return the first occurence of the `p` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b99e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c87b7",
   "metadata": {},
   "source": [
    "As you can see, the output is the same as when we use a dot for accessing the `p` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e8f15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaebdce1",
   "metadata": {},
   "source": [
    "With the `find_all()` method, we can get a list of all of the occurences of a certain tag matching the given criteria. Again, if we pass the \"p\" string to it, it will return all the occurences of the `p` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d0f8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d4a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(soup.find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e147bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552891f0",
   "metadata": {},
   "source": [
    "We can also specify attribute values and pass them to the method. The following line of code returns the list of all the `p` tags whose values for the `class` attribute is `\"outer-text\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7488c8d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.find_all('p', {'class': \"outer-text\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9037c67",
   "metadata": {},
   "source": [
    "This one returns the list of all tags whose `id` attributes equal `\"first\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac01b56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.find_all(id=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e5483d",
   "metadata": {},
   "source": [
    "#### select()\n",
    "\n",
    "Beautiful Soup has a `select()` method which uses the [SoupSieve](https://facelessuser.github.io/soupsieve/) package to run a CSS selector against a parsed document and return all the matching elements.\n",
    "\n",
    "The SoupSieve documentation lists all the currently supported CSS selectors, but here are some of the basics;\n",
    "\n",
    "You can find tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc3fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select(\"p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b8925",
   "metadata": {},
   "source": [
    "You can find tags beneath other tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7590441",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select(\"div p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f8b7f",
   "metadata": {},
   "source": [
    "You can find tags with specific classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012c9da4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soup.select(\"p.first-item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1351d88e",
   "metadata": {},
   "source": [
    "You can find tags by id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cfe757",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select(\"#second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c981b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select(\"p#second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d251f31f",
   "metadata": {},
   "source": [
    "And you can also find tags by a combination of the above-mentioned criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46b664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.select(\"div p.first-item#first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e129a42",
   "metadata": {},
   "source": [
    "#### get_text()\n",
    "\n",
    "If you only want the human-readable text inside a document or tag, you can use the get_text() method. It returns all the text in a document or beneath a tag, as a single Unicode string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cda836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07354c5d",
   "metadata": {},
   "source": [
    "You can tell Beautiful Soup to strip whitespace from the beginning and end of each bit of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ff3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.get_text(strip = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2de00d2",
   "metadata": {},
   "source": [
    "You can also specify a string to be used to join the bits of text together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.get_text(\"|\", strip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b3c5c6",
   "metadata": {},
   "source": [
    "But at that point you might want to use the `stripped_strings` generator instead, and process the text yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdfa59c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[text for text in soup.stripped_strings]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f68c312",
   "metadata": {},
   "source": [
    "### 4.2.2. Extracting relavent information from static webpages\n",
    "\n",
    "Depending on our research projects, we might need data from different sources. For example, if we want to investigate the news exposure in specific countries and compare the topics of news, the first step will be collecting news articles from news websites. Or, if our project is related to understanding the subjects discussed at the European Union meetings, we first should have those meeting minutes at hand. These two project examples' data collection processes are explained in detail in this section. You can always think of another project, which might use and combine similar techniques that are introduced here. \n",
    "\n",
    "#### Example: Scraping news articles from Aljazeera [<a href='#destination5'>8</a>] <a id='destination5_'></a>\n",
    "\n",
    "<img src='images/aljazeera.png' style='height: 150px; float: right; margin-left: 50px' >\n",
    "\n",
    "Now that you are familiar with the basics of Beautiful Soup, we can do a more practical scraping project for the news exposure in Turkey on a particular news website [aljazeera.com](https://www.aljazeera.com), and we will get to practice what you have learnt so far.\n",
    "\n",
    "To have a better idea of what exactly we are going to do, go the [Aljazeera website](https://www.aljazeera.com), use the search bar and search \"Turkey\". In the new page, sort the retrieved news articles by date. As you can see, the 10 most recent news articles related to Turkey are now displayed. We are going to scrape and store these articles and their relavent information (e.g., title, text, or url) in a pandas dataframe.\n",
    "\n",
    "First, we need to make sure we have all the necessary packages available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db3c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these libraries if you have not done so\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4184c6",
   "metadata": {},
   "source": [
    "Then we construct the right URL from `address` and `searchterm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0bbf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "address = \"https://www.aljazeera.com/search/\"\n",
    "searchterm = \"Turkey\"\n",
    "parameters = \"?sort=date\"\n",
    "url = address + searchterm + parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a65bf",
   "metadata": {},
   "source": [
    "The resulting URL is the same as that of the page you explored at the first stage. Now we fetch it using `requests`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d745cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(url) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546cc00a",
   "metadata": {},
   "source": [
    "Then we parse the webpage with Beautiful Soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3989cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef9807c",
   "metadata": {},
   "source": [
    "Now that we have the page parsed, we need to select the right elements of it to extract our desired information from. In the simple webpage that we investigated in the Beautiful Soup Basics section, it was easy to pick the right elements to investigate from the few lines of code. In real HTML web pages it's a bit different.\n",
    "\n",
    "In order to find the right elemets, right-click somewhere on the page and click on *inspect*. Then press Ctrl+Shift+C. Now you should be able to inspect the page and see the HTML code for each part of the page you hover the mouse. Equivalently, by hovering the mouse on certain lines of HTML code you can see what that code actually creates on the page.\n",
    "\n",
    "On Google Chrome it would look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada2f2a",
   "metadata": {},
   "source": [
    "<img src='images/inspect.png' style='height: 550px; float: right; margin-left: 50px' >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffdd488",
   "metadata": {},
   "source": [
    "It turns out that the elements that we would like to work on are the ones with the `article` tags. We'll select them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca690fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = soup.select('article')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527828c",
   "metadata": {},
   "source": [
    "Next, we will scrape different information from the articles. We do that by putting every article's title, text and URL in a corresponding dictionary, and will add all the dictionaries to the `results` list:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fae4a1",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Pouria, can you add the date here for each article let's keep the date as well for an overtime analysis chance?\n",
    "    \n",
    "<b>Pouria's note:</b>\n",
    "Done!    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57926e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c96262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty list for results\n",
    "results = []\n",
    "\n",
    "for article in articles: \n",
    "    \n",
    "    # Initialize empty dictionary\n",
    "    # Extract title, text and URL of articles \n",
    "    item = {}\n",
    "    item['title'] = article.select_one('span').text.strip()\n",
    "    item['text'] = article.select_one('p').text.strip()    \n",
    "    item['url'] = article.select_one('a').get('href')\n",
    "    item['date'] = articles[0].select('span')[2].text\n",
    "    # You can also get the URLs with article.select_one('a')['href']\n",
    "    \n",
    "    # Append items to result-list\n",
    "    results.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ebddb4",
   "metadata": {},
   "source": [
    "At last, we convert the results list to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657e7f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b66a6c7",
   "metadata": {},
   "source": [
    "We can save the resulting dataframe in a csv file. You can access the file in the `outputs` folder in the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc8c351",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Pouria, should we have a results or output folder for each Session? Let's save these results to output folder?\n",
    "    \n",
    "<b>Pouria's note:</b>\n",
    "It's a good idea. Now the csv file is saved to `outputs` folder. The results for feedparser and PDF downloading sections are also kept there.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0528374",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./outputs/aljazeera.csv', mode = 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f58d80",
   "metadata": {},
   "source": [
    "#### Example: Collecting multiple PDF files from different pages of a particular website\n",
    "\n",
    "<img src='images/eu_council.png' style='height: 90px; float: right; margin-left: 50px' >\n",
    "\n",
    "In this example, we will showcase how we can automate downloading numerous PDF files from different pages of a particular website, getting help from `requests` and `BeautifulSoup`. We will be collecting data from The Minutes of European Council Meetings; take a look at [this web page](https://www.consilium.europa.eu/en/documents-publications/public-register/council-minutes/?year=2023) and check out its structure and overview.\n",
    "\n",
    "The image below will be our starting web page for this task:\n",
    "\n",
    "<img src='images/pdf_download_1.png' style='height: 550px; float: right; margin-left: 50px' >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dda29e",
   "metadata": {},
   "source": [
    "As you can see, there are some main elements that are relevant to our work;\n",
    "\n",
    "   - The first one shows how many contents there are for each year. Some of these contents may contain PDF files, and some may not.\n",
    "   - The second one is a clickable link that takes you to the related contents of each year.\n",
    "   - The ones with label `3` are the contents.\n",
    "   - The ones with label `4` are the links for downloading PDF files.\n",
    "   - The ones with label `5` are the dates in which these contents have been added to the website.\n",
    "\n",
    "If you scroll down to the end of the page, you can see that there may be many different pages containing the above-mentioned contents. In fact, there are 20 contents listed in each page, and for a year like 2001 that has 242 contents, there are 13 different pages:\n",
    "\n",
    "\n",
    "<img src='images/pdf_download_2.png' style='height: 550px; float: right; margin-left: 50px' >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac681af1",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "    \n",
    "Pouria, can you please extract the date information from this webpage? For instance, in your function, you are using the counter in the pdf_counter and use this number in the file name? Can you add the date after the pdf_counter to the filename? If this task is not clear, let's chat?\n",
    "    \n",
    "<b>Pouria's note:</b>\n",
    "I added the dates to the end of the downloaded file names. This changed the structure of the code a little bit. I also changed the screenshots and the colors of numerical labels accordingly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6773c2",
   "metadata": {},
   "source": [
    "We are going to automatically go through all these pages for **any number of years** of our choice, and download all the available PDFs into a `PDFs` folder in the `outputs` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c335e143",
   "metadata": {},
   "source": [
    "#### Getting started\n",
    "\n",
    "After making sure that we have imported all the necessary libraries, we'll put the main url of the site in the `main_url` variable. We will be using it in our main function `get_PDFs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de330880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "main_url = \"https://www.consilium.europa.eu/en/documents-publications/public-register/council-minutes/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9218126",
   "metadata": {},
   "source": [
    "Once again, go to the main webpage and click on a different page than 2023, for example 2021. Take a look at the url of the new page:\n",
    "\n",
    "`https://www.consilium.europa.eu/en/documents-publications/public-register/council-minutes/?year=2021`\n",
    "\n",
    "As you can see, it is in the following form:\n",
    "\n",
    "`main_url` + `?year=2021`\n",
    "\n",
    "For each year that you click on, the resulting url is made of the `?year=` string, followed by the year number (here: `2021`), added to the `main_url`.\n",
    "\n",
    "If you click on the second page of 2021, you can see that there is a similar thing for different pages within each year. For second page, it is\n",
    "\n",
    "`https://www.consilium.europa.eu/en/documents-publications/public-register/council-minutes/?year=2021&Page=2`\n",
    "\n",
    "Which follows this structure: `main_url` + `?year=2021` + `&Page=2`\n",
    "\n",
    "We will use these simple rules to loop through all the pages that we want, and get all the PDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac96c3a",
   "metadata": {},
   "source": [
    "#### The main function: `get_PDFs`\n",
    "\n",
    "Now that you know the necessary principles for scraping the website, we can work with the `get_PDFs` function. It takes two arguments, `start_year` and `end_year`, and downloads every single PDF file available on the website for the `start_year` - `end_year` interval, including the end_year itself. It saves all the files in their corresponding folders in the `PDFs` folder. The rest of the explanations could be found in comments among the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa64b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PDFs (start_year, end_year):\n",
    "    \n",
    "    \n",
    "    # Making a list of all the years between start_year and end_year:\n",
    "    years = [i for i in range(start_year, end_year+1)]\n",
    "\n",
    "    # Looping through all years, using the  years list:\n",
    "    for year in years:\n",
    "\n",
    "        # A counter for PDFs that will be used in naming the downloaded files:\n",
    "        pdf_counter = 1\n",
    "\n",
    "        # Making the url of the first page of the year:\n",
    "        first_page_url = main_url + '?year=' + str(year)\n",
    "\n",
    "        \n",
    "        # Finding the number of contents in the page, to be used to determine the number of webpages in each year:\n",
    "        first_page = requests.get(first_page_url)\n",
    "        content = first_page.content\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        number_of_contents = int(soup.find('h2').text.split()[0])\n",
    "        number_of_pages = number_of_contents // 20 + 1\n",
    "\n",
    "        # Starting the download procedure:\n",
    "        print(f'Downloading PDFs of {year} (from {number_of_pages} webpages):\\n')\n",
    "\n",
    "        # Looping through all webpages of the desired year:\n",
    "        for page_number in range(number_of_pages):\n",
    "\n",
    "            print(f'Getting page {page_number + 1}')\n",
    "\n",
    "            url = main_url + '?year=' + str(year)\n",
    "\n",
    "            # For pages after page 1, we need to add the '&Page=n' element to the url to access other pages:\n",
    "            if page_number != 0:\n",
    "                url = url + '&Page=' + str(page_number + 1)\n",
    "\n",
    "            # Getting the page:\n",
    "            page = requests.get(url)\n",
    "            content = page.content\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Finding the links and dates of PDF files:\n",
    "            for j in soup.find_all('li', {'class': \"margin-0\"}):\n",
    "\n",
    "                # Ignoring the contents which do not have a pdf file to download:\n",
    "                if j.find('a', {'class': 'link-pdf'}) == None:\n",
    "                    continue\n",
    "\n",
    "                # Getting the date of the content, for including in the PDF file name:\n",
    "                date = j.find('span', {'class': 'pull-right'}).text\n",
    "\n",
    "                # Getting the actual download link of the PDF file:\n",
    "                pdf_link = j.find('a', {'class': 'link-pdf'})\n",
    "                pdf_url = pdf_link.get('href', [])\n",
    "\n",
    "\n",
    "                # The title of the download link, which will be used in the name of the downloaded file later:\n",
    "                file_name = pdf_link.text \n",
    "\n",
    "                # If the file name has got '/' character, we need to replace it, so that it won't make trouble\n",
    "                # when we make the folders and directories:\n",
    "                if '/' in file_name:\n",
    "                    file_name = file_name.replace('/', '-')\n",
    "\n",
    "                # We do the same thing for date:\n",
    "                if '/' in date:\n",
    "                    date = date.replace('/', '.')\n",
    "\n",
    "                # Similarly, if it contains quotation character, it will get troublesome in windows, so we need\n",
    "                # to replace it:\n",
    "                if '\"' in file_name:\n",
    "                    file_name = file_name.replace('\"', \"'\")\n",
    "\n",
    "                # The PDFs folder contains other folders named after the year:\n",
    "                file_name = './outputs/PDFs/' + str(year) + '/' + str(pdf_counter) + '. ' + file_name + ' (' + date + ')'\n",
    "\n",
    "                # Making the right folder/directory for downloaded files:\n",
    "                os.makedirs(os.path.dirname(file_name), exist_ok=True)          \n",
    "\n",
    "                # Getting the pdf file:\n",
    "                response = requests.get(pdf_url)\n",
    "\n",
    "                # Writing the PDF files:\n",
    "                with open(file_name, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "\n",
    "                pdf_counter = pdf_counter + 1\n",
    "\n",
    "        print ('\\nDone!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b1181",
   "metadata": {},
   "source": [
    "Let's try it for 2021 to 2023:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db1bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_PDFs(2001, 2001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c116e1c9",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "    \n",
    "    \n",
    "Pouria, please see the error above. Some of the meeting minutes in the website are not downloaded fully? For instance the second file in 2001 search? Why is this happenning? Also, after you find the reason to this, if the reason is not technically solvable, can you write a line of code that passes this error and continues downloading the other available pages? Let's also save these PDFs output under the outputs folder. ./outputs/PDFs/...\n",
    "\n",
    "<b>Pouria's note:</b>\n",
    "Solved. I could have run it on Linux, but windows is a bit different when it comes to file names. Now it works on Windows and Mac as well.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766d9a9",
   "metadata": {},
   "source": [
    "You can access the results in the `PDFs` folder under the `outputs` folder in your current directory!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6182d3a1",
   "metadata": {},
   "source": [
    "## 4.3. Scarping dynamic websites with Selenium \n",
    "\n",
    "<img src='images/selenium.png' style='height: 100px; float: right; margin-left: 100px' >\n",
    "\n",
    "In this section, we will show you how to use [Selenium](https://www.selenium.dev/) (a browser automation software) with python, to collect data from websites that only load content once you interact with them (scrolling, clicking, etc.), and are difficult to obtain through more traditional scraping approaches.\n",
    "\n",
    "As we mentioned in the first section, dynamic websites change their content (i.e., source code) due to various reasons, such as:\n",
    "   - Clicking, scrolling, mouse hovering \n",
    "   - Screen sizes, languages (IP-based), devices, time of the day\n",
    "   - Previous visits (user’s browsing history)\n",
    "   - And more!\n",
    "\n",
    "Changes can occur on the client side, such as JavaScript interactions, that do not necessarily change the source code from the websites but change its appearance, such as expanding text boxes. In that case, classical scraping methods are sufficient for grabbing the entire text because the displayed truncated version of a text might already be entirely stored in the source code. \n",
    "\n",
    "However, changes can also occur on the server side, leading to changes in the website’s appearance and content (i.e., source code). For example, scrolling to the bottom of a website leads to more content being loaded. Such effects can often be observed in social media feeds (e.g., Facebook, Twitter, Quora) or online shopping platforms, enabling users to scroll endlessly. Website interactions that lead to content changes are often challenging or not obtainable through classical scraping approaches since they require a JavaScript execution initiated by user interaction. Interestingly, we can use browser automation tools, such as Selenium, to help us imitate user interactions and make data collection possible.\n",
    "\n",
    "### 4.3.1. Setting up basic configurations\n",
    "\n",
    "Selenium is a [browser automation software](https://www.selenium.dev/) that can interface with many different browser types and programming languages. Thus, we can write programming scripts that control the browser and imitate our behavior, such as clicking or scrolling. Before we can start writing a programming script, we need to set up Selenium by downloading a driver. Depending on the browser we want to use (e.g., Firefox, Chrome), we need a different driver, which could be found [here](https://www.selenium.dev/downloads/). In this notebook, we will go through instructions for using both Google Chrome and Mozilla Firefox, for which you can find the drivers [here](https://chromedriver.chromium.org/downloads) and [here](https://github.com/mozilla/geckodriver/releases), respectively. To download the correct driver, you need to know which operating system (e.g. Windows, Linux, Mac) your machine runs on, and which browser version you have. For Chrome, you can find the browser version under Settings > About Chrome (see screenshot below):\n",
    "\n",
    "<img src='images/chrome_version.JPEG' width=\"1000\" height=\"1000\" align=\"center\"/>\n",
    "\n",
    "Download the correct driver, and after unpacking the zip folder, place the *.exe* driver file in the same folder we are running this script.\n",
    "\n",
    "The Selenium webpage contains documentation for all the programming languages, which you can find [here](https://www.selenium.dev/documentation/). However, the documentation is not as concise, and since we are using python, we can also find a separate documentation [here](https://selenium-python.readthedocs.io/).\n",
    "\n",
    "Both documentations are very handy and should be kept close when working with Selenium. When you inspect the documentation, you will recognize that besides sending specific behavioral commands to the browser, accessing web elements is very similar to other approaches, such as beautiful soup. You will need XPATH, CSS selectors, and other properties of web elements to interact with them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc61e0c3",
   "metadata": {},
   "source": [
    "### 4.3.2. Extracting relavent information from dynamic webpages\n",
    "\n",
    "<img src='images/quora_logo.png' style='height: 120px; float: right; margin-left: 10px' >\n",
    "\n",
    "As we already discussed in the section of extracting relavent information from static webpages, each and every project might require data from different sources by web scarping to answer some of our research questions. Or simply, we might like to make some tasks of our lives easier and faster by automated web scraping compared to manual browsing or scrolling; such as searching jobs on internet, finding your favorite bands' histories, or reading different minds over various questions. Along with the last example, let's explore scraping [Quora](www.quora.com), which is a social question-and-answer website where users can collaborate by editing questions and commenting on answers that have been submitted by other users. \n",
    "\n",
    "In this section, we will showcase how you can use Selenium with Chrome/Firefox to collect data from the dynamic website Quora. Before collecting data, we need to check whether we are allowed to collect data from the website. Quora states that we are permitted to employ scrapers but must adhere to the [robots.txt](https://www.quora.com/robots.txt), which specifies the allowed and disallowed contents for scraping, and that we make ourself known to the website so that they can contact us if they want to. We can give Quora our contact information by adding them to the user-agent, the information the browser sends to the website. *Section 4-d: Permitted uses of Quora’s terms of service* specifies the rules for scraping (see the screenshot below):\n",
    "\n",
    "<img src='images/quora.png' width=\"700\" height=\"700\" align=\"center\"/>\n",
    "\n",
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Pouria, what do you think of adding Quora's logo here and European Council's logo up there - to be consistent as we have other examples' logos?\n",
    "\n",
    "<b>Pouria's note:</b>\n",
    "    Done!\n",
    "</div>\n",
    "\n",
    "### 4.3.3 Example: Scraping posts on Quora\n",
    "\n",
    "Before scraping any information, we need to create an account on Quora. We would recommend creating a new account for your scraping project. Go to www.quora.com and create a new account.\n",
    "\n",
    "After creating the account, make sure to import all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # to work with data frames; you may have already imported it in this notebook\n",
    "from time import sleep # to slow down our scraper\n",
    "\n",
    "# all selenium specific packages:\n",
    "from selenium import webdriver # to load the browser\n",
    "from selenium.webdriver.common.keys import Keys # necessary to automate typings, like filling out the forms\n",
    "from selenium.webdriver.common.by import By # necessary to search for web elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d142b",
   "metadata": {},
   "source": [
    "In case you are using Chrome, import the first line in the next cell, if it is Firefox, import the second one (note that if you import both of them at the same time, it will only work for the last one- Firefox!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b20c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options # necessary to change our user agent when working with Chrome\n",
    "\n",
    "# from selenium.webdriver.firefox.options import Options # necessary to change our user agent when working with Firefox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2191cfe4",
   "metadata": {},
   "source": [
    "We can now start the driver (i.e., the browser), which should appear as a separate window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e4999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the driver\n",
    "\n",
    "# For Chrome:\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# For Firefox:\n",
    "# driver = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e266b822",
   "metadata": {},
   "source": [
    "As you can see, a new browser window opens, which is *being controlled by automated test software:*\n",
    "\n",
    "<img src='images/chrome.png' width=\"700\" height=\"700\" align=\"center\"/>\n",
    "\n",
    "For Firefox, it looks something like this:\n",
    "\n",
    "<img src='images/firefox.png' width=\"700\" height=\"700\" align=\"center\"/>\n",
    "\n",
    "The *driver* instance is the browser we will use to navigate the website and find web elements. \n",
    "We can now check what our user-agent for our browser is with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a2f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = driver.execute_script(\"return navigator.userAgent\")\n",
    "print(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dd7706",
   "metadata": {},
   "source": [
    "We can change the user-agent information to make ourself identifiable, and Quora can contact us if they want. We need to initiate a new driver with the changed information. Hence, we first need to quit our current session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ccc18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quit current session\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782503e7",
   "metadata": {},
   "source": [
    "Make sure to run the correct line of code when restarting the driver; in the middle two lines, the first line is for Chrome and second one (which is commented by default) is for Firefox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972aa7fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adding our e-mail address to the user-agent\n",
    "opts = Options()\n",
    "opts.add_argument(\"user-agent=Getting news feed data; contact me through: [e-mail address]\")\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(options=opts)# initaite driver with new user-agent for Chrome\n",
    "# driver = webdriver.Firefox(options=opts)# initaite driver with new user-agent for Firefox\n",
    "\n",
    "# lets check if we changed our user-agent\n",
    "agent = driver.execute_script(\"return navigator.userAgent\")\n",
    "print(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01247368",
   "metadata": {},
   "source": [
    "If we all have come to this point, now, we can start with our new project with scraping [Quora](https://www.quora.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e414312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url to visit\n",
    "url_search = \"https://www.quora.com/\"\n",
    "# go to url\n",
    "driver.get(url_search)\n",
    "sleep(1.5) # set sleep time for 1.5 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e138c05",
   "metadata": {},
   "source": [
    "We will set some pauses occasionally to slow down the scraping process and give the browser some time to load the website. Next, we want to sign into the website. With Selenium, we can automate the step and fill in all the text fields. \n",
    "\n",
    "Similarly, when working with other scarping approaches, we need to find the web elements by inspecting the HTML structure of the website and locating them through their paths, class, or names. Ideally, the elements have an ID we can identify them with, as in the case of the e-mail address and password fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed5fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# providing log-in credentials to the website\n",
    "\n",
    "EMail_field = driver.find_element(By.XPATH, '//*[@id=\"email\"]') # Find e-mail field\n",
    "# specify your e-mail address\n",
    "my_email = \"ENTER YOUR E-MAIL ADDRESS\"\n",
    "\n",
    "EMail_field.send_keys(my_email) # sending the string to the e-mail field\n",
    "sleep(1.5)\n",
    "\n",
    "PW_field = driver.find_element(By.XPATH, '//*[@id=\"password\"]') # find password field\n",
    "# specify your password \n",
    "my_password = \"ENTER YOUR PASSWORD\"\n",
    "\n",
    "PW_field.send_keys(my_password) # sending the string to the password field\n",
    "sleep(1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cae5a80",
   "metadata": {},
   "source": [
    "After we fill in all our information, we can find the log-in button and click on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_elements(By.CLASS_NAME, \"iyYUZT\")[4].click()\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d17aa4e",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Caution</b>\n",
    "    \n",
    "In case you encounter a recaptcha, you can click on the check box and then log in.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f3ad65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clicking on the checkbox\n",
    "driver.find_elements(By.CSS_SELECTOR, \"div.qu-mb--medium\")[3].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging in\n",
    "driver.find_elements(By.CLASS_NAME, \"iyYUZT\")[4].click()\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a6929",
   "metadata": {},
   "source": [
    "After we log into our account, we can see the cookie notification. We can also interact with pop-ups and accept or reject them. We will reject the cookies by finding the *Reject All* button and clicking on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a180b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejecting cookies\n",
    "driver.find_element(By.ID, \"onetrust-reject-all-handler\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780855c4",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "<b>Insight</b>\n",
    "\n",
    "Your website might not be in English, depending on the region you are accessing Quora from.\n",
    "\n",
    "In our case, we are accessing the website from Germany. However, through the language settings at the top of the website, we can change the language to English. We also can automate that step as in the following cell.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae26a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on menu\n",
    "driver.find_elements(By.CLASS_NAME, \"puppeteer_popper_reference\")[1].click()\n",
    "# click to select \"English\"\n",
    "driver.find_element(By.CSS_SELECTOR, \"div.qu-dynamicFontSize--button\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15732cb6",
   "metadata": {},
   "source": [
    "Next, we want to collect some of the information present in our news feed. We need to find the container for the entire feed to collect individual posts, answers, or questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access whole feed containing various forms of posts/questions/answers etc.\n",
    "NewsFeed = driver.find_element(By.CLASS_NAME, \"dom_annotate_multifeed_home\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16178dee",
   "metadata": {},
   "source": [
    "When we inspect the structure of the news feed, we can see that posts, questions, answers, or advertisements have different classes. Thus, we can leverage the class names to access the information we are interested in. For this guide, we only want to collect data from answered questions, which are the predominant elements in our feed.\n",
    "\n",
    "By inspecting the website, we know that one element with the class \"dom_annotate_multifeed_bundle_AnswersBundle\" contains the answers we are interested in.\n",
    "\n",
    "Let's have a look at the first answer in our feed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca83900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all answers\n",
    "Answers = NewsFeed.find_elements(By.CLASS_NAME, \"dom_annotate_multifeed_bundle_AnswersBundle\")\n",
    "# select first answer and print all containing texts\n",
    "answer1 = Answers[0]\n",
    "print(answer1.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02efd3",
   "metadata": {},
   "source": [
    "The text shows that we have several different elements, which are separated by a new line (\\n).\n",
    "The first separated element seems to be the author's name. We can also spot the story title and the number of shares and comments at the end of the string.\n",
    "\n",
    "Since newline characters in the current string separate all the information, we could just split the text by \"\\n\" and deduce by the order what each text section associates to which information. However, it is likely, that some answers will contain more or less data points, making the order of split elements not generalizable. Thus, a better way to obtain each piece of information is by selecting their elements.\n",
    "\n",
    "For example, we can find the title of the answer through the class name \"qu-userSelect--text\" with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ea1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = answer1.find_element(By.CLASS_NAME, \"qu-userSelect--text\").text\n",
    "print(\"the title is: \", title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb7beb9",
   "metadata": {},
   "source": [
    "Similarly, we can find the number of shares or comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53726a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shares = answer1.find_element(By.CLASS_NAME, \"dom_annotate_answer_action_bar_share\").text\n",
    "comments = answer1.find_element(By.CLASS_NAME, \"dom_annotate_answer_action_bar_comment\").text\n",
    "\n",
    "print(\"number of shares: \", shares)\n",
    "print(\"number of comments: \", comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b73f95",
   "metadata": {},
   "source": [
    "The numbers of shares and comments are still a string, but we can convert them into integers when we save the data in a data frame.\n",
    "\n",
    "Since we saved all answers of our feed in one variable, we can loop over it and extract all the information we are interested in. To make this process easier, we can define a function that extracts the information from an answer element and returns a list with the information we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2502cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the function to extract information from each answer post\n",
    "def get_post_info(PostInfo):\n",
    "    AuthorInfo = PostInfo.find_element(By.CLASS_NAME, \"qu-alignItems--flex-start\") # Container for author information \n",
    "    authorName = AuthorInfo.find_element(By.CLASS_NAME, \"qu-wordBreak--break-word\").text # author name\n",
    "    authorLink = AuthorInfo.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\") # author link\n",
    "   \n",
    "    title = PostInfo.find_element(By.CLASS_NAME, \"qu-userSelect--text\").text # answer title\n",
    "    \n",
    "    StoryLinkContainer = PostInfo.find_element(By.CLASS_NAME, \"qu-mb--tiny\").find_element(By.TAG_NAME, \"a\") # container for story link \n",
    "    StoryLink = StoryLinkContainer.get_attribute(\"href\") # get story link\n",
    "    \n",
    "    upvotes = PostInfo.find_element(By.CLASS_NAME, \"dom_annotate_answer_action_bar_upvote\").text # number of upvotes\n",
    "    shares = PostInfo.find_element(By.CLASS_NAME, \"dom_annotate_answer_action_bar_share\").text # number of shares\n",
    "    comments = PostInfo.find_element(By.CLASS_NAME, \"dom_annotate_answer_action_bar_comment\").text # number of comments\n",
    "    \n",
    "    # aggregate the answer data into a list\n",
    "    post_info = [authorName, authorLink, title, StoryLink, upvotes, shares, comments]\n",
    "    return post_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04affc3",
   "metadata": {},
   "source": [
    "Now, we need a loop to save the data into another list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnswersInfo = [] # initiate list to save data\n",
    "for post in Answers: # loop over all answers\n",
    "    AnswersInfo.append(get_post_info(post)) # add answer info to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc013a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look at the number of answers we gathered:\n",
    "len(AnswersInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84118b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnswersInfo[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31069acc",
   "metadata": {},
   "source": [
    "We collected only very few answers. Why is that?\n",
    "\n",
    "Because the website only loaded very few answers, and we need to scroll down on the website to generate more content. \n",
    "Luckily, Selenium can help us!\n",
    "\n",
    "We have several methods to imitate scrolling:\n",
    "\n",
    "   - scrolling by pixel \n",
    "   - scrolling to the bottom of the page\n",
    "\n",
    "Let's start with the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503cdaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. scoll down incrementally\n",
    "driver.execute_script(\"window.scrollTo(0, 1000)\")\n",
    "sleep(1) \n",
    "driver.execute_script(\"window.scrollTo(0, 2000)\")\n",
    "sleep(1) \n",
    "driver.execute_script(\"window.scrollTo(0, 3000)\")\n",
    "sleep(1) \n",
    "driver.execute_script(\"window.scrollTo(0, 4000)\")\n",
    "sleep(1) \n",
    "driver.execute_script(\"window.scrollTo(0, 5000)\")\n",
    "sleep(1) \n",
    "driver.execute_script(\"window.scrollTo(0, 6000)\")\n",
    "sleep(1) \n",
    "driver.execute_script(\"window.scrollTo(0, 7000)\")\n",
    "sleep(1) \n",
    "driver.execute_script(\"window.scrollTo(0, 8000)\")\n",
    "sleep(1) \n",
    "driver.execute_script(\"window.scrollTo(0, 9000)\")\n",
    "sleep(1) \n",
    "driver.execute_script(\"window.scrollTo(0, 10000)\")\n",
    "sleep(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa72459",
   "metadata": {},
   "source": [
    "We can also use another approach, but first, we can scroll to the top of the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to the top of the page\n",
    "driver.execute_script(\"window.scrollTo(0, -document.body.scrollHeight);\")\n",
    "sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8bdfed",
   "metadata": {},
   "source": [
    "For option two, we scroll to the bottom of the page, indicated by the document height of the website. Similarly, as before, we can repeat that process to load more content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d95482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to the bottom of the page - here we give the browser a bit more time to load all the content\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "sleep(2)\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "sleep(2) \n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "sleep(2) \n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "sleep(2) \n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "sleep(2)\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "sleep(2)\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "sleep(2) \n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "sleep(2) \n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "sleep(2) \n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "sleep(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d88e97",
   "metadata": {},
   "source": [
    "Of course, we can also implement loops for each of those processes, but for now, we loaded enough content.\n",
    "\n",
    "To grab the newly loaded content, we need to find the newsfeed again and find all answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5349940",
   "metadata": {},
   "outputs": [],
   "source": [
    "NewsFeed = driver.find_element(By.CLASS_NAME, \"dom_annotate_multifeed_home\")\n",
    "Answers = NewsFeed.find_elements(By.CLASS_NAME, \"dom_annotate_multifeed_bundle_AnswersBundle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4191270",
   "metadata": {},
   "source": [
    "Finally, we can re-run our loop to grab all the info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b46d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnswersInfo = []\n",
    "for post in Answers:\n",
    "    AnswersInfo.append(get_post_info(post))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3639a670",
   "metadata": {},
   "source": [
    "Let's check the number of answers we collected this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59bebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6014f",
   "metadata": {},
   "source": [
    "Better! If we want to collect more data, we can implement more scrolling, but we collected enough information for demonstration purposes now.\n",
    "\n",
    "Next, we can convert the list into a data frame, making it easier for us to work with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f1e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnswersInfo_df = pd.DataFrame(AnswersInfo, columns=[\"author\", \"author_link\",\n",
    "                                                    \"title\", \"story_link\",\n",
    "                                                    \"num_upvotes\", \"num_shares\", \n",
    "                                                    \"num_comments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e523e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnswersInfo_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c9ae48",
   "metadata": {},
   "source": [
    "Once we are done working with the driver, we can close the current session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1c526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2f254f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Hint:</b> \n",
    "    \n",
    "We can do similar operations to  gather advertisements, posts, or questions information by accessing other classes, such as:\n",
    "- \"dom_annotate_multifeed_bundle_AdBundle\" for Ads\n",
    "- \"dom_annotate_multifeed_bundle_PostBundle\" for posts\n",
    "\n",
    "However, our function for accessing author and post information might have to be adapted for those classes.\n",
    "\n",
    "It is also worth noting that instead of looking at the browser and its behavior, we can also implement a headless browser that will function in the background without us seeing it (have a look at [this Stackoverflow link](https://stackoverflow.com/questions/53657215/running-selenium-with-headless-chrome-webdriver). You can specify those settings at the beginning with options like `options.add_argument(\"--headless\")`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2833822",
   "metadata": {},
   "source": [
    "### 4.3.4 Example: Searching for a specific question and gathering all answers to that question on Quora\n",
    "\n",
    "In this example, we want to log into quora, search for a specific question, click on it, specify all answers to it and then scrape them. We will do this using Google Chrome.\n",
    "\n",
    "Like before, we begin with starting the driver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce749d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding our e-mail address to the user-agent\n",
    "opts = Options()\n",
    "opts.add_argument(\"user-agent=Getting answers data; contact me through: [e-mail address]\")\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(options=opts)# initaite driver with new user-agent for Chrome\n",
    "# driver = webdriver.Firefox(options=opts)# initaite driver with new user-agent for Firefox\n",
    "\n",
    "# lets check if we changed our user-agent\n",
    "agent = driver.execute_script(\"return navigator.userAgent\")\n",
    "print(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef83af46",
   "metadata": {},
   "source": [
    "Then we go to the website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69338044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url to visit\n",
    "url_search = \"https://www.quora.com/\"\n",
    "# go to url\n",
    "driver.get(url_search)\n",
    "sleep(1.5) # set sleep time for 1.5 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16b8433",
   "metadata": {},
   "source": [
    "We enter our credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c916bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# providing log-in credentials to the website\n",
    "\n",
    "EMail_field = driver.find_element(By.XPATH, '//*[@id=\"email\"]') # Find e-mail field\n",
    "# specify your e-mail address\n",
    "my_email = \"ENTER YOUR E-MAIL ADDRESS\"\n",
    "\n",
    "EMail_field.send_keys(my_email) # sending the string to the e-mail field\n",
    "sleep(1.5)\n",
    "\n",
    "PW_field = driver.find_element(By.XPATH, '//*[@id=\"password\"]') # find password field\n",
    "# specify your password \n",
    "my_password = \"ENTER YOUR PASSWORD\"\n",
    "\n",
    "PW_field.send_keys(my_password) # sending the string to the password field\n",
    "sleep(1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a07200",
   "metadata": {},
   "source": [
    "And then we click on the login button:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_elements(By.CLASS_NAME, \"iyYUZT\")[4].click()\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff89facd",
   "metadata": {},
   "source": [
    "Like before, in case you encountered a recaptcha, just click on the check box by hand and then log in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b047e9aa",
   "metadata": {},
   "source": [
    "Rejecting cookies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda675f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejecting cookies\n",
    "driver.find_element(By.ID, \"onetrust-reject-all-handler\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa0351f",
   "metadata": {},
   "source": [
    "Changing the language to English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f83c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on menu\n",
    "driver.find_elements(By.CLASS_NAME, \"puppeteer_popper_reference\")[1].click()\n",
    "# click to select \"English\"\n",
    "driver.find_element(By.CSS_SELECTOR, \"div.qu-dynamicFontSize--button\").click()\n",
    "sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d718ad",
   "metadata": {},
   "source": [
    "Now we should find the search bar, and pass our question to it for searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6323738",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_field = driver.find_element(By.XPATH, '//*[@enterkeyhint=\"search\"]') # finding search field\n",
    "\n",
    "question = \"Will AI kill the art industry?\"\n",
    "\n",
    "search_field.send_keys(question) # sending the question string to the search field\n",
    "sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4350c8",
   "metadata": {},
   "source": [
    "It will look like this:\n",
    "\n",
    "<img src='images/search_question.png' width=\"900\" height=\"700\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de74e077",
   "metadata": {},
   "source": [
    "As you can see, a list of related questions show up. The first one is the one we are looking for, So we click on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1f21a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on the question link\n",
    "driver.find_elements(By.CLASS_NAME, \"iyYUZT\")[4].click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48d295",
   "metadata": {},
   "source": [
    "Now, a list of results are shown on the page. These results contain both the answers to the question and also some related topics. We want to have the answers only, so we need to click on the \"All related\" button, and then select the \"Answers\" option. You can see how it looks like here:\n",
    "\n",
    "<img src='images/answers.png' width=\"900\" height=\"700\" align=\"center\"/>\n",
    "\n",
    "We first click on the \"All related\" button:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545d6f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting answers to be shown\n",
    "driver.find_elements(By.CLASS_NAME, \"iyYUZT\")[17].click()\n",
    "sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001cb99d",
   "metadata": {},
   "source": [
    "Now, before we click on the Answers option, let's find the number of answers to the question and save it in a variable. We will need this later for scrolling down to the last answer, so we don't miss out any of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2217f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = driver.find_elements(By.CLASS_NAME, \"iyYUZT\")[19].text\n",
    "text = text.split()[1]\n",
    "text = text.replace(\"(\", \"\")\n",
    "text = text.replace(\")\", \"\")\n",
    "answers_count = int(text)\n",
    "\n",
    "answers_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e7bcd",
   "metadata": {},
   "source": [
    "Now we can click on the Answers option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85be8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_elements(By.CLASS_NAME, \"iyYUZT\")[19].click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5b96e",
   "metadata": {},
   "source": [
    "If we scroll down to the end of the page, it will show us the first 20 answers to the question. If we want to scroll down to the last answer, we need to scroll down to the end of the page, and we need to repeat this process n times, with n being the number of answers divided by 20, plus 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988fdba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(answers_count // 20 + 1):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    sleep(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538dc73f",
   "metadata": {},
   "source": [
    "Now that we have completely scrolled down to the end of the page, we can extract the infromation from the answers. We get help from BeautifulSoup to ease the process, and we keep the author names, dates, number of upvotes and the answer texts in their corresponding lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae316b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "authors = []\n",
    "dates = []\n",
    "upvotes = []\n",
    "texts = []\n",
    "\n",
    "for i in range (answers_count):\n",
    "    \n",
    "    # Finding the answers html code:\n",
    "    answer = driver.find_element(By.CLASS_NAME, \"dom_annotate_question_answer_item_\" + str(i))\n",
    "    soup = BeautifulSoup(answer.get_attribute('innerHTML'), 'lxml')\n",
    "    \n",
    "    # Finding the desired information and saving them to their lists:\n",
    "    authors.append(soup.find_all('a', {'class': \"dFkjrQ\"})[1].text)\n",
    "    dates.append(soup.find('a', {'class': \"answer_timestamp\"}).text)\n",
    "    texts.append(soup.find_all('div', {'class': \"iyYUZT\"})[3].text)\n",
    "    \n",
    "    # Some of the answers do not have upvotes, we add None objects to the list for them:\n",
    "    upvote = soup.find('span', {'class': \"q-text qu-whiteSpace--nowrap qu-display--inline-flex qu-alignItems--center qu-justifyContent--center\"})    \n",
    "    if upvote == None:\n",
    "        upvotes.append(upvote)\n",
    "    else:\n",
    "        upvotes.append(upvote.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971ac57",
   "metadata": {},
   "source": [
    "Now that we have all the information, we can make a dataframe to keep the data in a more structured way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4f5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "quora_search_question_df = pd.DataFrame([authors, dates, texts, upvotes]).transpose()\n",
    "quora_search_question_df.columns = ['author', 'date', 'text', 'upvotes']\n",
    "\n",
    "quora_search_question_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9394d",
   "metadata": {},
   "source": [
    "We will save the dataframe to the outputs folder as a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fef92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quora_search_question_df.to_csv('./outputs/quora search question dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69752689",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Hi again, Pouria, this session is getting really good. Can you also get the userid or user name of that text, upvotes, downvotes, and date information for each answer. It will be also great to explore how to collect user information on Quora? Then, let's save all answers with the question id (thinking that we can collect answers to more than one question) to a dataframe and csv file under the outputs folder. Thank you!\n",
    "    \n",
    "<b>Pouria's note:</b>\n",
    "I added the usernames, together with the upvotes, dates and texts. The resulting dataframe is then saved to a csv file in the `outputs` folder.\n",
    "I'm not sure if I quite understand what you mean with the rest of it; for getting the users' information, if you mean clicking on each user's link and then collecting information from the new page, it gets really complicated and I will need some time to add it to the notebook. Let's talk about it in the next meeting!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16476b2b",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "<b>Insight</b>\n",
    "    \n",
    "We can also start considering whatelse we can do after learning these practices, such as searching for questions with a specific keywords or looking for multiple questions and their answers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446860c0",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "<b>Additional resources</b>\n",
    "\n",
    "If you would like to explore web scraping further, [Scrapy](https://scrapy.org/) can be your next address for more complex web tasks. It uses less memory and CPU storage and supports data extraction from HTML sources as well. We can even extend its functionality. As we mentioned, it is great a great library for complex and larger projects and we can easily transfer existing projects into another project.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1a5117",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Add related Meet the Expert Talks in a additional resources format or what? to be decided with the team! https://www.gesis.org/en/services/sharing-knowledge/meet-the-experts/meet-the-experts-season-2-computational-social-science-and-digital-behavioral-data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425df720",
   "metadata": {},
   "source": [
    "## 4.4. Important notes (and challanges?)\n",
    "\n",
    "Web scraping provides us the daily data we need for analysis and decision making. While different types of task sizes and websites are available to obtain data from, the various libraries that we teach here can make our tasks easier and faster. We have talked about BeautifulSoup and Selenium, which show us that BeautifulSoup is more user-friendly and allows us to learn faster and begin web scraping smaller tasks in an efficient way. Whereas in Selenium, tasks are more complicated and the target website has a lot of java elements in its code. We can use Selenium to control every major web browser such as chrome, internet explorer, and Firefox. Our actions aren’t limited to loading web pages, we can also perform other actions that allow us better interact with the websites such as mouse clicks and filling forms. \n",
    "\n",
    "In all, we should have good knowledge of all the tools available so that we can choose the best tool depending on our tasks. As we have seen, we can comfortably use any of the libraries as they are all free and also open source. Having a community of developers to support us is a big plus as we develop our projects and use the libraries. The choice of one over the other however depends on the project we have at hand. They all have their pros and cons.\n",
    "\n",
    "The main issue with scraping is that most websites have some restrictions for scraping or simply do not allow it. While scraping, complying with the Term of Use of the websites is essential. Scraping different platforms and their differences cause biases in the data collection process as well. Researchers should be aware of the data collection policies of different websites.\n",
    "\n",
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Gizem will add more sentences here about challanges.. Pouria, you can also add your own experiences here as well.\n",
    "    \n",
    "https://realpython.com/beautiful-soup-web-scraper-python/#challenges-of-web-scraping\n",
    "</div>\n",
    "\n",
    "\n",
    "Also, technical and legal challenges: ethical concerns, request rate limits, ...\n",
    "Just because of the website has an API, it does not mean that API solutions are easier and every info you want; web scraping then can be a tailored solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3006893c",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Pouria, as you know now :), we should make sure about the reference structure with a consistent citing along the notebook!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe29c81",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[<a href='#destination1_'>1</a>] https://pypi.org/project/feedparser/ <a id='destination1'></a>\n",
    "\n",
    "[<a href='#destination1_'>2</a>] https://rss.com/blog/find-rss-feed/#:~:text=Right%20click%20on%20the%20website's,between%20the%20quotes%20after%20href%3D\n",
    "\n",
    "[<a href='#destination1_'>3</a>] https://dev.to/mr_destructive/feedparser-python-package-for-reading-rss-feeds-5fnc\n",
    "\n",
    "[<a href='#destination2_'>4</a>] https://developer.mozilla.org/en-US/docs/Learn/HTML/Introduction_to_HTML/Getting_started <a id='destination2'></a>\n",
    "\n",
    "[<a href='#destination3_'>5</a>] http://web.simmons.edu/~grabiner/comm244/weekfour/document-tree.html <a id='destination3'></a>\n",
    "\n",
    "[<a href='#destination4_'>6</a>] https://www.crummy.com/software/BeautifulSoup/bs4/doc/ <a id='destination4'></a>\n",
    "\n",
    "[<a href='#destination4_'>7</a>] Fabian's notebook from GESIS fall seminar 2021: https://colab.research.google.com/drive/1uKxOc8mXTE2b05uUq-YlijJYzOTgi5DZ#scrollTo=ao_sLGiOSu7Y\n",
    "\n",
    "[<a href='#destination5_'>8</a>] The Social Comquant Workshop 10 at https://github.com/strohne/autocol <a id='destination5'></a>\n",
    "\n",
    "\n",
    "Han, S., & Anderson, C. K. (2021). Web scraping for hospitality research: Overview, opportunities, and implications. Cornell Hospitality Quarterly, 62(1), 89-104.https://doi.org/10.1177/1938965520973587\n",
    "\n",
    "Singrodia, V., Mitra, A., & Paul, S. (2019). A review on web scrapping and its applications. In 2019 international conference on computer communication and informatics (ICCCI) (pp. 1-6). IEEE.\n",
    "\n",
    "Hillen, J. (2019). Web scraping for food price research. British Food Journal.\n",
    "Can we use a use case on the food prices, this study's pipeline looks informative: https://www.emerald.com/insight/content/doi/10.1108/BFJ-02-2019-0081/full/pdf?title=web-scraping-for-food-price-research\n",
    "\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Pouria, let's be aware of the ordering of these links below! At least for the first two ones are in order in the intro sections.\n",
    "</div>\n",
    "\n",
    "\n",
    "https://realpython.com/beautiful-soup-web-scraper-python/#reasons-for-web-scraping\n",
    "\n",
    "https://medium.com/pythoneers/the-fundamentals-of-web-scraping-using-python-its-libraries-6f146b91efb4\n",
    "\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "https://www.dataquest.io/blog/web-scraping-python-using-beautiful-soup/#tve-jump-1788432a71d\n",
    "\n",
    "https://developer.mozilla.org/en-US/docs/Web/HTML/Element\n",
    "\n",
    "https://medium.com/geekculture/web-scraping-cheat-sheet-2021-python-for-web-scraping-cad1540ce21c#b81d\n",
    "\n",
    "https://trends.google.com/trends/yis/2021/DE/\n",
    "\n",
    "https://blog.google/products/search/15-tips-getting-most-out-google-trends/\n",
    "\n",
    "https://limeproxies.netlify.app/blog/selenium-vs-beautifulsoup\n",
    "\n",
    "\n",
    "Do not miss checking out the Social Comquant Workshop 10 at: https://github.com/strohne/autocol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec15403",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "<b>Document information</b>\n",
    "\n",
    "Contact and main author: N. Gizem Bacaksizlar Turbic\n",
    "\n",
    "Contributors: Pouria Mirelmi, Felix Soldner, Haiko Lietz & ..?\n",
    "\n",
    "Acknowledgements: Fabian Floeck? ...\n",
    "\n",
    "Version date: XX. January 2023\n",
    "\n",
    "License: ...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91acc17",
   "metadata": {},
   "source": [
    "#### Notes to be removed before publication\n",
    "\n",
    "Reviewers: Felix Soldner & Felix Schmidt?\n",
    "\n",
    "Review intro\n",
    "\n",
    "Review and finish red boxes\n",
    "\n",
    "Add insight boxes more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c978d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
