{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e607ca26",
   "metadata": {},
   "source": [
    "<img src='images/header.png' style='height: 50px; float: left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70bc56f",
   "metadata": {},
   "source": [
    "## Introduction to Computational Social Science methods with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71218328",
   "metadata": {},
   "source": [
    "# Session D2. Macro-level network analysis and network modeling\n",
    "\n",
    "macro: descriptive\n",
    "\n",
    "micro-macro: network modeling, statistical models and prediction (Menczer 6.3.4 and McLevey 30 on SBM)\n",
    "\n",
    "...\n",
    "\n",
    "Brockman on distance in networks (https://www.science.org/doi/abs/10.1126/science.1245200)\n",
    "\n",
    "Facebook Small-World study\n",
    "\n",
    "https://www.nature.com/articles/s41598-018-26951-y\n",
    "\n",
    "Fariba's paper on perception biases\n",
    "\n",
    "CHECK PLATT'S BOOK\n",
    "\n",
    "...\n",
    "\n",
    "<div class='alert alert-block alert-success'>\n",
    "<b>In this session</b>, \n",
    "\n",
    "you will learn how you can ...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105572f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8799b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/CNS/G_cns_fb_lcc.pickle', 'rb') as f:\n",
    "    G_fb = pickle.load(f)\n",
    "with open('../data/CNS/MD_cns_communication_week1_lcc.pickle', 'rb') as f:\n",
    "    MD_communication = pickle.load(f)\n",
    "with open('../data/CNS/D_cns_communication_week1_lcc.pickle', 'rb') as f:\n",
    "    D_communication = pickle.load(f)\n",
    "with open('../data/CNS/MG_cns_communication_week1_lcc.pickle', 'rb') as f:\n",
    "    MG_communication = pickle.load(f)\n",
    "with open('../data/CNS/G_cns_communication_week1_lcc.pickle', 'rb') as f:\n",
    "    G_communication = pickle.load(f)\n",
    "with open('../data/CNS/MG_cns_bt_f2f_p_lcc.pickle', 'rb') as f:\n",
    "    MG_bt = pickle.load(f)\n",
    "with open('../data/CNS/G_cns_bt_f2f_p_lcc.pickle', 'rb') as f:\n",
    "    G_bt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c39ff0",
   "metadata": {},
   "source": [
    "## D2.1. Cohesion\n",
    "\n",
    "Menczer *et al.* (2020, chapter 2.3) on components\n",
    "\n",
    "McLevey (2022, chapter 14.5) on components\n",
    "\n",
    "Ma & Seth (2020, 55â€“70) on triangles, cliques, and components\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b244a0",
   "metadata": {},
   "source": [
    "### D2.1.1. Connectivity\n",
    "\n",
    "The lecture prepared the ground for cohesion analysis as the analytical process of modeling network cores. Quite generally, social networks with agents as nodes tend to weakly weighted and sparse, cultural networks with facts as nodes tend to be strongly weighted and dense. As a result, two kinds of cohesion analysis are required: graph theoretical methods and simple edge filtering. These are the two sections in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc68ab1f",
   "metadata": {},
   "source": [
    "#### Connected Components\n",
    "Connected components are **maximal subgraphs** of undirected Graphs in which all node pairs are connected by **paths**:\n",
    "\n",
    "McLevey (2022, chapter 14.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f7ab09",
   "metadata": {},
   "source": [
    "#### Cutpoints\n",
    "A cutpoint is a node whose removal will increase the number of connected components. The method is not implemented for DiGraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25222a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer(G, layer, prune=True):\n",
    "    def filter_edge(u, v, key):\n",
    "        return G.has_edge(u, v, layer)\n",
    "    G_subgraph = nx.subgraph_view(G=G, filter_edge=filter_edge)\n",
    "    if prune:\n",
    "        def filter_node(v):\n",
    "            return G_subgraph.degree[v] > 0\n",
    "        G_subgraph_pruned = nx.subgraph_view(G=G_subgraph, filter_node=filter_node)\n",
    "        return nx.Graph(G_subgraph_pruned)\n",
    "    else:\n",
    "        return nx.Graph(G_subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734ae32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_bt_snapshot = get_layer(G=MG_bt, layer=0, prune=True)\n",
    "G_bt_snapshot_lcc = G_bt_snapshot.subgraph(nodes=sorted(nx.connected_components(G_bt_snapshot), key=len, reverse=True)[0])\n",
    "pos_bt_snapshot_lcc = nx.kamada_kawai_layout(G=G_bt_snapshot_lcc, weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87268c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.articulation_points(G_bt_snapshot_lcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473bb2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_cut = [v for v in sorted(nx.articulation_points(G_bt_snapshot_lcc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlights(G, nodes):\n",
    "    d = dict(zip(G.nodes, G.number_of_nodes() * ['white']))\n",
    "    for node in nodes:\n",
    "        d.update({node: 'red'})\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb19caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(\n",
    "    G = G_bt_snapshot_lcc, \n",
    "    pos = pos_bt_snapshot_lcc, \n",
    "    node_size = 40, \n",
    "    node_color = list(highlights(G=G_bt_snapshot_lcc, nodes=nodes_cut).values()), \n",
    "    alpha = .5, \n",
    "    edgecolors = 'black'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec276d3",
   "metadata": {},
   "source": [
    "Attack on scale-free network (Platt 82-3), Menczer *et al.* (2020, chapter 3.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b938996",
   "metadata": {},
   "source": [
    "#### Minimum cuts\n",
    "\n",
    "Platt 83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9c3c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.algorithms.connectivity.minimum_st_node_cut(G=G_bt_snapshot_lcc, s=3, t=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5955c611",
   "metadata": {},
   "source": [
    "#### Node connectivity\n",
    "\n",
    "Platt 84-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9410238",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.node_connectivity(G_bt_snapshot_lcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa47f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.average_node_connectivity(G_bt_snapshot_lcc) # takes long for large networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5bb40f",
   "metadata": {},
   "source": [
    "#### Bicomponents\n",
    "Biconnected components or bicomponents are maximal subgraphs that cannot be disconnected by the removal of any single node. The method is not implemented for DiGraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00419c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.biconnected_components(G_bt_snapshot_lcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c38ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_nodes_bicomp = [c for c in sorted(nx.biconnected_components(G_bt_snapshot_lcc), key=len, reverse=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cbc869",
   "metadata": {},
   "source": [
    "Largest bicomponent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836b6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_lbicomp = max(nx.biconnected_components(G_bt_snapshot_lcc), key=len) # or:\n",
    "nodes_lbicomp = sets_nodes_bicomp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a7bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(\n",
    "    G = G_bt_snapshot_lcc, \n",
    "    pos = pos_bt_snapshot_lcc, \n",
    "    node_size = 40, \n",
    "    node_color = list(highlights(G=G_bt_snapshot_lcc, nodes=nodes_lbicomp).values()), \n",
    "    alpha = .5, \n",
    "    edgecolors = 'black'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7377ddd1",
   "metadata": {},
   "source": [
    "#### k-components\n",
    "$k$-components are maximal subgraphs that cannot be disconnected by the removal of by the removal of any $k$ nodes. Bicomponents are the special case for $k=2$. The method may identify multiple overlapping $k$-components with similar connectivity $k$. It is not implemented for DiGraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d96b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kcomp = nx.k_components(G_bt_snapshot_lcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d06e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "sets_nodes_kcomp = [c for c in sorted(kcomp[k], key=len, reverse=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54444a9d",
   "metadata": {},
   "source": [
    "Largest k-component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e61cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_lkcomp = sets_nodes_kcomp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c8a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(\n",
    "    G = G_bt_snapshot_lcc, \n",
    "    pos = pos_bt_snapshot_lcc, \n",
    "    node_size = 40, \n",
    "    node_color = list(highlights(G=G_bt_snapshot_lcc, nodes=nodes_lkcomp).values()), \n",
    "    alpha = .5, \n",
    "    edgecolors = 'black'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b9233d",
   "metadata": {},
   "source": [
    "All k-comps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b359531",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_kcomp = [node for set_kcomp in sets_nodes_kcomp for node in set_kcomp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b015ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(\n",
    "    G = G_bt_snapshot_lcc, \n",
    "    pos = pos_bt_snapshot_lcc, \n",
    "    node_size = 40, \n",
    "    node_color = list(highlights(G=G_bt_snapshot_lcc, nodes=nodes_kcomp).values()), \n",
    "    alpha = .5, \n",
    "    edgecolors = 'black'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff8c201",
   "metadata": {},
   "source": [
    "Density plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kcomp_number(G, kcomp):\n",
    "    d = {node: 1 for node in G.nodes()}\n",
    "    for k in range(2, len(kcomp)+1):\n",
    "        for node in [node for set_kcomp in kcomp[k] for node in set_kcomp]:\n",
    "            d[node] = k\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418324e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aa7ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(\n",
    "    G = G_bt_snapshot_lcc, \n",
    "    pos = pos_bt_snapshot_lcc, \n",
    "    node_size = 40, \n",
    "    node_color = list(kcomp_number(G=G_bt_snapshot_lcc, kcomp=kcomp).values()), \n",
    "    #alpha = .5, \n",
    "    edgecolors = 'black', \n",
    "    cmap = plt.cm.hot\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f2e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cm.hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ace196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a9e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kcomp_density = pd.DataFrame(data=pos_bt_snapshot_lcc.values(), index=G_bt_snapshot_lcc.nodes(), columns=['x', 'y'])\n",
    "kcomp_density.loc[:, 'weights'] = kcomp_number(G=G_bt_snapshot_lcc, kcomp=kcomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a185675",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[6, 4.5])\n",
    "ax = sns.kdeplot(data=kcomp_density, x='x', y='y', weights='weights', alpha=.8, fill=True)\n",
    "ax.set(xticklabels=[], yticklabels=[])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb29e02",
   "metadata": {},
   "source": [
    "### D2.1.2. Assortativity and homophily\n",
    "\n",
    "Menczer *et al.* (2020, chapter 2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184f15a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.degree_assortativity_coefficient(G_fb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5c3034",
   "metadata": {},
   "source": [
    "Homophily analysis as cohesion\n",
    "\n",
    "Size-corrected measure from Fariba?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afc8803",
   "metadata": {},
   "source": [
    "The sex categories are 0: male; 1: female; 2: unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a34377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.attribute_assortativity_coefficient(G=G_fb, attribute='sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d22f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.attribute_mixing_matrix(G=G_fb, attribute='sex', normalized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2ec9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c88dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_density_matrix(G, attribute):\n",
    "    l = [data[attribute] for v, data in G.nodes(data=True)]\n",
    "    counts = list(Counter(l).values())\n",
    "    a = np.empty(shape=(len(counts), len(counts)))\n",
    "    for i in range(len(counts)):\n",
    "        for j in range(len(counts)):\n",
    "            if i == j:\n",
    "                a[i, j] = counts[i] * (counts[j] - 1)\n",
    "            else:\n",
    "                a[i, j] = counts[i] * counts[j]\n",
    "    return nx.attribute_mixing_matrix(G=G, attribute=attribute, normalized=False) / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e5784",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fb_sex = attribute_density_matrix(G=G_fb, attribute='sex').round(4)\n",
    "p_fb_sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fb = nx.density(G_fb)\n",
    "p_fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c89fcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "(p_fb_sex / p_fb).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ad655",
   "metadata": {},
   "source": [
    "### D2.2. Distance and search\n",
    "\n",
    "McLevey (2022, chapter 15) on degree inequality\n",
    "\n",
    "Menczer *et al.* (2020, chapter 2.2, 2.6, 2.7, 2.8) on the Small World\n",
    "\n",
    "Platt ch6\n",
    "\n",
    "Small-World paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of nodes\n",
    "n_fb = G_fb.number_of_nodes()\n",
    "n_communication = G_communication.number_of_nodes()\n",
    "n_bt = G_bt.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc629a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of edges\n",
    "m_fb = G_fb.number_of_edges()\n",
    "m_communication = G_communication.number_of_edges()\n",
    "m_bt = G_bt.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667cded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# density\n",
    "p_fb = nx.density(G_fb)\n",
    "p_communication = nx.density(G_communication)\n",
    "p_bt = nx.density(G_bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec09440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average clustering coefficient\n",
    "c_fb = nx.average_clustering(G_fb)\n",
    "c_communication = nx.average_clustering(G_communication)\n",
    "c_bt = nx.average_clustering(G_bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226861bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average shortest path length\n",
    "l_fb = nx.average_shortest_path_length(G_fb)\n",
    "l_communication = nx.average_shortest_path_length(G_communication)\n",
    "l_bt = nx.average_shortest_path_length(G_bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e6cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.DataFrame(index=['Facebook friendship', 'Text messages & phone calls', 'Face-to-face interaction'])\n",
    "stats['n'] = [n_fb, n_communication, n_bt]\n",
    "stats['m'] = [m_fb, m_communication, m_bt]\n",
    "stats['p'] = [p_fb, p_communication, p_bt]\n",
    "stats['c'] = [c_fb, c_communication, c_bt]\n",
    "stats['l'] = [l_fb, l_communication, l_bt]\n",
    "stats[['p', 'c', 'l']] = stats[['p', 'c', 'l']].round(2)\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25df11e9",
   "metadata": {},
   "source": [
    "What is a random graph? Erdos Renyi? Why do we care to compare the empirical graph to random graph? The point behind is to see if our empirical graph has unique features to observe than the random graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19895fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_fb_random = nx.erdos_renyi_graph(n=n_fb, p=p_fb, seed=42)\n",
    "G_communication_random = nx.erdos_renyi_graph(n=n_communication, p=p_communication, seed=42)\n",
    "MG_bt_random = nx.erdos_renyi_graph(n=n_bt, p=p_bt, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2026eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical = [G_fb, G_communication, G_bt]\n",
    "titles_empirical = ['Facebook friendship', 'Empirical networks\\n\\n Text messages & phone calls', 'Face-to-face interaction']\n",
    "random = [G_fb_random, G_communication_random, MG_bt_random]\n",
    "titles_random = ['', 'Corresponding random networks', '']\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "for i in range(3):\n",
    "    axs[0, i].set_title(titles_empirical[i])\n",
    "    nx.draw(\n",
    "        G = empirical[i], \n",
    "        pos = nx.get_node_attributes(G=empirical[i], name='pos'), \n",
    "        ax = axs[0, i], \n",
    "        node_size = 20, \n",
    "        alpha = .5\n",
    "    )\n",
    "for i in range(3):\n",
    "    axs[1, i].set_title(titles_random[i])\n",
    "    nx.draw(\n",
    "        G = random[i], \n",
    "        ax = axs[1, i], \n",
    "        node_size = 20, \n",
    "        alpha = .5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c9cc0",
   "metadata": {},
   "source": [
    "Normalized values calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19b654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.log(n_fb) - .57722) / np.log(2 * m_fb / n_fb) + .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd424d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_fb_random = (np.log(n_fb) - .57722) / np.log(2 * m_fb / n_fb) + .5\n",
    "l_communication_random = (np.log(n_communication) - .57722) / np.log(2 * m_communication / n_communication) + .5\n",
    "l_bt_random = (np.log(n_bt) - .57722) / np.log(2 * m_bt / n_bt) + .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d48581",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats['l_random'] = [l_fb_random, l_communication_random, l_bt_random]\n",
    "stats['c_norm'] = stats['c'] / stats['p']\n",
    "stats['l_norm'] = stats['l'] / stats['l_random']\n",
    "stats['q'] = stats['c_norm'] / stats['l_norm']\n",
    "stats[['l_random', 'c_norm', 'l_norm', 'q']] = stats[['l_random', 'c_norm', 'l_norm', 'q']].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b2ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b796b11",
   "metadata": {},
   "source": [
    "### D2.3. Inequality and hierarchy\n",
    "\n",
    "What is inequality? How we can measure it in social networks? Why do we need entropy?\n",
    "\n",
    "Menczer *et al.* (2020, chapter 3.2) on centrality distributions, log-log, (3.3) friendship paradox\n",
    "\n",
    "Scale-free network paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c851a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodelist_fb = pd.DataFrame(index=G_fb.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efffffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodelist_fb['degree'] = [k for v, k in G_fb.degree()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e9f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import powerlaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_degree_fb = powerlaw.Fit(data=nodelist_fb['degree'], xmin=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e28647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[3, 3])\n",
    "fig = fit_degree_fb.plot_pdf(marker='o', linestyle='', label='Facebook friendships')\n",
    "fit_degree_fb.power_law.plot_pdf(linestyle='--', label='Power law')\n",
    "fit_degree_fb.exponential.plot_pdf(linestyle='-', label='Exponential')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of friends')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70611436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# centralization\n",
    "# Platt 85\n",
    "1 - entropy(pk=nodelist_fb['degree']) / np.log(len(nodelist_fb['degree']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd9a7a",
   "metadata": {},
   "source": [
    "#### Hierarchical modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ee49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodelist_fb['log_degree'] = np.log10(nodelist_fb['degree'])\n",
    "nodelist_fb['clustering'] = list(nx.clustering(G_fb).values())\n",
    "nodelist_fb['log_clustering'] = np.log10(nodelist_fb['clustering'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aeb626",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = sns.jointplot(\n",
    "    data = nodelist_fb[nodelist_fb['clustering'] > 0], \n",
    "    x = 'log_degree', \n",
    "    y = 'log_clustering', \n",
    "    kind = 'reg', \n",
    "    joint_kws={'scatter_kws':{'alpha': .5}, 'line_kws':{'color': 'orange'}}\n",
    ")\n",
    "plot.fig.set_figwidth(3)\n",
    "plot.fig.set_figheight(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd66eec5",
   "metadata": {},
   "source": [
    "#### Flow hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b1c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.flow_hierarchy(D_communication)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050e744",
   "metadata": {},
   "source": [
    "### D2.4. Network modeling\n",
    "\n",
    "Menczer *et al.* (2020, chapter 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0338fc7e",
   "metadata": {},
   "source": [
    "#### Stochastic Blockmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f61a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes_sbm = list(Counter([data['sex'] for v, data in G_fb.nodes(data=True)]).values())\n",
    "sizes_sbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a84a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attr_sbm = list(np.repeat(range(len(sizes_sbm)), sizes_sbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56788787",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sbm = nx.stochastic_block_model(sizes=sizes_sbm, p=p_fb_sex, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacad839",
   "metadata": {},
   "source": [
    "To avoid that red nodes are always on top of blue nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9bdcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = list(range(G_sbm.number_of_nodes()))\n",
    "old_labels = new_labels.copy()\n",
    "np.random.shuffle(new_labels)\n",
    "mapping = dict(zip(old_labels, new_labels))\n",
    "\n",
    "G_sbm = nx.relabel_nodes(G_sbm, mapping)\n",
    "node_attr_sbm = [x for _, x in sorted(zip(new_labels, node_attr_sbm))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a84f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(\n",
    "    G = G_sbm, \n",
    "    node_size = 40, \n",
    "    node_color = node_attr_sbm, \n",
    "    alpha = .5, \n",
    "    cmap = plt.cm.brg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc7af62",
   "metadata": {},
   "source": [
    "`G_fb_sbm` is an improvement of `G_fb_random`: it exhibts the correct attribute mixing pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7b00a7",
   "metadata": {},
   "source": [
    "#### Watts-Strogatz model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_ring = 0\n",
    "k = 2\n",
    "while np.mean(degree_ring) < np.mean(nodelist_fb['degree']):\n",
    "    G_ring = nx.watts_strogatz_graph(n=G_fb.number_of_nodes(), k=k, p=0.)\n",
    "    degree_ring = [k for v, k in G_ring.degree()]\n",
    "    k += 1\n",
    "print('Each node is joined with its %.f nearest neighbors in a ring topology.' %k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c5a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.density(G=G_fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b571af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.density(G=G_ring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e99e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458966ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(list(nx.clustering(G_ring).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83079b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(\n",
    "    G = G_ring, \n",
    "    pos = nx.circular_layout(G=G_ring), \n",
    "    node_size = 40, \n",
    "    alpha = .5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_ws = 800\n",
    "p = 0.\n",
    "while l_ws > l_fb:\n",
    "    G_ws = nx.watts_strogatz_graph(n=G_fb.number_of_nodes(), k=19, p=p)\n",
    "    l_ws = nx.average_shortest_path_length(G=G_ws)\n",
    "    p += .01\n",
    "print('The probability of rewiring each edge is %.2f.' %p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d1b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.average_clustering(G_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629308f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(\n",
    "    G = G_ws, \n",
    "    pos = nx.circular_layout(G_ws), \n",
    "    node_size = 40, \n",
    "    alpha = .5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx.draw(\n",
    "#    G = G_ws, \n",
    "#    node_size = 40, \n",
    "#    alpha = .5\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239d541e",
   "metadata": {},
   "source": [
    "#### BarabÃ¡si-Albert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ba = 0\n",
    "m = 1\n",
    "while p_ba < p_fb:\n",
    "    G_ba = nx.barabasi_albert_graph(n=G_fb.number_of_nodes(), m=m)\n",
    "    p_ba = nx.density(G=G_ba)\n",
    "    m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.density(G_fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8563c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.density(G_ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf597be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodelist_fb['degree'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c529f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_ba = [k for v, k in G_ba.degree()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb9de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(degree_ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_degree_ba = powerlaw.Fit(data=degree_ba, xmin=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1eaaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[3, 3])\n",
    "fig = fit_degree_fb.plot_pdf(marker='o', linestyle='', label='Facebook friendships')\n",
    "fit_degree_ba.plot_pdf(marker='o', linestyle='', label='BarabÃ¡si-Albert model')\n",
    "plt.legend()\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e09dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx.draw(\n",
    "#    G = G_ba, \n",
    "#    node_size = degree_ba, \n",
    "#    alpha = .5\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a3a6c2",
   "metadata": {},
   "source": [
    "#### Model overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = [G_fb, G_sbm, G_ws, G_ba]\n",
    "node_sizes = [20, 20, degree_ba, 20]\n",
    "node_colors = [[data['sex'] for v, data in G_fb.nodes(data=True)], node_attr_sbm, None, None]\n",
    "titles = ['Facebook friendship', 'Stochastic Blockmodel', 'Watts-Strogatz model', 'BarabÃ¡si-Albert model']\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i in range(4):\n",
    "    axs[i].set_title(titles[i])\n",
    "    nx.draw(\n",
    "        G = networks[i], \n",
    "        ax = axs[i], \n",
    "        node_size = node_sizes[i], \n",
    "        node_color = node_colors[i], \n",
    "        alpha = .5, \n",
    "        cmap = plt.cm.brg\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32405bae",
   "metadata": {},
   "source": [
    "## D2.5. Hashtag co-occurrence network analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d08526",
   "metadata": {},
   "source": [
    "#### Small-World analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea0162",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = pd.read_csv('../data/TweetsCOV19/TweetsCOV19_tables/hashtags.tsv.gz', sep='\\t', index_col=None, encoding='utf-8')\n",
    "hashtag_cooccurrences = pd.read_csv('../data/TweetsCOV19/hashtag_cooccurrences.tsv.gz', sep='\\t', index_col=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags['tweets_norm'] = hashtag_cooccurrences[hashtag_cooccurrences['hashtag_idx_i'] == hashtag_cooccurrences['hashtag_idx_j']].reset_index(drop=True)['cooccurrence_norm']\n",
    "hashtag_cooccurrences = hashtag_cooccurrences[hashtag_cooccurrences['hashtag_idx_i'] != hashtag_cooccurrences['hashtag_idx_j']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57842af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_hashtag_cooccurrences = nx.from_pandas_edgelist(\n",
    "    df = hashtag_cooccurrences, \n",
    "    source = 'hashtag_idx_i', \n",
    "    target = 'hashtag_idx_j', \n",
    "    #edge_attr = ['cooccurrence', 'cooccurrence_norm'], \n",
    "    create_using = nx.Graph\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fafea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_hashtag_cooccurrences_lcc = G_hashtag_cooccurrences.subgraph(sorted(nx.connected_components(G_hashtag_cooccurrences), key=len, reverse=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbdacdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add figure of network drawn in graph-tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f23892",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hashtags = G_hashtag_cooccurrences_lcc.number_of_nodes()\n",
    "m_hashtags = G_hashtag_cooccurrences_lcc.number_of_edges()\n",
    "p_hashtags = nx.density(G=G_hashtag_cooccurrences_lcc)\n",
    "c_hashtags = 0.75261451\n",
    "l_hashtags = 3.27701117\n",
    "l_hashtags_random = (np.log(n_hashtags) - .57722) / np.log(2 * m_hashtags / n_hashtags) + .5\n",
    "c_hashtags_norm = c_hashtags / p_hashtags\n",
    "l_hashtags_norm = l_hashtags / l_hashtags_random\n",
    "q_hashtags = c_hashtags_norm / l_hashtags_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512bd66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.loc['Hashtag co-occurrences'] = [\n",
    "    n_hashtags, \n",
    "    m_hashtags, \n",
    "    p_hashtags, \n",
    "    c_hashtags, \n",
    "    l_hashtags, \n",
    "    l_hashtags_random, \n",
    "    c_hashtags_norm, \n",
    "    l_hashtags_norm, \n",
    "    q_hashtags\n",
    "]\n",
    "stats[['n', 'm']] = stats[['n', 'm']].astype(int)\n",
    "stats[['p']] = stats[['p']].round(5)\n",
    "stats[['c', 'l', 'l_random', 'c_norm', 'l_norm', 'q']] = stats[['c', 'l', 'l_random', 'c_norm', 'l_norm', 'q']].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e5d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c_hashtags = nx.average_clustering(G=G_hashtag_cooccurrences_lcc) # takes about 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c32e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#l_hashtags = nx.average_shortest_path_length(G=G_hashtag_cooccurrences_lcc) # takes very long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d453f03",
   "metadata": {},
   "source": [
    "BOX\n",
    "\n",
    "NX\n",
    "...\n",
    "\n",
    "GT\n",
    "10sec (computing c)\n",
    "13min (computing l)\n",
    "36min (processing l property map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbf562b",
   "metadata": {},
   "source": [
    "#### Scale-free analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_tweets_norm = powerlaw.Fit(data=hashtags['tweets_norm'], xmin=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ae1469",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_tweets_norm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4a5440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[3, 3])\n",
    "fig = fit_tweets_norm.plot_pdf(marker='o', linestyle='', label='Hashtags')\n",
    "fit_tweets_norm.power_law.plot_pdf(linestyle='-', label='Power law')\n",
    "fit_tweets_norm.exponential.plot_pdf(linestyle='--', label='Exponential')\n",
    "plt.ylim(3.29e-10, 1.38)\n",
    "plt.legend()\n",
    "plt.xlabel('Normalized number of tweets')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbbc27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#centralization\n",
    "1 - entropy(pk=hashtags['tweets_norm']) / np.log(len(hashtags['tweets_norm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f7c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx.degree_assortativity_coefficient(G=G_hashtag_cooccurrences_lcc) # takes long -- how long?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575d1ed4",
   "metadata": {},
   "source": [
    "#### Core detection by filtering co-selections\n",
    "This method does not take the structure of relations into account. Cores are identified simply by **removing weakly weighted edges**. For the purpose of demonstrating this, we will work with the Social Network Science dataset and the use of scholarly languagein that field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e405caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = hashtags.reset_index().sort_values(by='tweets_norm', ascending=False)\n",
    "hashtags['cum_attention'] = (hashtags['tweets_norm'] / hashtags['tweets_norm'].sum()).cumsum()\n",
    "hashtags = pd.merge(\n",
    "    left = hashtags[['index', 'hashtag', 'tweets', 'tweets_norm']], \n",
    "    right = hashtags[['tweets_norm', 'cum_attention']].groupby(by='tweets_norm').max(), \n",
    "    on = 'tweets_norm'\n",
    ").sort_values(by='index', ascending=True).set_index(keys='index')\n",
    "hashtags.index.name = None\n",
    "hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_cooccurrences = hashtag_cooccurrences.sort_values(by='cooccurrence_norm', ascending=False)\n",
    "hashtag_cooccurrences['cum_attention'] = (hashtag_cooccurrences['cooccurrence_norm'] / hashtag_cooccurrences['cooccurrence_norm'].sum()).cumsum()\n",
    "hashtag_cooccurrences = pd.merge(\n",
    "    left = hashtag_cooccurrences[['hashtag_idx_i', 'hashtag_idx_j', 'cooccurrence', 'cooccurrence_norm']], \n",
    "    right = hashtag_cooccurrences[['cooccurrence_norm', 'cum_attention']].groupby(by='cooccurrence_norm').max().reset_index(), \n",
    "    on = 'cooccurrence_norm'\n",
    ")\n",
    "hashtag_cooccurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851d83f0",
   "metadata": {},
   "source": [
    "Filter network to the core corresponding to the top 10% of the attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c631bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_cooccurrences_attention = hashtag_cooccurrences[hashtag_cooccurrences['cum_attention'] <= .1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58411f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hashtags = hashtags.sort_values(by='cum_attention', ascending=False)\n",
    "#hashtag_cooccurrences_attention = hashtag_cooccurrences_attention.sort_values(by='cum_attention', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5587845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr_values(df, type_of_list, attr, node='name', source='name_u', target='name_v'):\n",
    "    '''\n",
    "    Transforms one or multiple columns of a nodelist or edgelist into a dictionary that can be used to set the node or edge attributes of a graph.\n",
    "    \n",
    "    Parameters:\n",
    "        df : Pandas DataFrame\n",
    "            Nodelist or edgelist that contains the node or edge attributes.\n",
    "        type_of_list : String, either 'nodes' or 'edges'\n",
    "            To specify if attribute values should be created for nodes or edges.\n",
    "        attr : List\n",
    "            List containing the names of the columns in df that contain the attribute values.\n",
    "        name : String, only required if df is a 'nodes' type_of_list, default 'name'\n",
    "            Column name of the nodelist containing the node labels.\n",
    "        source : String, only required if df is an 'edges' type_of_list, default 'name_u'\n",
    "            Column name of the edgelist containing the source node labels.\n",
    "        target : String, only required if df is an 'edges' type_of_list, default 'name_v'\n",
    "            Column name of the edgelist containing the target node labels.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with nodes or edges as keys and an attribute dictionary as values.\n",
    "    '''\n",
    "    df_ = df.copy()\n",
    "    df_ = pd.DataFrame([df_[attr].to_dict('records')]).T\n",
    "    if type_of_list == 'nodes':\n",
    "        df_.index = df[node]\n",
    "    if type_of_list == 'edges':\n",
    "        df_.index = list(zip(df[source], df[target]))\n",
    "    return list(df_.to_dict().values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c32fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_hashtag_cooccurrences_core = nx.from_pandas_edgelist(\n",
    "    df = hashtag_cooccurrences_attention, \n",
    "    source = 'hashtag_idx_i', \n",
    "    target = 'hashtag_idx_j', \n",
    "    edge_attr = ['cooccurrence', 'cooccurrence_norm'], \n",
    "    create_using = nx.Graph\n",
    ")\n",
    "#G_hashtag_cooccurrences_core.add_nodes_from(hashtags.index)\n",
    "node_attr_hashtags = get_attr_values(df=hashtags.reset_index(), type_of_list='nodes', attr=['hashtag', 'tweets', 'tweets_norm'], node='index')\n",
    "nx.set_node_attributes(G=G_hashtag_cooccurrences_core, values=node_attr_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be483434",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_cc = sorted(nx.connected_components(G_hashtag_cooccurrences_core), key=len, reverse=True)\n",
    "G_hashtag_cooccurrences_core_lcc = G_hashtag_cooccurrences_core.subgraph(l_cc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d655ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_hashtag_cooccurrences_core_lcc_spring = nx.spring_layout(\n",
    "    G = G_hashtag_cooccurrences_core_lcc, \n",
    "    weight = 'weight'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e829fe48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_of_labels = 9 # G_hashtag_cooccurrences_core_lcc.number_of_nodes()\n",
    "\n",
    "plt.figure(figsize=[12, 12])\n",
    "nx.draw_networkx_nodes(\n",
    "    G = G_hashtag_cooccurrences_core_lcc, \n",
    "    pos = pos_hashtag_cooccurrences_core_lcc_spring, \n",
    "    node_size = [node_size/20 for node_size in list(nx.get_node_attributes(G=G_hashtag_cooccurrences_core_lcc, name='tweets_norm').values())], \n",
    "    node_color = 'white', \n",
    "    alpha = .5, \n",
    "    linewidths = 1., \n",
    "    edgecolors = 'black'\n",
    ")\n",
    "nx.draw_networkx_edges(\n",
    "    G = G_hashtag_cooccurrences_core_lcc, \n",
    "    pos = pos_hashtag_cooccurrences_core_lcc_spring, \n",
    "    width = [width/40 for width in list(nx.get_edge_attributes(G=G_hashtag_cooccurrences_core_lcc, name='cooccurrence_norm').values())], \n",
    "    edge_color = 'black', \n",
    "    alpha = .25, \n",
    "    node_size = [node_size/20 for node_size in list(nx.get_node_attributes(G=G_hashtag_cooccurrences_core_lcc, name='tweets_norm').values())]\n",
    ")\n",
    "nodes = hashtags.sort_values(by='tweets_norm', ascending=False).index.intersection(G_hashtag_cooccurrences_core_lcc.nodes())[:number_of_labels]\n",
    "for i in reversed(range(number_of_labels)):\n",
    "    nx.draw_networkx_labels(\n",
    "        G = G_hashtag_cooccurrences_core_lcc, \n",
    "        pos = pos_hashtag_cooccurrences_core_lcc_spring, \n",
    "        labels = {nodes[i]: nx.get_node_attributes(G=G_hashtag_cooccurrences_core_lcc, name='hashtag')[nodes[i]]}, \n",
    "        font_size = int(nx.get_node_attributes(G=G_hashtag_cooccurrences_core_lcc, name='tweets_norm')[nodes[i]]**(1/3)), \n",
    "        font_color = plt.cm.Set1(i/number_of_labels), \n",
    "        alpha = 1.\n",
    "    )\n",
    "plt.box(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a392e5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b98d082a",
   "metadata": {},
   "source": [
    "## Commented references\n",
    "\n",
    "### Recommended textbooks and NetworkX resources\n",
    "\n",
    "Ma, E. & Seth, M. (2022). *Network Analysis Made Simple*. LeanPub. https://leanpub.com/nams. *An easy, well-maintained, and self-explanatory resource for NetworkX self-education that hides the inner workings of algorithms in custom functions. Use the [website](https://ericmjl.github.io/Network-Analysis-Made-Simple/) as a guide to Jupyter Notebooks on constructing, importing, exporting, and analyzing networks. You can also launch a binder session to execute notebooks in the cloud.*\n",
    "\n",
    "Menczer, F., Fortunato, S., & Davis, C. A. (2020). *A First Course in Network Science*. Cambridge University Press. https://doi.org/10.1017/9781108653947. *An introductory course with exercises that emerged from years of teaching network analysis using NetworkX. Written from the physicist's perspective, this book is focused on the network science paradigms of small-world networks, scale-free networks, community detection, and complex systems modeling. The [website](https://cambridgeuniversitypress.github.io/FirstCourseNetworkScience/) provides all code in the form of Jupyter Notebooks, data, and solutions to the exercises.*\n",
    "\n",
    "Platt, E. L. (2019). *Network Science with Python and NetworkX Quick Start Guide*. Packt. https://www.packtpub.com/product/network-science-with-python-and-networkx-quick-start-guide/9781789955316. *Systematic introduction to the practice of network preprocessing and analysis. Have a look at chapters 1â€“4 and 10â€“11 for the focus of session 2 here. All Jupyter Notebooks are publically available on the [website](https://github.com/PacktPublishing/Network-Science-with-Python-and-NetworkX-Quick-Start-Guide).*\n",
    "\n",
    "### Advanced textbooks\n",
    "\n",
    "Artime, O., Benigni, B., Bertagnolli, G., d'Andrea, V., Gallotti, R., Ghavasieh, A., Raimondo, S., & De Domenico, M. (2022). *Multilayer Network Science*. Cambridge University Press. https://doi.org/10.1017/9781009085809. *Interdisciplinary state-of-the-art account of the topic, quite advanced and mathematical.*\n",
    "\n",
    "Caldarelli, G. & Chessa, A. (2016). *Data Science and Complex Networks: Real Case Studies with Python*. Oxford University Press. https://doi.org/10.1093/acprof:oso/9780199639601.001.0001. *Interdisciplinary display of case studies from the physics/complexity perspective. Jupyter Notebooks that use NetworkX but add many custom functions are available on the [website](https://github.com/datascienceandcomplexnetworks).*\n",
    "\n",
    "### Other cited references\n",
    "\n",
    "Breiger, R. L. (1974). \"The duality of persons and groups\". *Social Forces* 53:181â€“190. https://doi.org/10.1093/sf/53.2.181. *A fundamental paper about the sociological use of bipartite network structures.*\n",
    "\n",
    "Freeman, L. (2004). *The Development of Social Network Analysis: A Study in the Sociology of Science*. Empirical Press. *The history of the field of Social Network Analysis as told by one of its founders.*\n",
    "\n",
    "Fuhse, J. (2021). *Social Networks of Meaning and Communication*. Oxford University Press. https://doi.org/10.1093/oso/9780190275433.001.0001. *Accessible account of sociological network theory that delves deep into the mutual constitution of social networks and communication networks.*\n",
    "\n",
    "Krempel, L. (2014). \"Network visualization\". In: Scott, J. & Carrington, P. J. (Eds.), *The SAGE Handbook of Social Network Analysis* (pp. 558-577). Sage. https://doi.org/10.4135/9781446294413. *A systematic overview how to visualize rich network information.*\n",
    "\n",
    "Lee, M. & Martin, J. L. (2018). \"Doorway to the dharma of duality\". *Poetics* 68:18â€“30. https://doi.org/10.1016/j.poetic.2018.01.001. *A methodological account of socio-cultural analysis using publication data.*\n",
    "\n",
    "Lietz, H., Schmitz, A., & Schaible, J. (2021). \"Social Network Analysis with Digital Behavioral Data\". *easy_social_sciences* 66:41â€“48. https://doi.org/10.15464/easy.2021.005. *A short and easily accessible introduction to the topic.*\n",
    "\n",
    "Peixoto, T. P. (2015). \"Inferring the mesoscale structure of layered, edge-valued, and time-varying networks\". *Physical Review E* 92:042807. https://doi.org/10.1103/PhysRevE.92.042807. *Advanced stochastic analysis of various multilayer networks written in physics style.*\n",
    "\n",
    "Sekara, V., Stopczynski, A., & Lehmann, S. (2016). \"Fundamental structures of dynamic social networks\". *Proceedings of the National Academy of Sciences* 113:9977â€“9982. https://doi.org/10.1073/pnas.1602803113. *Insightful multilayer analysis of the Copenhagen Networks Study interaction dataset.*\n",
    "\n",
    "Sapiezynski, P., Stopczynski, A., Lassen, D. D., & Lehmann, S. (2019). \"Interaction data from the Copenhagen Networks Study\". *Scientific Data* 6:315 (2019). https://doi.org/10.1038/s41597-019-0325-x. *Description of one of the richest publicly available multilayer network datasets.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758d9eec",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "<b>Document information</b>\n",
    "\n",
    "Contact and main author: Haiko Lietz & Haiko Lietz & N. Gizem Bacaksizlar Turbic\n",
    "\n",
    "Version date: 27 May 2023\n",
    "\n",
    "License: ...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662b5253",
   "metadata": {},
   "source": [
    "#### Notes to be removed before publication\n",
    "\n",
    "- ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
