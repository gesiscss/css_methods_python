{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../images/header.png' style='height: 50px; float: left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Computational Social Science methods with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session C3: Natural Language Processing\n",
    "\n",
    "The field of study that focuses on the interactions between human language and computers is called natural language processing (NLP).\n",
    "\n",
    "NLP is a field of artificial intelligence in which computers analyze,¬†understand, and derive meaning from human language in a smart and useful way. NLP systems are used exploiting the signals in our language used to predict all of the aforementioned features: people‚Äôs age (Nguyen et al., 2011; Rosenthal & McKeown, 2011), gender (Alowibdi et al., 2013; Ciot et al., 2013; Liu & Ruths, 2013), personality (Park et al., 2015), job title (Preo≈£iuc-Pietro et al., 2015a), income (Preo≈£iuc-Pietro et al., 2015b), and much more (Volkova et al., 2014, 2015).\n",
    "\n",
    "In NLP, word embeddings have been at the forefront of this progress, which has expanded to include flexible model architectures (Hovy, 2021). The most publicly visible example of this shift is probably the translation quality of services like Google Translate (Wu et al., 2016).\n",
    "\n",
    "A collection of fundamental tasks appear frequently across various NLP projects (Vajjala et al., 2020). Let‚Äôs briefly introduce them (Figure 1):\n",
    "\n",
    "<img src='images/nlp_tasks.png' style='height: 400px; float: right'>\n",
    "\n",
    "*Language modeling* is the task of predicting what the next word in a sentence will be based on the history of previous words. The goal of this task is to learn the probability of a sequence of words appearing in a given language. Language modeling is useful for building solutions for a wide variety of problems, such as **speech recognition, optical character recognition, handwriting recognition, machine translation, and spelling correction**.\n",
    "\n",
    "*Text classification* is the task of bucketing the text into a known set of categories based on its content. Text classification is by far the most popular task in NLP and is used in a variety of tools, from **email spam identification** to **sentiment analysis**.\n",
    "\n",
    "*Information extraction* is the task of extracting relevant information from text, such as **calendar events from emails** or the **names of people mentioned** in a social media post.\n",
    "\n",
    "*Information retrieval* is the task of finding documents relevant to a user query from a large collection. Applications like **Google Search** are well-known use cases of information retrieval.\n",
    "\n",
    "*Conversational agent* is the task of building dialogue systems that can converse in human languages. **Alexa** and **Siri** are some common applications of this task.\n",
    "\n",
    "*Text summarization* aims to create short summaries of longer documents while retaining the **core content** and preserving the **overall meaning** of the text.\n",
    "\n",
    "*Question answering* is the task of building a system that can automatically answer questions posed in natural language.\n",
    "\n",
    "*Machine translation* is the task of converting a piece of text from one language to another. Tools like **Google Translate** are common applications of this task.\n",
    "\n",
    "*Topic modeling* is the task of uncovering the topical structure of a large collection of documents. Topic modeling is a common text-mining tool and is used in a wide range of domains, from **literature** to **bioinformatics**.\n",
    "\n",
    "\n",
    "\n",
    "Understanding human language is considered as a difficult task due to its complexity. For example, there is an infinite number of different ways to arrange words in a sentence. \n",
    "\n",
    "Also, words can have several meanings and contextual information is necessary to correctly interpret sentences as every language is unique and ambiguous. The ambiguity can be in lexical and syntactic forms.\n",
    "\n",
    "- In lexical ambiguity, a single word has two or more possible meanings. For example, \"I saw bats\".\n",
    "- In syntactic ambiguity, a single sentence or a sequence of words have multiple possible meanings. For example, \"The chicken is ready to eat\".\n",
    "\n",
    "This session will help you understand the basic and advanced NLP concepts and show you how to implement using the most advanced and popular NLP libraries, such as  <a href=\"https://spacy.io/\">spaCy</a> and <a href=\"https://radimrehurek.com/gensim/\">Gensim</a>.\n",
    "\n",
    "<!-- ‚Äì <a href=\"https://www.nltk.org/\">NLTK</a>, <a href=\"https://spacy.io/\">spaCy</a>, <a href=\"https://radimrehurek.com/gensim/\">Gensim</a>, and <a href=\"https://huggingface.co/\">Hugging Face</a>. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "<b>In this session</b>, \n",
    "\n",
    "you will learn about basics for the Natural Language Processing. In subsession **C3.1**, we will extract useful information / facts (communication symbols) from tweets. In **C3.2**, we will show how to implement a text preprocessing pipeline using XXX data at the end of which stands the document-term matrix that is ready for analysis (such as topic modeling).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this session, you should be able to\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C3.1. Extracting entities from tweet texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C3.1.1. Extracting patterns using Regular Expressions\n",
    "\n",
    "<a href=\"https://docs.python.org/3/howto/regex.html\">Regular expressions</a> (also called regex, regexes, regex pattern, regexp, or REs) are a sequence of characters that define a search pattern. They are used in programming and text processing to match and manipulate strings of text based on a specific pattern.\n",
    "A regular expression is a pattern used to match one or more text strings. It is usually composed of a combination of characters, symbols, and metacharacters. Metacharacters are special characters that have a specific meaning in regular expressions, for example, the period (.) that matches any single character, or the asterisk (*) that matches zero or more occurrences of the preceding character.\n",
    "Regular expressions can be used to perform a variety of operations on text data, such as searching for specific patterns, replacing text with other text, or extracting specific information from a text string. \n",
    "Some common examples of regular expressions are matching an email addresses, phone numbers, dates, and URLs.\n",
    "\n",
    "Regular expressions can be complex and difficult to read, but they are a powerful tool for manipulating and processing text data. Luckily, there are many resources that can help us write the correct regular expression for our task. Also, Python has built-in mobule (`re`) to use regular expressions.\n",
    "\n",
    "<img src='images/Regular_Expressions_Cheat_Sheet.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following examples, we will use the top 500 retweeted tweets from the TweetsCOV19 dataset, which was introduced in [Session 2: Data handling and visualization](https://github.com/gesiscss/css_methods_python/blob/main/a_introduction/2_data_handling_and_visualization.ipynb). To read and practice with this data, we need to import neccessary libraries below. If you have some difficulties with importing/installing, please check out the [Session 1: Computing environment](https://github.com/gesiscss/css_methods_python/blob/main/a_introduction/1_computing_environment.ipynb) for further installation information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the data and visualize the first rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1265465820995411973</td>\n",
       "      <td>This was me, and I want to make one thing clea...</td>\n",
       "      <td>257467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1266553959973445639</td>\n",
       "      <td>Mike Pence caught on hot mic delivering empty ...</td>\n",
       "      <td>135818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1258750892448387074</td>\n",
       "      <td>THE PANDEMIC IS STILL HAPPENING. THE PANDEMIC ...</td>\n",
       "      <td>88667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1263579286201446400</td>\n",
       "      <td>This just happened on live tv. Wow, what a dou...</td>\n",
       "      <td>82495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1266546753182056453</td>\n",
       "      <td>Mask on</td>\n",
       "      <td>66604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1265465820995411973  This was me, and I want to make one thing clea...   \n",
       "1  1266553959973445639  Mike Pence caught on hot mic delivering empty ...   \n",
       "2  1258750892448387074  THE PANDEMIC IS STILL HAPPENING. THE PANDEMIC ...   \n",
       "3  1263579286201446400  This just happened on live tv. Wow, what a dou...   \n",
       "4  1266546753182056453                                            Mask on   \n",
       "\n",
       "   retweets  \n",
       "0    257467  \n",
       "1    135818  \n",
       "2     88667  \n",
       "3     82495  \n",
       "4     66604  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('../data/TweetsCOV19/top_500_retweeted_tweets.csv', encoding = \"utf-8\")\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will extract all URLs from the text of the twee. A possible regular expression to match an URL is:\n",
    "\n",
    "`http[s]*\\S+`\n",
    "\n",
    "This regular expression will match all strings that starts with `http`, or eventually with `https`, followed by non-empty spaces. \n",
    "\n",
    "We will use the `findall` function from the Python module `re` to match all URLs in text of the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://t.co/349TZijtD8']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we create a new column where we store all the URLs mentioned in the tweet extracted using regex\n",
    "tweets_df['urls'] = tweets_df['text'].apply(lambda x: re.findall(\"http[s]*\\S+\", x))\n",
    "tweets_df['urls'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract **mentions and hashtags** applying opportune regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@realDonaldTrump']\n",
      "['#COVID']\n"
     ]
    }
   ],
   "source": [
    "tweets_df['mentions'] = tweets_df['text'].apply(lambda x: re.findall(\"@[a-zA-Z0-9_]{1,50}\", x))\n",
    "print(tweets_df['mentions'].values[-1])\n",
    "\n",
    "tweets_df['hashtags'] = tweets_df['text'].apply(lambda x: re.findall(\"#[a-zA-Z0-9_]{1,50}\", x))\n",
    "print(tweets_df['hashtags'].values[-5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **emoji** extraction, in addition to regex, we will use the library called emoji (if not installed before, please install it before running the following cell). This library helps us transform emojis into the related codes (i.e., texts). Once the emojis are converted to text, we apply the same logic applied so far with regex to find them. \n",
    "\n",
    "The full list of emojis and related codes is available here: https://unicode.org/emoji/charts/full-emoji-list.html\n",
    "\n",
    "Let's look at and example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':face_with_tears_of_joy:'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji.demojize(\"üòÇ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this approach to the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>urls</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emoji</th>\n",
       "      <th>emoji_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1264986843948277760</td>\n",
       "      <td>People who say ‚Äòwell, he‚Äôs doing the best he c...</td>\n",
       "      <td>9033</td>\n",
       "      <td>[https://t.co/5POEhfB6vi]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#COVID]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1260425005483073538</td>\n",
       "      <td>This young woman was killed in her home for no...</td>\n",
       "      <td>9021</td>\n",
       "      <td>[https://t.co/JzPgOzm4Rm]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#BreonnaTaylor]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1259587972728533000</td>\n",
       "      <td>I be like ‚Äúoh shit my mask‚Äù like I‚Äôm Batman or...</td>\n",
       "      <td>8994</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[üòÇüòÇ]</td>\n",
       "      <td>[:face_with_tears_of_joy::face_with_tears_of_j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1266251584461090816</td>\n",
       "      <td>Really disappointed by @SAfridiOfficial‚Äòs comm...</td>\n",
       "      <td>8984</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@SAfridiOfficial, @narendramodi]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[üáÆüá≥]</td>\n",
       "      <td>[:India:]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1266728243236950018</td>\n",
       "      <td>Let's be clear about what's happening:\\n\\n‚Üí Am...</td>\n",
       "      <td>8974</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@realDonaldTrump]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                               text  \\\n",
       "495  1264986843948277760  People who say ‚Äòwell, he‚Äôs doing the best he c...   \n",
       "496  1260425005483073538  This young woman was killed in her home for no...   \n",
       "497  1259587972728533000  I be like ‚Äúoh shit my mask‚Äù like I‚Äôm Batman or...   \n",
       "498  1266251584461090816  Really disappointed by @SAfridiOfficial‚Äòs comm...   \n",
       "499  1266728243236950018  Let's be clear about what's happening:\\n\\n‚Üí Am...   \n",
       "\n",
       "     retweets                       urls                           mentions  \\\n",
       "495      9033  [https://t.co/5POEhfB6vi]                                 []   \n",
       "496      9021  [https://t.co/JzPgOzm4Rm]                                 []   \n",
       "497      8994                         []                                 []   \n",
       "498      8984                         []  [@SAfridiOfficial, @narendramodi]   \n",
       "499      8974                         []                 [@realDonaldTrump]   \n",
       "\n",
       "             hashtags emoji                                         emoji_text  \n",
       "495          [#COVID]    []                                                 []  \n",
       "496  [#BreonnaTaylor]    []                                                 []  \n",
       "497                []  [üòÇüòÇ]  [:face_with_tears_of_joy::face_with_tears_of_j...  \n",
       "498                []  [üáÆüá≥]                                          [:India:]  \n",
       "499                []    []                                                 []  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_emojis(text, return_codes=False):\n",
    "    # first turn emojis into related text code\n",
    "    text_de = emoji.demojize(text)\n",
    "    # second find all emojis text code\n",
    "    emojis_list_de = re.findall(r'(:[\\a-z]+:)', text_de)\n",
    "    # reconvert text code to emojis\n",
    "    list_emoji = [emoji.emojize(x) for x in emojis_list_de]\n",
    "\n",
    "    if return_codes:\n",
    "        return emojis_list_de\n",
    "    else:\n",
    "        return list_emoji\n",
    "\n",
    "tweets_df['emoji'] = tweets_df['text'].apply(extract_emojis)\n",
    "tweets_df['emoji_text'] = tweets_df['text'].apply(extract_emojis, return_codes=True)\n",
    "\n",
    "tweets_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the final results from our extraction example and sort values according to mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>urls</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emoji</th>\n",
       "      <th>emoji_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>1258617080430997505</td>\n",
       "      <td>A Black New York State Senator (@zellnor4ny) a...</td>\n",
       "      <td>9151</td>\n",
       "      <td>[https://t.co/NoT8g4uAli]</td>\n",
       "      <td>[@zellnor4ny, @YourFavoriteASW]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>1266956300908363776</td>\n",
       "      <td>NEW: A volunteer on Kushner's coronavirus resp...</td>\n",
       "      <td>9327</td>\n",
       "      <td>[https://t.co/jvs2h4IfNQ]</td>\n",
       "      <td>[@yabutaleb7]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[: A volunteer on Kushner's coronavirus respon...</td>\n",
       "      <td>[: A volunteer on Kushner's coronavirus respon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1260559563972960256</td>\n",
       "      <td>Wow! The Front Page @washingtonpost Headline r...</td>\n",
       "      <td>11591</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@washingtonpost]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>1262940294305071104</td>\n",
       "      <td>it would appear that @vp was joking about carr...</td>\n",
       "      <td>11196</td>\n",
       "      <td>[https://t.co/hI9cO4lxcX]</td>\n",
       "      <td>[@vp]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1261718681882693632</td>\n",
       "      <td>Very happy to present this unseen image of @ta...</td>\n",
       "      <td>10245</td>\n",
       "      <td>[https://t.co/3dzvynlUq3]</td>\n",
       "      <td>[@tarak9999, @DabbooRatnani]</td>\n",
       "      <td>[#HappyBirthdayNTR, #StayHomeStaySafe]</td>\n",
       "      <td>[üòé\\n\\nüì∏ By @DabbooRatnani \\n\\n#HappyBirthdayNT...</td>\n",
       "      <td>[:smiling_face_with_sunglasses:\\n\\n:camera_wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1256717572373913605</td>\n",
       "      <td>Update: Got her permission with a fuck yeah. T...</td>\n",
       "      <td>19289</td>\n",
       "      <td>[https://t.co/MqV0QJ0D8h]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1265624335898869760</td>\n",
       "      <td>Y'all, the mask goes OVER your nose.</td>\n",
       "      <td>19351</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1258599146522464256</td>\n",
       "      <td>Because if its Baghdad its okay for this to ha...</td>\n",
       "      <td>19457</td>\n",
       "      <td>[https://t.co/UdFy61zoT5]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1266343312304324608</td>\n",
       "      <td>I gotta be honest the worst looting I've ever ...</td>\n",
       "      <td>19527</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1258018688231407618</td>\n",
       "      <td>‚ÄúOh shit my mask.‚Äù</td>\n",
       "      <td>15036</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                               text  \\\n",
       "489  1258617080430997505  A Black New York State Senator (@zellnor4ny) a...   \n",
       "464  1266956300908363776  NEW: A volunteer on Kushner's coronavirus resp...   \n",
       "347  1260559563972960256  Wow! The Front Page @washingtonpost Headline r...   \n",
       "360  1262940294305071104  it would appear that @vp was joking about carr...   \n",
       "412  1261718681882693632  Very happy to present this unseen image of @ta...   \n",
       "..                   ...                                                ...   \n",
       "167  1256717572373913605  Update: Got her permission with a fuck yeah. T...   \n",
       "165  1265624335898869760               Y'all, the mask goes OVER your nose.   \n",
       "164  1258599146522464256  Because if its Baghdad its okay for this to ha...   \n",
       "163  1266343312304324608  I gotta be honest the worst looting I've ever ...   \n",
       "250  1258018688231407618                                 ‚ÄúOh shit my mask.‚Äù   \n",
       "\n",
       "     retweets                       urls                         mentions  \\\n",
       "489      9151  [https://t.co/NoT8g4uAli]  [@zellnor4ny, @YourFavoriteASW]   \n",
       "464      9327  [https://t.co/jvs2h4IfNQ]                    [@yabutaleb7]   \n",
       "347     11591                         []                [@washingtonpost]   \n",
       "360     11196  [https://t.co/hI9cO4lxcX]                            [@vp]   \n",
       "412     10245  [https://t.co/3dzvynlUq3]     [@tarak9999, @DabbooRatnani]   \n",
       "..        ...                        ...                              ...   \n",
       "167     19289  [https://t.co/MqV0QJ0D8h]                               []   \n",
       "165     19351                         []                               []   \n",
       "164     19457  [https://t.co/UdFy61zoT5]                               []   \n",
       "163     19527                         []                               []   \n",
       "250     15036                         []                               []   \n",
       "\n",
       "                                   hashtags  \\\n",
       "489                                      []   \n",
       "464                                      []   \n",
       "347                                      []   \n",
       "360                                      []   \n",
       "412  [#HappyBirthdayNTR, #StayHomeStaySafe]   \n",
       "..                                      ...   \n",
       "167                                      []   \n",
       "165                                      []   \n",
       "164                                      []   \n",
       "163                                      []   \n",
       "250                                      []   \n",
       "\n",
       "                                                 emoji  \\\n",
       "489                                                 []   \n",
       "464  [: A volunteer on Kushner's coronavirus respon...   \n",
       "347                                                 []   \n",
       "360                                                 []   \n",
       "412  [üòé\\n\\nüì∏ By @DabbooRatnani \\n\\n#HappyBirthdayNT...   \n",
       "..                                                 ...   \n",
       "167                                                 []   \n",
       "165                                                 []   \n",
       "164                                                 []   \n",
       "163                                                 []   \n",
       "250                                                 []   \n",
       "\n",
       "                                            emoji_text  \n",
       "489                                                 []  \n",
       "464  [: A volunteer on Kushner's coronavirus respon...  \n",
       "347                                                 []  \n",
       "360                                                 []  \n",
       "412  [:smiling_face_with_sunglasses:\\n\\n:camera_wit...  \n",
       "..                                                 ...  \n",
       "167                                                 []  \n",
       "165                                                 []  \n",
       "164                                                 []  \n",
       "163                                                 []  \n",
       "250                                                 []  \n",
       "\n",
       "[500 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.sort_values(by='mentions', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final exercise, let's clean text from urls, hashtags, mentions, and emojis for further text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    # find all URLs in text using regex\n",
    "    urls = re.findall(\"http[s]*\\S+\", text)\n",
    "    # iterate through the URLs and remove them\n",
    "    for url in urls:\n",
    "        text = text.replace(url, \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    # find all hashtags in text using regex\n",
    "    hashtags = re.findall(\"@[a-zA-Z0-9_]{1,50}\", text)\n",
    "    # iterate through the hashtags and remove them\n",
    "    for hashtag in hashtags:\n",
    "        text = text.replace(hashtag, \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_mentions(text):\n",
    "    # find all mentions in text using regex\n",
    "    mentions = re.findall(\"#[a-zA-Z0-9_]{1,50}\", text)\n",
    "    # iterate through the mentions and remove them\n",
    "    for mention in mentions:\n",
    "        text = text.replace(mention, \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_emojis(text):\n",
    "    # find all emoji in text\n",
    "    emojis = extract_emojis(text, return_codes=False)\n",
    "    # iterate through the emojis and remove them\n",
    "    for emoji in emojis:\n",
    "        text = text.replace(emoji, \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # create a cleaning pipeline \n",
    "    text = remove_urls(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = remove_emojis(text)\n",
    "    return text\n",
    "\n",
    "tweets_df['cleaned_text'] = tweets_df['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tweet: Very happy to present this unseen image of @tarak9999 .. I hope you all like it üòé\n",
      "\n",
      "üì∏ By @DabbooRatnani \n",
      "\n",
      "#HappyBirthdayNTR üéâ\n",
      "\n",
      "#StayHomeStaySafe üôèüèº https://t.co/3dzvynlUq3\n",
      "\n",
      "\n",
      "Cleaned Tweet: Very happy to present this unseen image of  .. I hope you all like it  \n"
     ]
    }
   ],
   "source": [
    "print('Original Tweet:', tweets_df.text.values[412])\n",
    "print('\\n\\nCleaned Tweet:', tweets_df.cleaned_text.values[412])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C3.1.2. Extracting named entities\n",
    "\n",
    "A named entity is a real-life object which can be identified and denoted with a proper name. Named Entities can be a place, person, organization, time, object, or geographic entity. For example, named entities would be Joe Biden, New York city, and congress. Named entities are usually instances of entity instances. For example, Joe Biden is an instance of a politician/person, New York City is an instance of a place, and congress is as instance of an organization. \n",
    "\n",
    "**Named Entity Recognition** (NER) is the process of NLP for identifying and classifying named entities. The raw and structured text are used to find out named entities, which are classified into persons, organizations, places, money, time, etc. NER systems are developed with various linguistic approaches, as well as statistical and machine learning methods. \n",
    "\n",
    "NER model first identifies an entity and then categorizes the entity into the most suitable class. Some of the common types of Named Entities will be as follows and others can be found in the further example of a Wikipedia page text.\n",
    "\n",
    "1. Organisations : NASA, CERN\n",
    "\n",
    "2. Places: Istanbul, Germany\n",
    "\n",
    "3. Money: 1 Billion Dollars, 50 Euros\n",
    "\n",
    "4. Date: 24th January 2023, season 4\n",
    "\n",
    "5. Person: Richard Feynman, George Floyd\n",
    " \n",
    "<img src='images/NER.png' style='height: 500px; float: left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Insight</b></big>\n",
    "\n",
    "    \n",
    "For NLP tasks like NER, POS tagging, dependency parsing, word vectors and more, <a href=\"https://spacy.io/\">spaCy</a> has distinct features that provide clear advantage for processing text data and modeling. It is the most trending and advanced free open-source library for implementing NLP in Python nowadays. \n",
    "    \n",
    "An important thing about NER models is that their ability to understand Named Entities depending on the data they have been trained on. There are many applications of NER. NER can be used for content classification, the various Named Entities of a text can be collected, and based on that data, the content themes can be understood.\n",
    "    \n",
    "We can use spaCy very easily for NER tasks. However, we need to consider training our own data for research, commercial, and business specific needs, the spaCy model generally performs well for all types of text data. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's import necessary libraries and packages and start with a toy example from our tweets dataframe, which is the second line of the text column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# before loading trained pipelines for English we need to install this module via the following code.\n",
    "# Please uncomment the next line \n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\css_methods_python\\lib\\site-packages\\spacy\\util.py:887: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.5.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike Pence caught on hot mic delivering empty boxes of PPE for a PR stunt. \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Print the second tweet of our dataset\n",
    "raw_text = tweets_df.cleaned_text[1]\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we print the data on the Named Entities found in this raw text sample from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike Pence PERSON\n",
      "PPE ORG\n"
     ]
    }
   ],
   "source": [
    "# extract the entities using the spacy objects previously defined in the\n",
    "NER_text = NER(raw_text)\n",
    "\n",
    "# show all the entities extracted from the text\n",
    "for word in NER_text.ents:\n",
    "    print(word.text, word.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Insight</b></big>\n",
    "    \n",
    "Here, PPE is a context specific word to be labeled as organization. In the COVID-19 context like in our example, it stands for \"personal protective equipment\"; which is not an organization. On the other hand, as an abbreviation of the Philosophy, Politics, and Economics Society, PPE can be labeled as an organization.\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run NER on the full dataset and find out the output with Named Entities and who are the most cited people:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('George Floyd', 9),\n",
       " ('Donald Trump', 7),\n",
       " ('Flynn', 7),\n",
       " ('Fauci', 7),\n",
       " ('Biden', 6)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load the pre-trained model with NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define an array to store the people cited\n",
    "persons_cited = []\n",
    "\n",
    "# Loop over each text and analyze it with spaCy's NER\n",
    "for text in tweets_df.cleaned_text:\n",
    "    doc = NER(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            # If the entity is a person, add it to the array \n",
    "            persons_cited.append(ent.text)\n",
    "\n",
    "citations_count = Counter(persons_cited)\n",
    "citations_count.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C3.2. Text representation: Implementing a preprocessing pipeline\n",
    "\n",
    "### C3.2.1. Word descriptors\n",
    "\n",
    "To refer to the entire collection of documents/observations, we use the word corpus (plural corpora). The raw text data often referred to as *text corpus* has punctuations, suffices, and stop words that do not give us important information. To have more useful information for NLP tasks, Text Preprocessing involves preparing the text corpus. Let's start with basic terminology of NLP.\n",
    "\n",
    "#### Tokens and splitting \n",
    "\n",
    "The set of all the unique terms in our data is called the vocabulary. Each element in this set is called a type. Each occurrence of a type in the data is called a token. \n",
    "\n",
    "Let's practice: Our sentence is\n",
    "\n",
    ">‚ÄúToday is a great day with learning NLP, such a power tool!‚Äù\n",
    "\n",
    "Thi sentece has 14 tokens but only 13 types (namely, 'Today', 'is', 'a', 'great', 'day', 'with', 'learning', 'NLP', ',', 'such', 'a', 'powerful', 'tool', '!'). Note that types can also include punctuation marks and multiword expressions.\n",
    "\n",
    "In other words, the words of a text document/file separated by spaces and punctuation are called as tokens.\n",
    "\n",
    "The process of extracting tokens from a text file/document is referred as tokenization. Let's see an example below of a tokenization process using spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Mike Pence caught on hot mic delivering empty boxes of PPE for a PR stunt. \n",
      "\n",
      "Tokens in the text:\n",
      "\t Mike\n",
      "\t Pence\n",
      "\t caught\n",
      "\t on\n",
      "\t hot\n",
      "\t mic\n",
      "\t delivering\n",
      "\t empty\n",
      "\t boxes\n",
      "\t of\n",
      "\t PPE\n",
      "\t for\n",
      "\t a\n",
      "\t PR\n",
      "\t stunt\n",
      "\t .\n",
      "\n",
      "Total tokens: 16\n"
     ]
    }
   ],
   "source": [
    "# Load the small English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# As a text example we will use a tweet from the previous dataset\n",
    "text = tweets_df.cleaned_text[1]\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print the original and tokenized text\n",
    "print('Original text:', text)\n",
    "print('\\nTokens in the text:',)\n",
    "\n",
    "for token in doc:\n",
    "    print('\\t', token.text)\n",
    "\n",
    "print('\\nTotal tokens:', len(doc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also push furhter our analysis and extract the vocabulary from the corpus of tweets from the previous dataset. Since the vocabulary of a text corpus is the collection of unique tokens present in that corpus, we will just need to tokenize each single tweet and keep unique occurence of each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Process all tweets with spaCy and extract all tokens\n",
    "tokens = []\n",
    "for text in tweets_df.cleaned_text:\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        tokens.append(token.text)\n",
    "\n",
    "# Count the occurrences of each token and create a vocabulary of unique tokens\n",
    "vocabulary = Counter(tokens)\n",
    "\n",
    "# Print the extracted vocabulary\n",
    "print(\"Size of extracted vocabulary: {0}\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "When we look up a word in a dictionary, we usually just look for the base form. This dictionary base form is called the **lemma**.\n",
    "For instance, we might see forms like ‚Äúgo‚Äù, ‚Äúgoes‚Äù, ‚Äúwent‚Äù, ‚Äúgone‚Äù, or ‚Äúgoing‚Äù and we look up dictionary in a lemmatized form, such as \"go\" (Hovy, 2020). These words have clearly different meaning, in some contexts it is not fundamental to distinguish them. On the contrary, it is much more convenient to trace them back to their lemma. Indeed, this may simplify some analysis and allow easier extraction of relevant information from the text. Let's see an example of lemmatization applied to the corpus of tweets using spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the small English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# As a text example we will use a tweet from the previous dataset\n",
    "text = tweets_df.cleaned_text[1]\n",
    "\n",
    "# Process the text with spaCy and perform lemmatization\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print words and extractes lemmas\n",
    "for token in doc:\n",
    "    print(\"{0} -> {1}\".format(token.text, token.lemma_))\n",
    "\n",
    "# Finally we can recover the text of the tweet after lemmatization\n",
    "print('\\n\\nOriginal text:', text)\n",
    "lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "print('Lemmatized text:', lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "Another strategy to reduce different forms of a word to a common base or root form is stemming. Stemming involves removing the suffixes of words to create a simplified form of the word. For example, the stem of the words \"running,\" \"runner,\" and \"run\" is \"run.\" This can be achieved using several algorithms like the one developed by Porter (1980). This algorithm defines a number of suffixes and the order in which they should be removed or replaced. These actions are then applied iteratively untill a word is reduced to its stem.\n",
    "\n",
    "Note how, although similar, stemming and lemmatization are different and give different results. Generally speaking, lemmatization tends to produce more accurate and meaningful results with respect to stemming. Nonethelss, stemming is often faster and simpler to implement, which makes it useful for tasks that require real-time processing or have limited computational resources.\n",
    "\n",
    "An implementation of the Porter stemmer is available in the Python library NLTK. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download popular NLTK data\n",
    "!python -m nltk.downloader popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# As a text example we will use a tweet from the previous dataset\n",
    "text = tweets_df.cleaned_text[1]\n",
    "\n",
    "# This performs tokenization on the text (NLTK equivalalent of what we did with spaCy)\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Create a PorterStemmer object\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to each word in the text\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "# Let's see results \n",
    "for token, stem in zip(tokens, stemmed_tokens):\n",
    "    print(\"{0} -> {1}\".format(token, stem))\n",
    "\n",
    "# Finally we can recover the text of the tweet after lemmatization\n",
    "print('\\n\\nOriginal text:', text)\n",
    "stemmed_text = \" \".join(stemmed_tokens)\n",
    "print('Stemmed text:', stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams\n",
    "\n",
    "In natural language processing (NLP), **N-grams** are contiguous sequences of n elements from a given text sample, where an element can be a word, a character, or part of speech. In most cases, n-grams are created from a text by dragging a window of size n over the text and extracting the sequences of n elements that fall within that window.\n",
    "\n",
    "N-grams are used in a variety of NLP tasks such as language modeling, machine translation, and text classification. By extracting n-grams from a text, it is possible to capture the local context of a word or word sequence, which can help improve the accuracy of many NLP tasks.\n",
    "\n",
    "For example, a bigram (n=2) is \"natural language\", a trigram (n=3) is \"natural language processing\", and a 4-gram (n=4) is \"natural language processing task\". By examining the frequency of different n-grams in a text or corpus, it is possible to gain insight into the distribution of words and their relationships.\n",
    "\n",
    "N-grams can also be used to generate new texts through techniques such as n-gram language modeling. In this approach, the probabilities of different N-grams in a text are used to generate a new text that is similar in style and content to the original text.\n",
    "\n",
    "However, it should be noted that n-grams can be constrained by the sparsity problem, especially for larger values of n. That is, as the value of n increases, the number of unique n-grams in a text can increase rapidly, making it difficult to capture meaningful patterns or relationships. Therefore, choosing an appropriate value of n is an important consideration in many NLP tasks.\n",
    "\n",
    "Let's see an example of  N-grams extraction applied to the corpus of tweets using spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# As a text example we will use a tweet from the previous dataset\n",
    "text = tweets_df.cleaned_text[1]\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Define the function to extract n-grams\n",
    "def extract_ngrams(doc, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(doc) - n + 1):\n",
    "        ngram = \" \".join([doc[j].text for j in range(i, i + n)])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "# Extract unigrams, bigrams, and trigrams from the text\n",
    "unigrams = extract_ngrams(doc, 1)\n",
    "bigrams = extract_ngrams(doc, 2)\n",
    "trigrams = extract_ngrams(doc, 3)\n",
    "\n",
    "# Print the extracted n-grams\n",
    "print(\"Unigrams:\", unigrams)\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Trigrams:\", trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use Gensim, another popular library for NLP, to automatically extract the most common n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# gensim expect as input tokenized texts\n",
    "texts = []\n",
    "for text in tweets_df.cleaned_text.values:\n",
    "    texts.append(word_tokenize(text))\n",
    "\n",
    "# extract bigrams\n",
    "bigrams = gensim.models.Phrases(texts, min_count=5, threshold=100)\n",
    "texts_bigrams = [bigrams[text] for text in texts]\n",
    "\n",
    "# visualize the extracted bigrams\n",
    "extracted_bigrams = []\n",
    "for text in texts_bigrams:\n",
    "    for el in text:\n",
    "        if \"_\" in el:\n",
    "            extracted_bigrams.append(el)\n",
    "\n",
    "extracted_bigrams = set(extracted_bigrams)\n",
    "print(extracted_bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C3.2.2. Stopwords\n",
    "\n",
    "In natural language processing (NLP), stop words refer to words that are frequently used in a language but usually do not have much meaning or semantic value when used in context. Examples of stop words in English are \"the\", \"a\", \"an\", \"and\", \"in\", \"on\", \"is\", \"are\", \"for\", \"with\", and so on.\n",
    "\n",
    "Stop words are usually removed from text during preprocessing in NLP tasks such as text classification, sentiment analysis, and information retrieval. The reason is that they do not contribute much to the overall meaning or topic of a text and can potentially degrade algorithm performance by adding noise to the data. Removing stop words can also help reduce the size of vocabulary and improve the efficiency of text processing algorithms.\n",
    "\n",
    "However, there are certain cases where the inclusion of stop words in the analysis may be useful or even necessary. For example, stopwords can be useful in tasks such as authorship attribution, to identify common themes, or writing styles. In such cases, it is important to carefully consider the use of stop words and their potential impact on the analysis\n",
    "\n",
    "We will now see a simple example on how to remove Stop Words from a text using spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Load the small English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# As a text example we will use a tweet from the previous dataset\n",
    "text = tweets_df.cleaned_text[1]\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Define the list of stop words\n",
    "stop_words = list(STOP_WORDS)\n",
    "\n",
    "# Remove stop words from the text\n",
    "filtered_text = [token.text for token in doc if not token.text not in stop_words]\n",
    "stop_words_removed = [token.text for token in doc if token.text in stop_words]\n",
    "\n",
    "# Print the original and filtered text, and the stop words removed\n",
    "print(\"Original tokens: \", [token.text for token in doc])\n",
    "print(\"Filtered tokens:\", filtered_text)\n",
    "print(\"\\nStop words removed: \", stop_words_removed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the task, one can also add custom stop words. This can be easily done by appending additional words to the stop words list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stop_words))\n",
    "stop_words.append(\"place\")\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C3.2.3. Parts of Speech\n",
    "\n",
    "**Part of speech tagging** (POS) is the process of assigning a part of speech to each word in a sentence, such as noun, verb, adjective, or adverb. POS Tagging is an important step in many NLP applications, such as named entity recognition, sentiment analysis, and machine translation.\n",
    "\n",
    "The goal of POS tagging is to identify the grammatical structure of a sentence by labelling each word with its corresponding part of speech. This information can then be used to extract meaning and context from the text. For example, knowing whether a word is a noun or a verb can help determine the subject and predicate of a sentence.\n",
    "\n",
    "POS tagging is typically performed using machine learning algorithms, such as hidden Markov models, conditional random fields, or neural networks. These algorithms are trained on annotated text corpora in which each word is labelled with the corresponding word type. After training, the algorithm can then predict the word type for a new unseen text.\n",
    "\n",
    "POS tagging is not always an easy task, as some words may have multiple possible word types depending on the context. For example, \"run\" can be a verb (\"I run every morning\") or a noun (\"I went for a run\"). In these cases, the algorithm must use contextual clues to determine the most likely part of speech for the word.\n",
    "\n",
    "Overall, POS tagging is an important technique in NLP that helps extract meaning and context from texts by identifying the grammatical structure of sentences.\n",
    "\n",
    "English has 9 main categories:\n",
    "\n",
    "- verb ‚Äî Expresses an action or a state of being. E.g. jump, is, write, become\n",
    "- noun ‚Äî identifies a person, a place or a thing or names of particular of one of these (pronoun). E.g. man, house, happiness\n",
    "- pronoun ‚Äî can replace a noun or noun phrase. E.g. she, we, they, it\n",
    "- determiner ‚Äî Is placed in front of a noun to express a quantity or clarify what the noun refers to ‚Äî briefly a noun introducer. E.g. my, that, the, many\n",
    "- adjective ‚Äî modifies a noun or a pronoun. E.g. pretty, old, blue, smart\n",
    "- adverb ‚Äî modifies a verb, an adjective, or another adverb. E.g. gently, extremely, carefully, well\n",
    "- preposition ‚Äî Connect a noun/pronoun to other parts of the sentence. E.g. by, with, about, until\n",
    "- conjunction ‚Äî glue words, clauses, and sentences together. E.g. and, but, or, while, because\n",
    "- interjection ‚Äî Expresses emotion in a sudden or exclamatory way. E.g. oh!, wow!, oops!\n",
    "\n",
    "\n",
    "<img src='images/POS.png' style='height: 200px; float: center'>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We will now see a simple example on how to perform POS on a text using spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# load the English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# As a text example we will use a tweet from the previous dataset\n",
    "text = tweets_df.cleaned_text[1]\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# iterate over each token in the doc and print its text and POS tag\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the meaning of a POS tag is not clear to us, we ask spaCy to explain it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"PROPN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how spaCy POS tagging works on more tricky examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here the word run is used as verb\n",
    "text1 = \"I run every morning\"\n",
    "\n",
    "# here the word run is used as a noun\n",
    "text2 = \"I went for a run\"\n",
    "\n",
    "# POS tagging of sentence 1\n",
    "for token in nlp(text1):\n",
    "    print(token.text, token.pos_)\n",
    "\n",
    "print(\"\\n\")\n",
    "# POS tagging of sentence 2\n",
    "for token in nlp(text2):\n",
    "    print(token.text, token.pos_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that spaCy correctly tag the word \"run\" differently in these two examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C3.2.4. Bag-of-words and TF-IDF preprocessing\n",
    "\n",
    "So far we have introduced what are tokenization, stemming, lemmatization, stop words, n-grams, and part of speech tagging. As we have seen, these are all preprocessing techniques that aims at cleaning, removing unnecessary information, and extracting structure from the text. As a last step of this section, we will introduce two techniques, the Bag-of-Words (BoW) approach and its natural extension, a technique called TF-IDF (Term Frequency-Inverse Document Frequency). These approaches combines the techniques described so far, and finally prepares the texts for the actual analysis, such as topic modeling, text classification, and sentiment analysis.\n",
    "\n",
    "\n",
    "In natural language processing (NLP), a **\"bag of words\"** is a representation of a text document that describes the occurrence of words in it. It is a simple and commonly used approach to convert text data into a numerical format that can be used for analysis and machine learning.\n",
    "The bag-of-words model ignores the order and structure of the text and only considers the frequency of occurrence of each word in the document. The resulting representation is a \"bag\" of words in which each word is represented as a separate feature, and the value of each feature is the count of the corresponding word in the document.\n",
    "\n",
    "The bag-of-words representation of a corpus is usually stored in a matrix called the **document-term matrix**, where each row represents a document and each column represents a term (i.e., a word). The value in each cell is the number of occurrences of the corresponding term in the corresponding document. The document-term matrix is generally a sparse matrix, meaning that it contains a large number of zero elements and few non-zero ones. Indeed, most documents only contain a small fraction of the possible words in a language, and most words occur in only a subset of the documents. This means that the vast majority of the entries in the document-term matrix are zero.\n",
    "\n",
    "We will now see how to derive the document-term matric using spaCy and Gensim. For this task, we will use another dataset that contains news articles. Let's import the data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "news = pd.read_csv(\"../data/news/news_subset.csv\")\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before deriving the document-term matrix, we preprocess the articles using the technique described before: tokenization, lemmatization (or stemming), stop words and punctuation removal: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import spacy\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # remove punctuation and special characters\n",
    "    pattern = r\"[^\\w\\s]\"\n",
    "    text_clean = re.sub(pattern, \"\", text)\n",
    "\n",
    "    # remove numbers\n",
    "    pattern = r\"\\d+\"\n",
    "    text_clean = re.sub(pattern, \"\", text_clean)\n",
    "\n",
    "    # remove all non-ASCII characters\n",
    "    pattern = r\"[^\\x00-\\x7F]+\"\n",
    "    text_clean = re.sub(pattern, \"\", text_clean)\n",
    "\n",
    "    # remove new line characters\n",
    "    text_clean.replace(\"\\n\", \"\")\n",
    "\n",
    "    # remove empty spaces left by regex\n",
    "    text_clean = ' '.join(text_clean.split())\n",
    "    \n",
    "    return text_clean\n",
    "\n",
    "\n",
    "def tokenization(texts):\n",
    "    #return [text.split(\" \") for text in texts]\n",
    "    return [word_tokenize(text) for text in texts]\n",
    "\n",
    "\n",
    "def remove_stop_words(texts, stop_words=[]):\n",
    "    if stop_words == []:\n",
    "        stop_words = list(STOP_WORDS)\n",
    "    return [[word for word in doc if word.lower() not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "def add_bigrams(texts):\n",
    "    bigrams = gensim.models.Phrases(texts, min_count=5, threshold=100)\n",
    "    return [bigrams[text] for text in texts]\n",
    "\n",
    "\n",
    "def stemming(texts):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [[stemmer.stem(word) for word in doc] for doc in texts]\n",
    "\n",
    "\n",
    "def lemmatization(texts):\n",
    "    texts_lemma = []\n",
    "    for text in texts:\n",
    "        doc = nlp(\" \".join(text)) \n",
    "        texts_lemma.append([token.lemma_ for token in doc])\n",
    "    return texts_lemma\n",
    "\n",
    "\n",
    "def pipeline(corpus):\n",
    "    print(\"Cleaning text...\")\n",
    "    corpus = [clean_text(text) for text in corpus]\n",
    "\n",
    "    print(\"Tokenization...\")\n",
    "    corpus = tokenization(corpus)\n",
    "\n",
    "    print(\"Lowercasing...\")\n",
    "    corpus = [[el.lower() for el in text] for text in corpus]\n",
    "\n",
    "    print(\"Stop Words removal...\")\n",
    "    corpus = remove_stop_words(corpus)\n",
    "\n",
    "    print(\"Extract bigrams...\")\n",
    "    corpus = add_bigrams(corpus)\n",
    "\n",
    "    print(\"Stemming...\")\n",
    "    corpus = stemming(corpus)\n",
    "\n",
    "    print(\"Stop Words removal after stemming...\")\n",
    "    corpus = remove_stop_words(corpus)\n",
    "\n",
    "    print(\"Removing tokens that are too short...\")\n",
    "    corpus = [[c for c in text if len(c) > 2] for text in corpus]\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the preprocessing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for index, row in news.iterrows():\n",
    "    corpus.append(row.headline + \". \" + row.short_description)\n",
    "corpus = np.array(corpus)\n",
    "\n",
    "# run the preprocessing pipeline\n",
    "corpus = pipeline(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to extract the document-term matrix from the preprocessed corpus. We will use and implementation of the BoW approach in Gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# we create a dictionary\n",
    "dictionary = Dictionary(corpus)\n",
    "\n",
    "# we filter very common and very rare words\n",
    "#dictionary.filter_extremes(no_below=10, no_above=0.3)\n",
    "\n",
    "# covert the corpus to bag of words format \n",
    "document_term_matrix = [dictionary.doc2bow(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous cell we have created a dictionary (i.e., a collection of all the words appearing in the corpus) and the document_term matrix. Let's see these outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of words in the dictionary: {0}\".format(len(dictionary)))\n",
    "print(\"Dictionary first 5 elements (id, token):\", list(dictionary.items())[:5])\n",
    "\n",
    "print(\"\\nFirst document in bag-of-words format (raw):\", document_term_matrix[0])\n",
    "print(\"First document in bag-of-words format (word, frequency):\", [[dictionary[id], freq] for id, freq in document_term_matrix[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also save the output for future analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory = 'results'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(\"./results/dict_gensim.pkl\", \"wb\") as file:\n",
    "    pkl.dump(dictionary, file)\n",
    "\n",
    "with open(\"./results/corpus.pkl\", \"wb\") as file:\n",
    "    pkl.dump(corpus, file)\n",
    "\n",
    "with open(\"./results/document_term_matrix.pkl\", \"wb\") as file:\n",
    "    pkl.dump(document_term_matrix, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite its simplicity, the BoW approach is a powerful technique to turn collections of text into a numerical format that can then be inputed to a variety of models for several applications, including topic modeling. Nonetheless, it has some limitations, such as:\n",
    "\n",
    "- Importance of rare words: The bag of words model assigns equal weight to all words in a document, regardless of their importance or rarity. \n",
    "- Discrimination of common words: The bag of words model assigns high weights to common words, which are not very informative and may not be discriminative for distinguishing between different documents. \n",
    "\n",
    "These limitations, can be corrected using the **Term Frequency-Inverse Document Frequency** (TF-IDF) matrix instead of the simple document-matrix. The idea behind TF-IDF is to assign a weight to each word in a document based on how frequently it occurs in the document and how important it is in the overall corpus. This weight is calculated by multiplying two factors:\n",
    "\n",
    "- **Term Frequency (TF)**: this is a measure of how often a word occurs in a document. It is calculated by dividing the number of occurrences of a word in a document by the total number of words in the document. The TF value for a word is high if it occurs very often in a document, and low if it occurs only a few times. In mathematical terms, the TF value for word $t$ in document $d$ is:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "tf(t, d) = \\frac{f_{t,d}}{\\sum_{t' \\in d}f_{t', d}}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "- **Inverse Document Frequency (IDF)**: This is a measure of how important a word is in a corpus. It is calculated by dividing the total number of documents in the corpus by the number of documents containing the word. The IDF value for a word is high if it occurs in a few documents and low if it occurs in many documents. In general, it is used the logarithm of the IDF factor. Indeed, if a word appears in only a very small number of documents, the resulting IDF value can be very large. This can lead to a situation where the TF-IDF weight of a word is dominated by its IDF value, even if its term frequency (TF) is relatively low. Taking the logarithmof the IDF value has the effect of compressing the range of possible IDF values and reducing the impact of very high IDF values. In mathematical terms, the TF value for word $t$ in a corpus $D$ of $N$ document is:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "idf(t, D) = log \\frac{N}{|\\{d \\in D : t \\in d\\}|}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "The TF-IDF weighting $w_{t, d, D}$ for a word $t$ is then calculated by multiplying the TF value and the IDF value for that word:\n",
    "$\n",
    "\\begin{align}\n",
    "w_{t, d, D} = tf(t, d) \\times idf(t, D)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "The higher the TF-IDF weighting, the more important the word is in the document or corpus. We can simply derive the TF-IDF matrix using Gensim starting from the bag-of-words representation of the preprocessed corpus previously obtained:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "# fit TF-IDF model\n",
    "model = TfidfModel(document_term_matrix)\n",
    "tf_idf = model[document_term_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFirst document in TF-IDF format (raw):\", tf_idf[0])\n",
    "print(\"First document in TF-IDF format (word, frequency):\", [[dictionary[id], freq] for id, freq in tf_idf[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the TF-IDF object for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/tf_idf_gensim.pkl\", \"wb\") as file:\n",
    "    pkl.dump(tf_idf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C3.3 Documents Similarity \n",
    "\n",
    "It is often necessary to estimate the similarity between text documents. Various similarity metrics exist for this purpose, including Cosine similarity, Jaccard Similarity, and Euclidean Distance measurement. Each of these metrics exhibits different characteristics.\n",
    "\n",
    "This tutorial provides a detailed explanation of the Jaccard Similarity metric, along with an example. You can move to subsection C3.3.2 for an exploration of the Cosine similarity metric.\n",
    "\n",
    "## C3.3.1 Jaccard - Similarity on sets\n",
    "\n",
    "Jaccard Similarity, also known as the Jaccard index and Intersection over Union, is a measure used to determine the similarity between two text documents. It indicates how closely the two documents are related in terms of their context, specifically by evaluating the number of common words compared to the total number of words.\n",
    "\n",
    "Mathematicly, Jaccard Similarity is defined as the intersection of two documents divided by the union of those two documents, which represents the ratio of common words to the total number of words.\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "J(d_1, d_2) = \\frac{d_1 \\cap d_2} {d_1 \\cup d_2}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "The Jaccard Similarity score falls within a range of 0 to 1. When the two documents are exactly the same, the Jaccard Similarity is 1. Conversely, if there are no common words between the two documents, the Jaccard Similarity score is 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will write function that takes two bags of words and calculates Jaccard index for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard_Similarity(d1, d2):\n",
    "    \n",
    "    intersection = len(set.intersection(*[set(d1), set(d2)]))\n",
    "    union = len(set.union(*[set(d1), set(d2)]))\n",
    "    \n",
    "    return float(intersection) / union\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example. we will calculate similarities between tweet 0 and tweet 8532, tweet 0 and tweet 30563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [8532,30563]:\n",
    "    print (Jaccard_Similarity(corpus[0],corpus[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check which tweet are those. For readability reasons we will change the maximum width of columns in DataFrame to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_colwidth=pd.get_option('display.max_colwidth')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "news.iloc[[0,8532,30563]][[\"headline\",\"short_description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', tmp_colwidth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard Similarity may not be considered the most suitable metric for measuring document similarity in certain cases due to a few reasons:\n",
    "\n",
    "1. Lack of consideration for word order: Jaccard Similarity only focuses on the presence or absence of words in documents, disregarding their positions or orders. This means that two documents with different word arrangements but containing the same words would still yield a high Jaccard Similarity score, even if their actual semantic similarity is low.\n",
    "\n",
    "2. Ignoring word importance: Jaccard Similarity treats all words equally, without considering their importance or relevance in capturing the essence of a document. Consequently, it may not accurately reflect the similarity between documents that contain crucial keywords or terms.\n",
    "\n",
    "3. Sensitivity to document length: Jaccard Similarity can be sensitive to the length of documents. Longer documents are more likely to have overlapping words by chance, leading to higher similarity scores, even if their topical or contextual similarity is limited.\n",
    "\n",
    "4. Lack of contextual understanding: Jaccard Similarity solely focuses on the presence or absence of words, failing to capture the broader contextual meaning or semantic relationships between words within documents. As a result, it may not effectively capture the true similarity between documents that share a common theme or topic but differ in specific word choice.\n",
    "\n",
    "Considering these limitations, alternative similarity metrics such as Cosine similarity, which considers word frequency and vector representations, or more advanced techniques like Word Embeddings or Doc2Vec, which capture semantic relationships, are often used to overcome the shortcomings of Jaccard Similarity when assessing document similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C3.3.2 TF-IDF with Cosine Similarity\n",
    "\n",
    "\n",
    "In NLP, cosine similarity is a metric used to gauge the similarity between two documents, regardless of their size. This metric involves representing words as vectors and representing the text documents in a vector space with multiple dimensions.\n",
    "\n",
    "Mathematically, cosine similarity measures the cosine of the angle formed between two n-dimensional vectors when projected in a multi-dimensional space. The resulting cosine similarity value between two documents falls within the range of 0 to 1. A score of 1 indicates that the two vectors have the same orientation, while a value closer to 0 suggests less similarity between the two documents.\n",
    "\n",
    "In other words, cosine similarity is defined as following:\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "Similarity(D_1, D_2)= cos(\\theta) = \\frac{\\mathbf{D_1}\\cdot\\mathbf{D_2} } {\\|\\mathbf{D_1}\\| \\|\\mathbf{D_2}\\|}\n",
    "\\end{align}\n",
    "$,\n",
    "where  **D_1** and **D_2** are n-dimensional vectors of attributes. \n",
    "\n",
    "Further instead of simple frequenices of word occurencies (i.e., term-document vectors), we will use TF-IDF weights described in the previous section. One should consider using TF-IDF instead of raw frequencies for several reasons:\n",
    "\n",
    "1. Accounting for term importance: TF-IDF takes into account not only the frequency of a term within a document (term frequency) but also its significance in the entire corpus (inverse document frequency). This helps to highlight terms that are more important or informative, as they are less likely to appear in every document.\n",
    "\n",
    "2. Mitigating the impact of common terms: Raw frequencies tend to assign higher importance to terms that appear frequently in a document but may not necessarily carry much meaning or distinguishability. TF-IDF reduces the weight of such common terms by scaling them down.\n",
    "\n",
    "3. Enhancing document discrimination: By emphasizing rare terms that occur in specific documents, TF-IDF can better distinguish between documents and capture their unique characteristics. It helps to highlight terms that are discriminative and contribute more to the understanding of document content.\n",
    "\n",
    "4. Flexibility across different document lengths: Unlike raw frequencies, TF-IDF can accommodate documents of varying lengths. It normalizes the term frequencies based on document length, allowing for fair comparisons between short and long documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will import package similarities from ginsim library. This package has function *Similarity* that calculates cosine similraty across a collection of documents. We would need to specify: number of features (this will be the length of our dictionary), corpus (our tf-idf object) and the path for temporary storage of the created index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the tfidf vector of corpus\n",
    "from gensim import similarities\n",
    "index=similarities.Similarity(output_prefix='./output/', corpus=tf_idf,num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let select similarities to Tweet 0 and display indexes of the 3 most similar tweets to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simil = index[tf_idf[0]] \n",
    "sorted(enumerate(simil), key=lambda item: -item[1])[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results show that most similar are Tweets 8532 and 30563. \n",
    "\n",
    "P.S.: As you have seen similarity between Tweet 0 and itself is 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, TF-IDF provides a more nuanced and context-aware representation of terms within documents, capturing their relative importance and facilitating more accurate and effective analysis, retrieval, and understanding of text data.\n",
    "\n",
    "Calculating similarity between two documents can be useful in several tasks within computational social science. Some of these tasks include:\n",
    "\n",
    "Document Clustering: Similarity measures can be employed to group or cluster similar documents together. By calculating the similarity between documents, clustering algorithms can identify patterns and group related documents, enabling researchers to explore topics or themes within a large collection of text data.\n",
    "\n",
    "Text Classification: Similarity metrics can aid in text classification tasks, where documents need to be categorized into predefined classes or labels. By measuring the similarity between an unlabeled document and a set of labeled documents, researchers can assign the unlabeled document to the most similar class.\n",
    "\n",
    "Recommender Systems: Similarity calculations are often used in recommender systems to suggest relevant documents or items based on a user's preferences or previous interactions. By measuring the similarity between a user's profile or preferences and available documents, the system can recommend items that are likely to be of interest to the user.\n",
    "\n",
    "Information Retrieval: When searching for relevant information in large document collections, similarity measures can help rank documents based on their similarity to a query. By calculating the similarity between the query and each document, researchers can retrieve the most relevant documents and present them to the user.\n",
    "\n",
    "Plagiarism Detection: Similarity calculations can be valuable in identifying cases of plagiarism or document duplication. By comparing the similarity between a given document and a set of reference documents, researchers can detect instances where substantial portions of the document have been copied from other sources.\n",
    "\n",
    "Similarity measures play a crucial role in tasks involving document analysis, clustering, classification, recommendation, information retrieval, and plagiarism detection within computational social science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Recommended readings\n",
    "\n",
    "\n",
    "\n",
    "### Complementary readings\n",
    "\n",
    "Hovy, D. (2020). Text analysis in Python for social scientists: Discovery and exploration. Cambridge University Press.\n",
    "\n",
    "Hovy, D. (2021). Text Analysis in Python for Social Scientists: Prediction and Classification. Cambridge University Press.\n",
    "\n",
    "Vajjala, S., Majumder, B., Gupta, A., & Surana, H. (2020). Practical natural language processing: a comprehensive guide to building real-world NLP systems. O'Reilly Media.\n",
    "\n",
    "Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
    "\n",
    "Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\n",
    "\n",
    "He, R., & McAuley, J. (2016, April). Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In proceedings of the 25th international conference on world wide web (pp. 507-517).: http://jmcauley.ucsd.edu/data/amazon/index_2014.html\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/natural-language-processing-guide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Credits\n",
    "\n",
    "[1] Vajjala, S., Majumder, B., Gupta, A., & Surana, H. (2020). Practical natural language processing: a comprehensive guide to building real-world NLP systems. O'Reilly Media. Chapter 1: https://www.oreilly.com/library/view/practical-natural-language/9781492054047/ch01.html\n",
    "\n",
    "POS image credit: https://machinelearningknowledge.ai/tutorial-on-spacy-part-of-speech-pos-tagging/\n",
    "spacy pipeline image credit: https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "<b>Document information</b>\n",
    "\n",
    "Contact and main author: Nicol√≤ Gozzi, N. Gizem Bacaksizlar Turbic, & Olga Zagovora\n",
    "\n",
    "Contributors: Pouria Mirelmi & Haiko Lietz\n",
    "\n",
    "Version date: 18 August 2023\n",
    "\n",
    "License: Creative Commons Attribution 4.0 International (CC BY 4.0)\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Z0-GgEFeNJbn"
   ],
   "name": "Day 1: Introduction to Jupyter Notebook and Text Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
