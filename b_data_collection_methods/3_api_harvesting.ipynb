{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26acf247",
   "metadata": {},
   "source": [
    "<img src='images/gesis.png' style='height: 60px; float: left'>\n",
    "<img src='images/social_comquant.png' style='height: 50px; float: left; margin-left: 40px'>\n",
    "<img src='images/isi.png' style='height: 50px; float: left; margin-left: 20px'>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c8be6",
   "metadata": {},
   "source": [
    "Authors = N. Gizem Bacaksizlar Turbic and Haiko Lietz\n",
    "\n",
    "Date = 19 July 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304c55db",
   "metadata": {},
   "source": [
    "## Introduction to Computational Social Science methods with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd4ea4a",
   "metadata": {},
   "source": [
    "# Session 3: API Harvesting\n",
    "\n",
    "Data collection is a procedure of gathering information from subjects (all relevant sources), measuring and analyzing accurate insights for research using various techniques. Researchers can evaluate their research questions and hypotheses on the basis of collected data. In most cases, data collection is the primary and most important step for research, irrespective of the field of study. The approach of data collection varies for different fields of study, depending on the required information.\n",
    "\n",
    "The ease of access to the technology has made various social media platforms more popular as communication tools, therefore as a source of data. With this rise of social media use as a data source, data collection using APIs has become a demanding skill. Here, in this session, we aim to teach how to collect data from various social media platforms, such as Twitter and Reddit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66abdeec",
   "metadata": {},
   "source": [
    "## 3.1. Social Media Platforms for Data Harvesting through API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c403de1f",
   "metadata": {},
   "source": [
    "<img src=\"./images/database.png\"  width=\"150\" height = \"150\" align=\"right\"/>\n",
    "\n",
    "In order to access APIs, you first need to create an account and apply to have a developer account on the platform that you want to work on. With this developer account, platforms provide you KEYS (e.g., secret, public, or access) to authenticate their system.\n",
    "\n",
    "While web scraping is one of the common ways of collecting data from websites, a lot of websites offer APIs to access the public data that they host on their website. This is to avoid unnecessary traffic on the websites.\n",
    "\n",
    "However, even though we have access to these APIs, as researchers, we should not forget to respect API access rules and always read the documents before collecting data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09370b8",
   "metadata": {},
   "source": [
    "## 3.2. A demonstration using Python to collect data from Twitter \n",
    "\n",
    "Twitter is one of the most used social media platforms in the academic research. This microblogging and social networking service hosts users who can post and interact with messages known as \"tweets\". Registered users can post, like, and retweet tweets, but unregistered users can only read those that are publicly available. As of 2022, Twitter has 436 million active users worldwide (Statista, 2022*). \n",
    "\n",
    "<img src=\"./images/twitter.png\"  width=\"200\" height = \"200\" align=\"left\"/>\n",
    "\n",
    "Different access options for different purposes:\n",
    "\n",
    "- Twitter Developer: https://developer.twitter.com/\n",
    "- APIs: https://developer.twitter.com/en/docs\n",
    "- GNIP: http://support.gnip.com/apis/\n",
    "- Twitter Enterprise: https://developer.twitter.com/en/enterprise\n",
    "\n",
    "IMPORTANT to note that free APIs cover 7 days Tweets; Premium APIs exist for 30-day search and beyond. If you have an Academic Research access level, you can access even more data with full-archive search endpoint. There are changes to APIs policies over time, such as functionalities and user agreements. Also, limitations on volume and functions should be considered. \n",
    "\n",
    "Before we start with our first project on Twitter, first you need to sign up for Twitter and then, create a Developer account: \n",
    "\n",
    "- Sign up from [here.](https://help.twitter.com/en/using-twitter/create-twitter-account)\n",
    "- Create a Developer Account from [here.](https://developer.twitter.com)\n",
    "\n",
    "\n",
    "**https://www.statista.com/statistics/272014/global-social-networks-ranked-by-number-of-users/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194eb703",
   "metadata": {},
   "source": [
    "### 3.2.1 Getting started\n",
    "\n",
    "In this section, we will begin with our first project of collecting tweets. Import the following libraries if you have already installed them. If you have not, install them using pip in your command prompt, or using !pip in your jupyter notebook.\n",
    "\n",
    "We will be using `datetime` library for working with human readable date formats, and `tweepy` as the python wrapper of the twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193473b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import tweepy as tw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3088ebf3",
   "metadata": {},
   "source": [
    "You will have to use your twitter keys registered with your account. Create an account if you don't have one yet. Then obtain the access token and access token secret. They can be generated in your developer portal [here](https://developer.twitter.com/en/portal/dashboard), under \"Keys and tokens\" tab of your developer app.\n",
    "\n",
    "After getting them ready, use the following variables to save them for further steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f9a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "apikey = 'YOURapikey' #25 alphanumeric characters\n",
    "apisecretkey = 'YOURapisecretkey'\n",
    "accesstoken = 'YOURaccesstoken'\n",
    "accesstokensecret = 'YOURaccesstokensecret'\n",
    "bearertoken = 'YOURbearertoken'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39538a76",
   "metadata": {},
   "source": [
    "<img src=\"./images/developer_portal.png\"  width=\"500\" height = \"500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070d9b3f",
   "metadata": {},
   "source": [
    "If you are sharing your scripts with other people and want to keep your keys secret, you can follow the steps below instead of assigning your keys in the above 5 variables:\n",
    "\n",
    "- Create a simple python script called `keys.py`\n",
    "- Store all passwords the way you did in the notebook already, with the same names\n",
    "- Save the script in the same folder as this notebook's\n",
    "- Import the keys like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7068dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keys import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c73743",
   "metadata": {},
   "source": [
    "The next step is setting up your access to the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(apikey, apisecretkey)\n",
    "auth.set_access_token(accesstoken, accesstokensecret)\n",
    "api = tw.API(auth, wait_on_rate_limit = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d43db28",
   "metadata": {},
   "source": [
    "### 3.2.2 Retrieving tweets with keyword search\n",
    "\n",
    "Now we want to retrieve the tweets that contain certain words. Let's say we want to get the ones that contain *at least* one of the words **ComputationalSocialScience**, **GESIS** or **SocialComQuant**. We need to save them in a string, seperated with `OR`s like the following (You can try with any other search terms of your own choice):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62217aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words = \"ComputationalSocialScience OR GESIS OR SocialComQuant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962f766",
   "metadata": {},
   "source": [
    "If you want to remove retweets from your search results, you can include `-filter:retweets` in the `search_words` string.\n",
    "\n",
    "Now we collect the desired tweets like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7fc12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tw.Cursor(api.search_tweets,  q=search_words, lang=\"en\").items()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce36775",
   "metadata": {},
   "source": [
    "**Note**: Be aware of the attribute names from the new version of the packages, they may change in time.\n",
    "\n",
    "You can pass a number as an argument to the `.items()` at the end of the line to limit the number of search results.\n",
    "\n",
    "The tweets that are now kept in the `tweets` object above contain a lot of information, in the form of dictionaries. You can check an overview of this information [here](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet).\n",
    "\n",
    "We will take a look at some of this information and store it in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474926c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_details = [[tweet.user.screen_name, tweet.user.id, tweet.id_str, \n",
    "                  tweet.created_at, tweet.text, tweet.user.profile_image_url, tweet.user.location] \n",
    "                  for tweet in tweets]\n",
    "\n",
    "tweets_df = pd.DataFrame(data=tweet_details, \n",
    "                        columns = [\"user_name\",\"user_id\", \"tweet_id\", \"tweet_date\",\"tweet\",\"user_image\",\n",
    "                                   \"user_location\"])\n",
    "\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c46586a",
   "metadata": {},
   "source": [
    "Here is some information about the data each column keeps:\n",
    "\n",
    "- user_name: The username of the user that has tweeted the desired tweet.\n",
    "- user_id: Each user has a unique ID, this columns keeps those IDs.\n",
    "- tweet_id: Each tweets has also a unique ID.\n",
    "- tweet_date: The date that the tweet has been posted.\n",
    "- tweet: The text of the tweet\n",
    "- user_image: The profile photo of the user\n",
    "- user_location: The location of the user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc571629",
   "metadata": {},
   "source": [
    "You can store the dataframe for later access, if you need to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970e72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv(\"./data/test_tweets.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1807e572",
   "metadata": {},
   "source": [
    "You can also get the number of the tweets retrieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af3e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The length of the dataframe:', len(tweets_df['tweet_id'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b323d0dd",
   "metadata": {},
   "source": [
    "To access the user photos, you can simply search the links in your browser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57270ca6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_df.user_image.values[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7ffde1",
   "metadata": {},
   "source": [
    "### 3.2.3 Retrieving users' information\n",
    "\n",
    "Let's say you have a list of users. This can be a list of IDs like the ones we have in the `user_id` column of the dataframe in the previous section, or it can be a list of usernames as shown on the screen for every user in twitter (Like the ones we have in the `user_name` column of the dataframe in the previous section).\n",
    "\n",
    "We can access the infromation like profile description or profile photo of any of these users. As an example, take the following list of users. It contains the first 10 unique user IDs of the dataframe in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e1459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ids = list(tweets_df['user_id'][:50].unique())\n",
    "users_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a2555",
   "metadata": {},
   "source": [
    "We can get their profile information using `get_user()` and store it in the `information`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "information = []\n",
    "\n",
    "for i in users_ids:\n",
    "    user = api.get_user(user_id = i)\n",
    "    information.append(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba5d8ef",
   "metadata": {},
   "source": [
    "Next, we can extract information like users' location, profile description, profile photo, etc and make a dataframe to keep them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd25167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "locations = []\n",
    "descriptions = []\n",
    "profile_pics = []\n",
    "background_pics = []\n",
    "friends = []\n",
    "followers = []\n",
    "\n",
    "for i in information:   \n",
    "    names.append(i.name)\n",
    "    locations.append(i.location)\n",
    "    descriptions.append(i.description)\n",
    "    profile_pics.append(i.profile_image_url)\n",
    "    background_pics.append(i.profile_background_image_url)\n",
    "    friends.append(i.friends_count)\n",
    "    followers.append(i.followers_count)\n",
    "    \n",
    "users_df = pd.DataFrame({'ID': users_ids, \"name\": names, \"location\": locations, \"description\": descriptions, \"profile picture\": profile_pics, \"background picture\": background_pics,\n",
    "                                   \"friends\": friends, \"followers\": followers})\n",
    "\n",
    "users_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ab966",
   "metadata": {},
   "source": [
    "If, instead of list of users IDs, you have a list of usernames, you can still get the information above. You just need to change the argument name to `screen_name` in `get_user()` function.\n",
    "\n",
    "For example, let's say your list of usernames is something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ed9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames = list(tweets_df['user_name'][:50].unique())\n",
    "usernames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7488831",
   "metadata": {},
   "source": [
    "It can be done like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f261c2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "information = []\n",
    "\n",
    "for i in usernames:\n",
    "    user = api.get_user(screen_name = i)\n",
    "    information.append(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d203ea",
   "metadata": {},
   "source": [
    "The rest is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85c984",
   "metadata": {},
   "source": [
    "### 3.2.4 Rehydrating tweets\n",
    "\n",
    "In research, sharing large tweet data sets is done through sharing tweet identifiers, since Twitter Terms of Service does not allow researchers to share the full tweets data. In order to get the tweets used in a research work, we need to retrieve/reconstruct the tweets data using those tweet identifiers. This is called hydrating/rehydrating tweets.\n",
    "\n",
    "Since some of the tweets used in a research work migh have been deleted in time, we may not be able to access every single tweet used at the time when that research work has been done. We will see about that in more details later in this section.\n",
    "\n",
    "In order to rehydrate tweets, we will be using Twarc library, which is a python wrapper for twitter API. You can install it with `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f9f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from twarc import Twarc2, expansions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e45c6a6",
   "metadata": {},
   "source": [
    "We will rehydrate tweets from [this](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0237073) paper for our teaching purposes here. Download the `2021.csv` data set from [this](https://figshare.com/articles/dataset/The_Twitter_Parliamentarian_Database/10120685) link and read it like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ea7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('2021.csv', header = None)\n",
    "data.columns = ['country', 'party', 'author', 'author_id', 'district','date','tweet_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba123f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eddba2e",
   "metadata": {},
   "source": [
    "We will take the tweets for Turkey and keep their IDs to rehydrate. We'll try rehydrating a random sample of 1000 of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "turkey = data[data['country'] == 'Turkey']\n",
    "\n",
    "tweet_ids = list(turkey.sample(1000)['tweet_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be561b5e",
   "metadata": {},
   "source": [
    "We will use the following `rehydrate` [function](https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research/blob/main/modules/6a-labs-code-academic-python.md) to rehydrate the tweets and keep their data in the `tweets` list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf42714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Use your bearer token here\n",
    "client = Twarc2(bearer_token=bearer_token)\n",
    "\n",
    "tweets = []\n",
    "\n",
    "def rehydrate(ids: list):\n",
    "    # List of Tweet IDs you want to lookup\n",
    "    tweet_ids = ids\n",
    "    # The tweet_lookup function from twarc \n",
    "    lookup = client.tweet_lookup(tweet_ids=tweet_ids)\n",
    "    for page in lookup:\n",
    "        # The Twitter API v2 returns the Tweet information and the user, media etc.  separately\n",
    "        # so we use expansions.flatten to get all the information in a single JSON\n",
    "        result = expansions.flatten(page)\n",
    "        for tweet in result:\n",
    "            tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e97db8",
   "metadata": {},
   "source": [
    "Running the function for the 1000 tweet IDs takes around 30 seconds, since twarc sends a GET request for 100 tweet IDs every 3 seconds. More on twitter rate limits [here](https://developer.twitter.com/en/docs/twitter-api/rate-limits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rehydrate(tweet_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0d3874",
   "metadata": {},
   "source": [
    "To see the information returned for each tweet ID, we can check the first item in `tweets` list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cecb157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4340a782",
   "metadata": {},
   "source": [
    "We can also check to see how many tweets could have been rehydrated from the IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f4b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94324fbb",
   "metadata": {},
   "source": [
    "As you can see, almost 92 percent of tweets could have been rehydrated; others are not available anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ef1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "turkey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffc42e5",
   "metadata": {},
   "source": [
    "### 3.2.5 Getting users info\n",
    "\n",
    "Condider the `turkey` dataframe that we created from the `2021.csv` data in the previous section. We want to get the profile information of a fraction of the politicians whose tweets are in that dataframe. First, we get the unique politicians in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6741b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all IDs in the dataframe \n",
    "author_ids = turkey['author_id']\n",
    "\n",
    "# Getting unique IDs\n",
    "unique_ids = list(author_ids.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d144e5",
   "metadata": {},
   "source": [
    "With the following `get_user()` [function](https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research/blob/main/modules/6a-labs-code-academic-python.md), we can get the users' information based on their IDs (It's a bit like tweet rehydration, but we use users' IDs this time), and save them in the `users` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece47f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from twarc import Twarc2, expansions\n",
    "import json\n",
    "\n",
    "users = []\n",
    "\n",
    "# Replace your bearer token below\n",
    "client = Twarc2(bearer_token=bearer_token)\n",
    "\n",
    "def get_user(ids):\n",
    "    # List of user IDs to lookup, add the ones you would like to lookup\n",
    "    users = ids\n",
    "    # The user_lookup function gets the hydrated user information for specified users\n",
    "    lookup = client.user_lookup(users=users)\n",
    "    for page in lookup:\n",
    "        result = expansions.flatten(page)\n",
    "        for user in result:\n",
    "            # Here we are printing the full Tweet object JSON to the console\n",
    "            users.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62673100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "some_ids = random.sample(unique_ids, 50)\n",
    "\n",
    "get_user(some_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2f1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354c28db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecaa8a5c",
   "metadata": {},
   "source": [
    "### 3.2.6 Keyword search limited to a time window\n",
    "\n",
    "We can use the the `search_all()` function of twarc to search for tweets in any time window of our choice. The following `search()` [function](https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research/blob/main/modules/6a-labs-code-academic-python.md) looks for any tweet containing the query it takes, limited to a beginning and end time, and saves them into the `tweets` list.\n",
    "\n",
    "\n",
    "You can find more information on writing search queries [here](https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research/blob/main/modules/5-how-to-write-search-queries.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from twarc import Twarc2, expansions\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "# Replace your bearer token below\n",
    "client = Twarc2(bearer_token=bearer_token)\n",
    "\n",
    "def search(beginning, end, q):\n",
    "    \n",
    "    # Specify the start time in UTC for the time period you want Tweets from\n",
    "    start_time = beginning\n",
    "\n",
    "    # Specify the end time in UTC for the time period you want Tweets from\n",
    "    end_time = end\n",
    "\n",
    "    # This is where we specify our query\n",
    "    query = q\n",
    "\n",
    "    # The search_all method call the full-archive search endpoint to get Tweets based on the query, start and end times\n",
    "    search_results = client.search_all(query=query, start_time=beginning, end_time=end, max_results=100)\n",
    "\n",
    "    # Twarc returns all Tweets for the criteria set above, so we page through the results\n",
    "    for page in search_results:\n",
    "        # The Twitter API v2 returns the Tweet information and the user, media etc.  separately\n",
    "        # so we use expansions.flatten to get all the information in a single JSON\n",
    "        result = expansions.flatten(page)\n",
    "        for tweet in result:\n",
    "            # Here we are printing the full Tweet object JSON to the console\n",
    "            tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c4891",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "# Beginning time for the time window\n",
    "beginning = datetime.datetime(2023, 1, 5, 0, 0, 0, 0, datetime.timezone.utc)\n",
    "\n",
    "# End time for the time window\n",
    "end = datetime.datetime(2023, 1, 8, 0, 0, 0, 0, datetime.timezone.utc)\n",
    "\n",
    "# The query for searching\n",
    "q = \"ComputationalSocialScience\"\n",
    "\n",
    "search (beginning, end, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4fc6f0",
   "metadata": {},
   "source": [
    "You can take a look at the number of retrieved tweets and the first tweet like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af37f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of retrieved tweets\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12998500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The information available for each tweet\n",
    "tweets[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd3f98c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The overall information of the first tweet \n",
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03e4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7af42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be50d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927ff1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fdcbb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ca882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API v2 (if you have a full access)\n",
    "client = tw.Client(bearer_token=bearer_token)\n",
    "\n",
    "# Replace with your own search query\n",
    "query = 'from:SocialComquant -is:retweet' # you can change from with your own choice of username (without retweets)\n",
    "\n",
    "# Replace with time period of your choice\n",
    "start_time = '2021-01-01T00:00:00Z'\n",
    "\n",
    "# Replace with time period of your choice\n",
    "end_time = '2022-01-01T00:00:00Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e314ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the start_time by yourself with writing\n",
    "start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175eb30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# You can search Tweets from the last 7 days or all Tweets with different functions. Check available functions in Tweepy!\n",
    "Tweepy: https://docs.tweepy.org/en/stable/client.html#search-tweets\n",
    "# A helpful link for setting up your query: \n",
    "https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research/blob/main/modules/5-how-to-write-search-queries.md\n",
    "'''\n",
    "# Connect to Twitter API and search all tweets if you have a full academic access\n",
    "tweets = client.search_all_tweets(query=query, tweet_fields=['created_at','text', 'context_annotations','entities'],\n",
    "                                  start_time=start_time,\n",
    "                                  end_time=end_time, max_results=10) #set your max results between 10 and 500\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09368050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see a fairly new field for context annotations.\n",
    "for tweet in tweets.data:\n",
    "    print(tweet.created_at)\n",
    "    print(tweet.context_annotations) #context annotations (https://developer.twitter.com/en/docs/twitter-api/annotations/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6182d3a1",
   "metadata": {},
   "source": [
    "## 3.3. A demonstration using Python to collect Reddit comments <img src=\"./images/reddit.svg\"  width=\"150\" height = \"150\" align=\"right\"/>\n",
    "\n",
    "Reddit is one of the oldest social media platforms which is still generating content with its users. Millions of users are creating on a daily basis in the form of questions and comments. Reddit also offers such API which is easy to access this vast amount of data.\n",
    "\n",
    "First thing you need to do is to have a Reddit account. You should create it from [here.](https://www.reddit.com/)\n",
    "- [Official Reddit API](https://www.reddit.com/dev/api/)\n",
    "    - [Collecting Reddit data](https://towardsdatascience.com/scrape-reddit-data-using-python-and-google-bigquery-44180b579892)\n",
    "    \n",
    "Alternative ways of getting Reddit data:\n",
    "- [Google BigQuery](https://cloud.google.com/bigquery) (GBQ)\n",
    "    - [Scraping Reddit data with GBQ](https://towardsdatascience.com/scrape-reddit-data-using-python-and-google-bigquery-44180b579892)\n",
    "- [Pushshift.io](https://medium.com/@RareLoot/using-pushshifts-api-to-extract-reddit-submissions-fb517b286563)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2deb3d30",
   "metadata": {},
   "source": [
    "''' \n",
    "    Let's get started with our first project of colleting data from Reddit.\n",
    "    Import libraries if you install them before.\n",
    "    If you have not installed them, then install with pip on your command prompt or your jupyter notebook with !pip.\n",
    "'''\n",
    "# import relevant packages\n",
    "import pandas as pd # data manipulation library\n",
    "import praw # Python Reddit API Wrapper libray\n",
    "# Please make sure you have installed all of these libraries!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66c761a3",
   "metadata": {},
   "source": [
    "# Enter your keys registered with Reddit\n",
    "# Client_id: your 14 characters personal use script key\n",
    "# Client_secret: your 27 characters secret key\n",
    "# Username and password are your Reddit account credentials\n",
    "reddit = praw.Reddit(user_agent='Comment Extraction (by /u/USERNAME)',\n",
    "                     client_id='**********',client_secret=\"***********\",username='********', password='*******')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e82a004",
   "metadata": {},
   "source": [
    "We need to decide which subreddit you would like to focus on getting the data: Let's say \"Computational Social Science\" and be creative :)\n",
    "\n",
    "title, score, url, id, number of comments, date of creation, body text are the fields that are available from Reddit API. \n",
    "Here, we will focus on getting the bodytext(comments) from the subreddit. Refer to [praw documentation](https://praw.readthedocs.io/en/latest/code_overview/models/subreddit.html) for different kinds of implementations. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca12bd79",
   "metadata": {},
   "source": [
    "comm_list = [] # empty list for the comments\n",
    "header_list = [] # empty list for header_list\n",
    "i = 0\n",
    "for submission in reddit.subreddit('Computational Social Science').hot(limit=2): # .hot(limit) can be any number\n",
    "    submission.comments.replace_more(limit=None) # Getting the list of comment ids\n",
    "    comment_queue = submission.comments[:]  # Seed with top-level\n",
    "    while comment_queue:\n",
    "        header_list.append(submission.title)\n",
    "        comment = comment_queue.pop(0)\n",
    "        comm_list.append(comment.body)\n",
    "        t = []\n",
    "        t.extend(comment.replies)\n",
    "        while t: # If the comment has nested replies, it will enter into the next loop and will extract information.\n",
    "            header_list.append(submission.title)\n",
    "            reply = t.pop(0)\n",
    "            comm_list.append(reply.body)\n",
    "            \n",
    "df = pd.DataFrame(header_list)\n",
    "df['comm_list'] = comm_list\n",
    "df.columns = ['header','comments']\n",
    "df['comments'] = df['comments'].apply(lambda x : x.replace('\\n',''))\n",
    "df.to_csv('CSS_comments.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c76de5",
   "metadata": {},
   "source": [
    "## 3.4. More APIs and precollected datasets \n",
    "\n",
    "<img src=\"./images/datasets.jpg\" width=\"500\" height = \"900\" align=\"left\"/>  \n",
    "\n",
    "- __More APIs__\n",
    "\n",
    "    [Facebook for Developers](https://developers.facebook.com/)  \n",
    "    [Facebook Ads API](https://developers.facebook.com/docs/marketing-apis/)  \n",
    "    [Instagram Developer](https://developers.facebook.com/docs/instagram-basic-display-api)  \n",
    "    [YouTube Developers](https://developers.google.com/youtube/)  \n",
    "    [Weibo API](http://open.weibo.com/wiki/API%E6%96%87%E6%A1%A3/en)  \n",
    "    [CrowdTangle](https://www.crowdtangle.com/request)  \n",
    "    [4chan](https://github.com/4chan/4chan-API)  \n",
    "    [Gab](https://github.com/a-tal/gab)  \n",
    "    [Github REST API](https://docs.github.com/en/rest)  \n",
    "    [Github GraphQL](https://docs.github.com/en/graphql)  \n",
    "    [Stackoverflow](https://api.stackexchange.com/docs)  \n",
    "    [Facepager](https://github.com/strohne/Facepager)  \n",
    "\n",
    "\n",
    "- __Precollected datasets__  \n",
    "    https://datasetsearch.research.google.com  \n",
    "    https://www.kaggle.com/datasets  \n",
    "    https://data.gesis.org/sharing/#!Search  \n",
    "\n",
    "\n",
    "- __Locating or Requesting Social Media Data__\n",
    "    https://www.programmableweb.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac0efa",
   "metadata": {},
   "source": [
    "## 3.5. Data harvesting from Wikipedia through API\n",
    "\n",
    "<img src='images/wikipedia_logo.png' style='height: 190px; float: right; margin-left: 50px' >\n",
    "\n",
    "Wikipedia is a rich source of data for social science research. Although we can access its data through other techniques like web scraping, there are also useful APIs that could ease collecting data from the website.\n",
    "\n",
    "Since Wikipedia is built on [MediaWiki](https://en.wikipedia.org/wiki/MediaWiki), we will be using python wrappers written for its API, [Mediawiki Action API](https://www.mediawiki.org/wiki/API:Main_page). Each of these wrappers provide some useful methods, and we will try to go through the ones that are the most important to our data collection tasks.\n",
    "\n",
    "We will also introduce two useful parsers for the Wikipedia markup language, and will see how they could be used for extracting clean data from the raw markup code.\n",
    "\n",
    "### 3.5.1 Wikipedia library\n",
    "\n",
    "https://wikipedia.readthedocs.io/en/latest/code.html#api\n",
    "\n",
    "Installation and importing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e81e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f28f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c2a9e",
   "metadata": {},
   "source": [
    "Searching a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9515cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.search(\"Barack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be845c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.suggest(\"Barak Obama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a31e0b",
   "metadata": {},
   "source": [
    "Fewer or more results with a specific number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f09823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.search(\"Ford\", results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f2e934",
   "metadata": {},
   "source": [
    "Getting the summary of an article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.summary(\"Barack Obama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cad19a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.summary(\"Barack Obama\", sentences=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23126943",
   "metadata": {},
   "source": [
    "wikipedia.summary will raise a DisambiguationError if the page is a disambiguation page, or a PageError if the page doesn’t exist (although by default, it tries to find the page you meant with suggest and search.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b34048",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.summary(\"Mercury\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ba9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mercury = wikipedia.summary(\"Mercury\")\n",
    "except wikipedia.exceptions.DisambiguationError as e:\n",
    "    print (e.options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c68225c",
   "metadata": {},
   "source": [
    "wikipedia.page enables you to load and access data from full Wikipedia pages. Initialize with a page title (keep in mind the errors listed above), and then access most properties using property methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc46d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "bo = wikipedia.page(\"Barack Obama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60b78ad",
   "metadata": {},
   "source": [
    "Getting the title of the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d27d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bo.title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e9d658",
   "metadata": {},
   "source": [
    "Getting the url of the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3f6b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "bo.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102546aa",
   "metadata": {},
   "source": [
    "Getting the full text of the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3dd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "bo.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c95d7ea",
   "metadata": {},
   "source": [
    "Getting the images of the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5672abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bo.images[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7853f8",
   "metadata": {},
   "source": [
    "Getting the links in the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ef619",
   "metadata": {},
   "outputs": [],
   "source": [
    "bo.links[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a738f891",
   "metadata": {},
   "source": [
    "To change the language of the Wikipedia you are accessing, use wikipedia.set_lang. Remember to search for page titles in the language that you have set, not English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8edbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.set_lang(\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.summary(\"Francois Hollande\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffebbbd",
   "metadata": {},
   "source": [
    "List of URLs of the external links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bo.references[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de8050",
   "metadata": {},
   "source": [
    "Getting the plain text content of a section in the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e0dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bo.section('Early life and career')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8441157",
   "metadata": {},
   "source": [
    "List of section titles: an example of a bug!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642e7b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bo.sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12e4cdb",
   "metadata": {},
   "source": [
    "### 3.5.2 Pywikibot & parsers\n",
    "\n",
    "https://doc.wikimedia.org/pywikibot/stable/\n",
    "\n",
    "https://mwparserfromhell.readthedocs.io/en/latest/index.html\n",
    "\n",
    "https://github.com/5j9/wikitextparser\n",
    "\n",
    "Using pywikibot to get the wikipedia markup code and then parse it with parsers like mwparserfromhell and wikitextparser.\n",
    "\n",
    "Installation and importing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bffa610",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pywikibot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2802fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af5f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wikitextparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52914658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywikibot\n",
    "import mwparserfromhell as mwp\n",
    "import wikitextparser as wtp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de06950",
   "metadata": {},
   "source": [
    "Getting the markup code of the page [List of political parties in Germany]('https://en.wikipedia.org/wiki/List_of_political_parties_in_Germany'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b199e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "site = pywikibot.Site('en', 'wikipedia')\n",
    "page = pywikibot.Page(site, \"List of political parties in Germany\")\n",
    "# text = page.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc772410",
   "metadata": {},
   "outputs": [],
   "source": [
    "revs = page.revisions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03923edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikicode = mwp.parse(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c961ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikicode.get_sections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e1c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99432ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "revsl = []\n",
    "for i in revs:\n",
    "    revsl.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71563a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "revsl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7234dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev1 = revsl[0].text\n",
    "# page = wtp.parse(rev1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce297d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "revsl[1000]['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e45b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e723131",
   "metadata": {},
   "source": [
    "Parsing the page with wikitextparser, by first making a page object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b9cc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = wtp.parse(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcbd022",
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d5742b",
   "metadata": {},
   "source": [
    "Getting page templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd036cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.templates[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d74182c",
   "metadata": {},
   "source": [
    "Like in the previous section, we can get the links in the page, this time with a different order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f0672",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.wikilinks[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e63fa6",
   "metadata": {},
   "source": [
    "Getting sections, no bugs with wikitexmtparser!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6160c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.sections[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be93c7f",
   "metadata": {},
   "source": [
    "Tables data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = page.tables[1].data()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af55af",
   "metadata": {},
   "source": [
    "Putting the data in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30e4e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data[1:])\n",
    "df.columns = data[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb05f3f",
   "metadata": {},
   "source": [
    "Parsing each cells data with mwparserfromhell and then making the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i])):\n",
    "        wikicode = mwp.parse(data[i][j])\n",
    "        data[i][j] = wikicode.strip_code(data[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8f925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data[1:])\n",
    "df.columns = data[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d020a393",
   "metadata": {},
   "source": [
    "### Alternatives for extracting tables data:\n",
    "\n",
    "**1. wikitables library:** Small bugs need to be handled by hand:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c553191",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wikitables import import_tables\n",
    "\n",
    "tables = import_tables('List of political parties in Germany')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59348be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f51fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tables[0].rows[0]['Abbr.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e31f0e9",
   "metadata": {},
   "source": [
    "**2. Introducing DBpedia:** www.dbpedia.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3783843f",
   "metadata": {},
   "source": [
    "### 3.5.3 Pywikibot & parsers 2: Main text of different revisions\n",
    "\n",
    "Extracting the main text of the first revision of an article in each year since the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywikibot\n",
    "import mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45428be",
   "metadata": {},
   "outputs": [],
   "source": [
    "site = pywikibot.Site('en', 'wikipedia')\n",
    "page = pywikibot.Page(site, \"Koç University\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b8a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "revisions = page.revisions(content=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d99ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "revisions_list = []\n",
    "years = []\n",
    "\n",
    "for i in revisions:\n",
    "    revisions_list.append(i)\n",
    "    years.append(int(str(i['timestamp'])[:4]))\n",
    "years.reverse()\n",
    "revisions_list.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7c04ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba7ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# revisions_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4963af",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_revisions = []\n",
    "for i in range(years[0], years[-1]+1):\n",
    "    index = years.index(i)\n",
    "    yearly_revisions.append(revisions_list[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d90e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearly_revisions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafeb949",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = yearly_revisions[-1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899c8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = mwparserfromhell.parse(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c5173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parsed.strip_code())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c536c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "425df720",
   "metadata": {},
   "source": [
    "## 3.6. Challanges\n",
    "\n",
    "Facebook completely closed down many of it’s APIs and it is not very hard to get Facebook data besides CrowdTangle or FB Ads.\n",
    "\n",
    "Twitter’s API now has the version 2 with substantial changes. \n",
    "\n",
    "These challanges make us stay vigilant and continuously update our code to keep up with the APIs.\n",
    "\n",
    "- More on Social Media data collection and data quality:\n",
    "https://www.slideshare.net/suchprettyeyes/working-with-socialmedia-data-ethics-good-practice-around-collecting-using-and-storing-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe29c81",
   "metadata": {},
   "source": [
    "## 3.7. References\n",
    "\n",
    "Zenk-Möltgen, Wolfgang (GESIS - Leibniz Institute for the Social Sciences), Python Script to rehydrate Tweets from Tweet IDs https://doi.org/10.7802/1504\n",
    "\n",
    "Pfeffer, Morstatter (2016): Geotagged Twitter posts from the United States: A tweet collection to investigate representativeness. Dataset. http://dx.doi.org/10.7802/1166\n",
    "\n",
    "Do not miss checking out the Social Comquant Workshop 10 at:https://github.com/strohne/autocol\n",
    "\n",
    "- Useful links for getting started with Twitter API v2\n",
    "    - [Comprehensive Guide for Using the Twitter API v2](https://dev.to/twitterdev/a-comprehensive-guide-for-using-the-twitter-api-v2-using-tweepy-in-python-15d9#:~:text=Tweepy%20is%20a%20popular%20package,the%20academic%20research%20product%20track)\n",
    "    - [Step by Step Guide to Making Your First Request to the Twitter API v2](https://developer.twitter.com/en/docs/tutorials/step-by-step-guide-to-making-your-first-request-to-the-twitter-api-v2)\n",
    "    - [Getting Started with Data Collection Using Twitter API v2](https://towardsdatascience.com/getting-started-with-data-collection-using-twitter-api-v2-in-less-than-an-hour-600fbd5b5558#39c4)\n",
    "    - [An Extensive Guide to Collecting Tweets from Twitter API v2 for Academic REsearch Using Python 3](https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a)\n",
    "    - [What Pythong package is best for getting data from Twitter](https://towardsdatascience.com/what-python-package-is-best-for-getting-data-from-twitter-comparing-tweepy-and-twint-f481005eccc9)\n",
    "\n",
    "- Useful links for getting started with Reddit API\n",
    "    - https://www.reddit.com/r/TheoryOfReddit/wiki/collecting_data/- \n",
    "    - https://towardsdatascience.com/scrape-reddit-data-using-python-and-google-bigquery-44180b579892\n",
    "    - https://github.com/akhilesh-reddy/Cable-cord-cutter-Sentiment-analysis-using-Reddit-data\n",
    "    \n",
    "<a href=\"https://www.flaticon.com/free-icons/database\" title=\"database icons\">Database icons created by Smashicons - Flaticon</a>\n",
    "\n",
    "<a href=\"https://de.freepik.com/vektoren/logo\">Logo Vektor erstellt von rawpixel.com - de.freepik.com</a>\n",
    "\n",
    "<a href=\"http://www.freepik.com\">Designed by stories / Freepik</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3932b9e5",
   "metadata": {},
   "source": [
    "### Note: Alternative Ways for Twitter Academic API or Premium Account\n",
    "\n",
    "The search function mandatorily requires environment label and query argument. Label your Application on Twitter Developer page: https://developer.twitter.com/en/account/environments\n",
    "\n",
    "You can optionally add the fromDate and toDate fields to filter search results by time.\n",
    "\n",
    "The format of dates should \"YYYYMMDDHHMM\".\n",
    "\n",
    "tweets_month = api.search_30_day(label='teaching', query=search_words, \n",
    "                                 fromDate=\"202202201000\", toDate=\"202203010000\")\n",
    "\n",
    "Now, you can dump your results into json format *don't forget to import json*: print(json.dumps(tweet_results[0]._json, indent=4, sort_keys=True))\n",
    "                                 \n",
    "For further interest, visit: https://towardsdatascience.com/how-to-use-twitter-premium-search-apis-for-mining-tweets-2705bbaddca\n",
    "\n",
    "Also, there is another library called Twarc2 to explore for further data collection with Twitter v2 API:\n",
    "https://twarc-project.readthedocs.io/en/latest/api/client2/\n",
    "\n",
    "An academic research product:\n",
    "https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research/blob/main/modules/6a-labs-code-academic-python.md\n",
    "\n",
    "A standart product: \n",
    "https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research/blob/main/modules/6b-labs-code-standard-python.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
