{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/gesis.png' style='height: 50px; float: left'>\n",
    "<img src='images/social_comquant.png' style='height: 50px; float: left; margin-left: 40px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes to be removed before publication\n",
    "\n",
    "Still work in progres..\n",
    "Reviewers: Arnim (author)?, more GESIS people, or SCQ summer school/workshops? Yelena, Malak, Ahmed\n",
    "\n",
    "Review intro\n",
    "\n",
    "Review and finish red boxes\n",
    "\n",
    "Add insight boxes more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Computational Social Science methods with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6: Natural Language Processing\n",
    "The field of study that focuses on the interactions between human language and computers is called natural language processing (NLP).\n",
    "\n",
    "NLP is a field of artificial intelligence in which computers analyze, understand, and derive meaning from human language in a smart and useful way. NLP systems are used exploiting the signals in our language used to predict all of the aforementioned features: people’s age (Nguyen et al., 2011; Rosenthal & McKeown, 2011), gender (Alowibdi et al., 2013; Ciot et al., 2013; Liu & Ruths, 2013), personality (Park et al., 2015), job title (Preoţiuc-Pietro et al., 2015a), income (Preoţiuc-Pietro et al., 2015b), and much more (Volkova et al., 2014, 2015).\n",
    "\n",
    "In NLP, word embeddings have been at the forefront of this progress, which has expanded to include flexible model architectures (Hovy, 2021). The most publicly visible example of this shift is probably the translation quality of services like Google Translate (Wu et al., 2016).\n",
    "\n",
    "A collection of fundamental tasks appear frequently across various NLP projects (Vajjala et al., 2020). Let’s briefly introduce them (Figure 1):\n",
    "\n",
    "*Language modeling* is the task of predicting what the next word in a sentence will be based on the history of previous words. The goal of this task is to learn the probability of a sequence of words appearing in a given language. Language modeling is useful for building solutions for a wide variety of problems, such as **speech recognition, optical character recognition, handwriting recognition, machine translation, and spelling correction**.\n",
    "\n",
    "*Text classification* is the task of bucketing the text into a known set of categories based on its content. Text classification is by far the most popular task in NLP and is used in a variety of tools, from **email spam identification** to **sentiment analysis**.\n",
    "\n",
    "*Information extraction* is the task of extracting relevant information from text, such as **calendar events from emails** or the **names of people mentioned** in a social media post.\n",
    "\n",
    "*Information retrieval* is the task of finding documents relevant to a user query from a large collection. Applications like **Google Search** are well-known use cases of information retrieval.\n",
    "\n",
    "*Conversational agent* is the task of building dialogue systems that can converse in human languages. **Alexa** and **Siri** are some common applications of this task.\n",
    "\n",
    "*Text summarization* aims to create short summaries of longer documents while retaining the **core content** and preserving the **overall meaning** of the text.\n",
    "\n",
    "*Question answering* is the task of building a system that can automatically answer questions posed in natural language.\n",
    "\n",
    "*Machine translation* is the task of converting a piece of text from one language to another. Tools like **Google Translate** are common applications of this task.\n",
    "\n",
    "*Topic modeling* is the task of uncovering the topical structure of a large collection of documents. Topic modeling is a common text-mining tool and is used in a wide range of domains, from **literature** to **bioinformatics**.\n",
    "\n",
    "<img src='images/nlp_tasks.png' style='height: 500px; float: right'>\n",
    "\n",
    "Understanding human language is considered as a difficult task due to its complexity. For example, there is an infinite number of different ways to arrange words in a sentence. \n",
    "\n",
    "Also, words can have several meanings and contextual information is necessary to correctly interpret sentences as every language is unique and ambiguous. The ambiguity can be in lexical and syntactic forms.\n",
    "\n",
    "- In lexical ambiguity, a single word has two or more possible meanings. For example, \"I saw bats\".\n",
    "- In syntactic ambiguity, a single sentence or a sequence of words have multiple possible meanings. For example, \"The chicken is ready to eat\".\n",
    "\n",
    "This session will help you understand the basic and advanced NLP concepts and show you how to implement using the most advanced and popular NLP libraries – <a href=\"https://www.nltk.org/\">NLTK</a>, <a href=\"https://spacy.io/\">spaCy</a>, <a href=\"https://radimrehurek.com/gensim/\">Gensim</a>, and <a href=\"https://huggingface.co/\">Hugging Face</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "<b>In this session</b>, \n",
    "\n",
    "you will learn about basics for the Natural Language Processing. In subsession **6.1**, **6.2**, we will show .. **6.3**,... Finally, in subsession **6.4**, we will compare these libraries and talk about the challanges and data privacy approaches.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Reminder:</b></big>\n",
    "    \n",
    "ONLY use pip unless there is no Conda option. Please make sure that ALL paclages we need are installed in \n",
    "<a href=\"https://github.com/gesiscss/css_methods_python/blob/main/a_introduction/1_computing_environment.ipynb\"> Session 1 </a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfg6soCx1CND"
   },
   "source": [
    "### NLTK (Natural Language Toolkit)\n",
    "\n",
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum. For more details, check out <a href=\"https://www.nltk.org/\">NLTK</a>'s webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "\n",
    "spaCy is the most trending and advanced free open-source library for implementing NLP in Python today. It has distinct features that provide clear advantage for processing text data and modeling such as name entity recognition (NER), part-of-speech (POS) tagging, dependency parsing, word vectors and more. For more details, check out <a href=\"https://spacy.io/\">spaCy</a>'s webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy and create nlp object with loading the models and data for English Language\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') # if you get error, run python -m spacy download en on your Anaconda prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensim\n",
    "\n",
    "Gensim is a free open-source Python library for representing documents as semantic vectors, as efficiently (computer-wise) as possible. It was developed for topic modelling, which supports the NLP tasks like Word Embedding, text summarization and many others, such as <a href=\"https://radimrehurek.com/gensim/models/ldamodel.html\">LDA Topic Modeling</a> and <a href=\"https://radimrehurek.com/gensim/models/phrases.html\">Bigrams/Trigrams</a>. For more details, check out  <a href=\"https://radimrehurek.com/gensim/auto_examples/index.html#documentation\">Gensim</a>'s webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Transformers\n",
    "\n",
    "Transformers was developed by <a href=\"https://huggingface.co/\">Hugging Face</a> and provides state of the art models. It is an advanced library known for the transformer modules with high-level NLP tasks. Hugging Face is one of the most widely used libraries in NLP community. It provides native support for PyTorch and Tensorflow-based models, increasing its applicability in the deep learning community. <a href=\"https://arxiv.org/abs/1810.04805\">BERT</a>  and <a href=\"https://arxiv.org/abs/1907.11692\">RoBERTa</a> are two of the most valuable models supplied by the Hugging Face library, which is used for machine translation, question/answer activities, and many other applications. \n",
    "\n",
    "Hugging Face pipeline provides a rapid and simple approach to perform a range of NLP operations, and the Hugging Face library also supports GPUs for training. As a result, processing speeds are multiplied by a factor of ten. Check out their <a href=\"https://huggingface.co/docs/transformers/main_classes/pipelines\">Pipelines</a> for what 10+ tasks we can perform as one-liners basically. Their model repository is vast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Reminder:</b></big>\n",
    "    \n",
    "Please install the package if you haven't done so, uncomment the code line below:\n",
    "    \n",
    "!pip install transformers check out for conda in <a href=\"https://github.com/gesiscss/css_methods_python/blob/main/a_introduction/1_computing_environment.ipynb\"> Session 1 </a>.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QkINdLLSGe5"
   },
   "source": [
    "# 6.2. Text Preprocessing methods\n",
    "\n",
    "To refer to the entire collection of documents/observations, we use the word corpus (plural corpora). The raw text data often referred to as *text corpus* has punctuations, suffices, and stop words that do not give us important information. To have more useful information for NLP tasks, Text Preprocessing involves preparing the text corpus.\n",
    "\n",
    "Let's walk through some basic steps of preprocessing of a raw text corpus!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Haiko</b></big>\n",
    "\n",
    "So far the session is very technical. But what is the teaching goal beyond coding? Could we introduce \"language models\" here as a larger concept that allows us to structure content here? Wikipedia lists four main models(https://en.wikipedia.org/wiki/Language_model). How do the simple text processing techniques we discuss here connect to the actual models (e.g., markoc models); how do bag of words techniques fit in; what's the relationship to those learned representation techniques? This session should provide the answers.\n",
    "\n",
    "We could structure the session around Hovy's section 8: start with language models and proceed with the necessary steps in that context (following the logic of his subsections).\n",
    "\n",
    "This session could apply an n-gram model to a large corpus and discuss the meaning of the n-grams.\n",
    "\n",
    "This session should also prepare the ground for the later session on supervised text mining. We could ask ourselves \"what is needed later on?\" and make the connection...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A text can be converted into nlp object of spaCy as it was shown in the earlier step in spaCy. First, we should convert raw text into a spaCy object. However, to do that, we need to have our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = 'Today is a great day with learning NLP, such a powerful tool!'\n",
    "text_doc = nlp(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "I might remove all toy examples and continue with the reviews from Amazon. What do you think?\"\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download publicly available Amazon reviews data from He, R., & McAuley, J. (2016): http://jmcauley.ucsd.edu/data/amazon/index_2014.html\n",
    "\n",
    "The dataset that we will work on is already downloaded for this Session in data folder, filename: \"Industrial_and_Scientific_5.json.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neccessary libraries and check out the Session 1 for further installation information\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with gzip.open('./data/Industrial_and_Scientific_5.json.gz') as f:\n",
    "    for l in f:\n",
    "        data.append(json.loads(l.strip()))\n",
    "    \n",
    "# total length of list, this number equals total number of products\n",
    "print(len(data))\n",
    "\n",
    "# first row of the list\n",
    "print(data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list into pandas dataframe\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first five rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Reminder:</b></big>\n",
    "    \n",
    "If you need more time to learn about pandas, please get back to <a href=\"https://github.com/gesiscss/css_methods_python/blob/main/a_introduction/2_data_handling_and_visualization.ipynb\"> Session 2 </a>.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYJ0Y4OAj2gq"
   },
   "source": [
    "## 6.2.1. Word Descriptors\n",
    "\n",
    "### Tokens and splitting \n",
    "\n",
    "#### What is a Token?\n",
    "\n",
    "The set of all the unique terms in our data is called the vocabulary. Each element in this set is called a type. Each occurrence of a type in the data is called a token. \n",
    "\n",
    "Let's practice: Our sentence “Today is a great day with learning NLP, such a power tool!”, has 14 tokens but only 13 types (namely, 'Today', 'is', 'a', 'great', 'day', 'with', 'learning', 'NLP', ',', 'such', 'a', 'powerful', 'tool', '!'). Note that types can also include punctuation marks and multiword expressions.\n",
    "\n",
    "In other words, the words of a text document/file separated by spaces and punctuation are called as tokens.\n",
    "\n",
    "#### What is a Tokenization?\n",
    "The process of extracting tokens from a text file/document is referred as tokenization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can print text of the tokens by accessing token.text while using spaCy\n",
    "# printing tokens\n",
    "tokens = []\n",
    "for token in text_doc:\n",
    "    print(token.text)\n",
    "    tokens.append(token.text)\n",
    "    \n",
    "len(tokens)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we want to find a particular token with alphabetic characters?\n",
    "\n",
    "for token in text_doc:\n",
    "    print(token.text, token.is_alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we want to know if the particular token is space, or a stop word or punctuation?\n",
    "print(\"Text\".ljust(10), ' ', \"Alpha\", \"Space\", \"Stop\", \"Punct\")\n",
    "for token in text_doc:\n",
    "    print(token.text.ljust(10), ':', token.is_alpha, token.is_space, token.is_stop, token.is_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try tokenization with nltk.\n",
    "nltk doesn't come fully installed, you need to use nltk.download() to use some of the missing functions, and we will also use spaCy to show similar features that these library have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jAcPU658j67u",
    "outputId": "9b0cccfc-f7f9-4d29-c10e-b09a23b066da"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "sentence = \"At 10:30 o'clock on Monday mornings, we have Social ComQuant meetings. Let's have our meeting another time.\"\n",
    "tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "print(tokens)\n",
    "print()\n",
    "print(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "lLQWxbNM6Ldw",
    "outputId": "3759aeb6-30f7-4209-b9dd-7ecba72153a7"
   },
   "outputs": [],
   "source": [
    "# convert string to upper case characters\n",
    "sentence.upper() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ZMlnCTpP6To_",
    "outputId": "e33e6074-0bb1-4776-fb03-455e2e9c35c8"
   },
   "outputs": [],
   "source": [
    "# convert string to lower case characters\n",
    "sentence.lower() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvMwNW877fZj"
   },
   "source": [
    "#### Let's remove punctuation and stop words. But, wait, what do we mean by stop words?\n",
    "Stop words are a set of commonly used words in any language. For example, in English, “the”, “is”, and “and” would easily qualify as stop words. In NLP and text mining applications, stop words are used to eliminate unimportant words, allowing applications to focus on the important words instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dRBUQU44rIQN",
    "outputId": "a1edf0b6-0b55-4e91-ffa9-16534add66e1"
   },
   "outputs": [],
   "source": [
    "# Access the built-in stop words in nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the built-in stop words in Spacy\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "list_stopwords=list(stopwords)\n",
    "\n",
    "# printing a fraction of the list through indexing\n",
    "for word in list_stopwords[:5]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfa4ueiY7eWK",
    "outputId": "946a1cb7-1543-46d2-f4ee-b4df22db949b"
   },
   "outputs": [],
   "source": [
    "# Filter out the stopwords\n",
    "filtered_text= [token for token in text_doc if not token.is_stop]\n",
    "\n",
    "# Count the tokens after removal of stopwords\n",
    "token_count_without_stopwords=0\n",
    "for token in filtered_text:\n",
    "    print(token)\n",
    "    token_count_without_stopwords+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations\n",
    "filtered_text=[token for token in filtered_text if not token.is_punct]\n",
    "\n",
    "token_count_without_stop_and_punct=0\n",
    "\n",
    "# Counting the new no of tokens\n",
    "for token in filtered_text:\n",
    "    print(token)\n",
    "    token_count_without_stop_and_punct += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fxyRQ7X399J"
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look up a word in a dictionary, we usually just look for the base form. This dictionary base form is called the lemma.\n",
    "For instance, we might see forms like “go,” “goes,” “went,”, “gone,” or “going” and we look up dictionary in a lemmatized form, such as \"go\" (Hovy, 2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rXdJ76j4Byh",
    "outputId": "ae40be62-74a3-48b4-ad2a-0a587adaddd0"
   },
   "outputs": [],
   "source": [
    "# Let's give an example with nltk\n",
    "# import nltk already imported\n",
    "# import the lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Remember our sentence with the Social ComQuant meeting.\n",
    "sentence = \"At 10:30 o'clock on Monday mornings, we have Social ComQuant meetings. Let's have our meeting another time.\"\n",
    "\n",
    "WNL = WordNetLemmatizer() # declaring an instance of our preprocessor.\n",
    "tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "\n",
    "lemmatized_tokens = []\n",
    "for t in tokens:\n",
    "    t_lemma = WNL.lemmatize(t)\n",
    "    lemmatized_tokens.append(t_lemma)\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Haiko</b></big>\n",
    "\n",
    "At this point it occurrs to me that just providing the commands how things can be done is not enough. We also want to teach how users can proproces their corpus and save intermediate steps like \"corpus.txt\" > \"corpus_stemmed.txt\" > \"corpus_stemmed_nostopwords.txt\" > ...\n",
    "\n",
    "In general, do we need spacy, nltk, and gensim to work along this pipeline? Even if not, we should tell in the session why we introduce all packages.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's give an example with spaCy\n",
    "new_sentence = \"What about going to festivals? You said you like dancing.\"\n",
    "\n",
    "text_doc = nlp(new_sentence)\n",
    "for token in text_doc:\n",
    "    print(token.text, '----->', token.lemma_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmSoE-YBnDek"
   },
   "source": [
    "### Stemming \n",
    "\n",
    "From Hovy's book: \"Rather than reducing a word to the lemma, we strip away everything but the irreducible morphological core (the stem). For example, for a word like “anticonstitutionalism”, which can be analyzed as “anti+constitut+ion+al+ism,” we remove everything but “constitut.” The most famous and commonly used stemming tool is based on the algorithm developed by Porter (1980). For each language, it defines a number of suffixes (i.e., word endings) and the order in which they should be removed or replaced. By repeatedly applying these actions, we reduce all words to their stems. In our example, all words derive from the stem “constitut–” by attaching different endings. Again, a version of the Porter stemmer is already available in Python, in the nltk library (Loper & Bird, 2002), but we have to specify the language.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G8NdEilRnC8i",
    "outputId": "817f4d81-b30a-41d6-e9ed-e7bcdd187467"
   },
   "outputs": [],
   "source": [
    "# import stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "sentence = \"\"\"Every weekday evening, our editors guide you through the biggest stories of the day,\n",
    "help you discover new ideas, and surprise you with moments of delight. Subscribe to get this delivered to your inbox..\"\"\"\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "\n",
    "stemmed_tokens = []\n",
    "# for loop for each token in stemmed_tokens\n",
    "for token in tokens:\n",
    "    # add the stemmed version of the token to the new list\n",
    "    stemmed_tokens.append(stemmer.stem(token))\n",
    "# join a list of tokens into one string\n",
    "stemmed_sentence = \" \".join(stemmed_tokens) \n",
    "print(stemmed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://docs.python.org/3/howto/regex.html\">Regular expressions</a> (called regex, regexes, regex pattern, regexp, or REs) specify search patterns. Typical examples of regular expressions are the patterns for matching email addresses, phone numbers, and credit card numbers.\n",
    "\n",
    "Regular expressions are essentially a specialized programming language embedded in Python, and you can interact with regular expressions via the built-in `re` module in Python, which has some functions that match a string for a pattern:\n",
    "\n",
    "- `match()`\n",
    "- `search()`\n",
    "- `findall()`\n",
    "- `finditer()`\n",
    "\n",
    "Pattern... character set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import PyPDF2\n",
    "from tika import parser # needs to be imported and note that Tika is written in Java so you will need a Java runtime installed\n",
    "import re\n",
    "import pprint\n",
    "# read the speech data\n",
    "raw = parser.from_file(\"data/king_dreamspeech.pdf\")\n",
    "\n",
    "# remove spaces backslashes\n",
    "text_corpus = raw['content'].replace(\"\\\\\", \"\").lower()\n",
    "\n",
    "search_keywords = ['but', 'because', 'while', 'as']\n",
    "\n",
    "sentences = text_corpus.split('\\n')\n",
    "\n",
    "# or with spaCy\n",
    "\n",
    "for keyword in search_keywords:\n",
    "    matches = [s for s in sentences if re.search(r'\\b' + keyword + r'\\b', s)]\n",
    "    print(matches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency analysis\n",
    "\n",
    "<div class='alert-info'>\n",
    "<big><b>Haiko</b></big>\n",
    "\n",
    "Yes, I think it should be here. Hovy has it in the \"text representation\" section. It is then a statistic of the document-term matrix for example.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, pprint # if you haven't done so\n",
    "\n",
    "# tokenize documents with gensim's tokenize() function\n",
    "tokens = [list(gensim.utils.tokenize(doc, lower=True)) for doc in sentences]\n",
    "\n",
    "# build bigram model\n",
    "bigram_mdl = gensim.models.phrases.Phrases(tokens, min_count=1, threshold=2)\n",
    "\n",
    "# do more pre-processing on tokens (remove stopwords, stemming etc.)\n",
    "# NOTE: this can be done better\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, stem_text\n",
    "CUSTOM_FILTERS = [remove_stopwords, stem_text]\n",
    "tokens = [preprocess_string(\" \".join(doc), CUSTOM_FILTERS) for doc in tokens]\n",
    "\n",
    "# apply bigram model on tokens\n",
    "bigrams = bigram_mdl[tokens]\n",
    "\n",
    "pprint.pprint(list(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in bigrams:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in bigrams]\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(dictionary.token2id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "\n",
    "    \n",
    "Add here freq visualizations, word count --> dictionary based, freqs (Haiko's suggested paper on uncertainity)\n",
    "principle behind the algorithms, create your own dictionary (25 words, cite the paper) extracting URLs, hashtags and emojis can come after that\n",
    "\n",
    "tweets for emojis\n",
    "    \n",
    "over time analysis?\n",
    "\n",
    "multi corpus for tfidf\"\n",
    "<div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2. Parts of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.3. Named entities\n",
    "NER with NLTK\n",
    "\n",
    "NER with spaCy\n",
    "\n",
    "<img src='images/NER.png' style='height: 500px; float: left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3. NLP tasks implementation\n",
    "\n",
    "Text summarization (gensim + spaCy)\n",
    "Text similarity cosine similarity\n",
    "\n",
    "Use Amazon reviews: https://nijianmo.github.io/amazon/index.html 2018\n",
    "\n",
    "\n",
    "Justifying recommendations using distantly-labeled reviews and fined-grained aspects\n",
    "Jianmo Ni, Jiacheng Li, Julian McAuley\n",
    "Empirical Methods in Natural Language Processing (EMNLP), 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "\n",
    "To session 10? Huggingface’s transformers: State-of-the-art NLP\n",
    "Currently, Hugging face is supported by Pytorch and tensorflow 2.0. We can use transformers of Hugging Face to implement Summarization, Text Generation, Language Translation, ChatBot...\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Haiko</b></big>\n",
    "\n",
    "pytorch is available in Anaconda, torch is not. Can we use pytorch as we have the policy that we prioritize packages available in Anaconda?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you haven't installed yet, please install torch first\n",
    "# !pip install torch\n",
    "import torch\n",
    "# !pip install transformers --upgrade\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Hovy, D. (2020). Text analysis in Python for social scientists: Discovery and exploration. Cambridge University Press.\n",
    "\n",
    "Hovy, D. (2021). Text Analysis in Python for Social Scientists: Prediction and Classification. Cambridge University Press.\n",
    "\n",
    "Vajjala, S., Majumder, B., Gupta, A., & Surana, H. (2020). Practical natural language processing: a comprehensive guide to building real-world NLP systems. O'Reilly Media.\n",
    "\n",
    "Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
    "\n",
    "Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\n",
    "\n",
    "He, R., & McAuley, J. (2016, April). Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In proceedings of the 25th international conference on world wide web (pp. 507-517).: http://jmcauley.ucsd.edu/data/amazon/index_2014.html\n",
    "\n",
    "\n",
    "https://www.nltk.org/book/ch01.html\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/natural-language-processing-guide/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Credits\n",
    "\n",
    "[1] Vajjala, S., Majumder, B., Gupta, A., & Surana, H. (2020). Practical natural language processing: a comprehensive guide to building real-world NLP systems. O'Reilly Media. Chapter 1: https://www.oreilly.com/library/view/practical-natural-language/9781492054047/ch01.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "<b>Document information</b>\n",
    "\n",
    "Contact and main author: N. Gizem Bacaksizlar Turbic & ..?\n",
    "\n",
    "Contributors: Haiko Lietz & Pouria Mirelmi & ..?\n",
    "\n",
    "Acknowledgements: ...\n",
    "\n",
    "Version date: XX. December 2022\n",
    "\n",
    "License: ...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Z0-GgEFeNJbn"
   ],
   "name": "Day 1: Introduction to Jupyter Notebook and Text Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
