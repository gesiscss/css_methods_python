{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26acf247",
   "metadata": {},
   "source": [
    "<img src='images/gesis.png' style='height: 60px; float: left'>\n",
    "<img src='images/social_comquant.png' style='height: 50px; float: left; margin-left: 40px'>\n",
    "<img src='images/isi.png' style='height: 50px; float: left; margin-left: 20px'>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c8be6",
   "metadata": {},
   "source": [
    "Authors = N. Gizem Bacaksizlar Turbic and Haiko Lietz\n",
    "\n",
    "Date = 19 July 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd4ea4a",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Data collection is a procedure of gathering information from subjects (all relevant sources), measuring and analyzing accurate insights for research using various techniques. Researchers can evaluate their research questions and hypotheses on the basis of collected data. In most cases, data collection is the primary and most important step for research, irrespective of the field of study. The approach of data collection varies for different fields of study, depending on the required information.\n",
    "\n",
    "The ease of access to the technology has made various social media platforms more popular as communication tools, therefore as a source of data. With this rise of social media use as a data source, data collection using APIs has become a demanding skill. Here, in this session, we aim to teach how to collect data from various social media platforms, such as Twitter and Reddit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66abdeec",
   "metadata": {},
   "source": [
    "# 2. Social Media Platforms for Data Harvesting through API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c403de1f",
   "metadata": {},
   "source": [
    "<img src=\"./images/database.png\"  width=\"150\" height = \"150\" align=\"right\"/>\n",
    "\n",
    "In order to access APIs, you first need to create an account and apply to have a developer account on the platform that you want to work on. With this developer account, platforms provide you KEYS (e.g., secret, public, or access) to authenticate their system.\n",
    "\n",
    "While web scraping is one of the common ways of collecting data from websites, a lot of websites offer APIs to access the public data that they host on their website. This is to avoid unnecessary traffic on the websites.\n",
    "\n",
    "However, even though we have access to these API, as researchers, we should not forget to respect API access rules and always read the documents before collecting data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09370b8",
   "metadata": {},
   "source": [
    "## 2.1. A demonstration using Python to collect data from Twitter \n",
    "\n",
    "Twitter is one of the most used social media platforms in the academic research. This microblogging and social networking service host users who can post and interact with messages known as \"tweets\". Registered users can post, like, and retweet tweets, but unregistered users can only read those that are publicly available. As of 2022, Twitter has 436 million active users worldwide (Statista, 2022*). \n",
    "\n",
    "<img src=\"./images/twitter.png\"  width=\"200\" height = \"200\" align=\"left\"/>\n",
    "\n",
    "Different access options for different purposes:\n",
    "\n",
    "- Twitter Developer: https://developer.twitter.com/\n",
    "- APIs: https://developer.twitter.com/en/docs\n",
    "- GNIP: http://support.gnip.com/apis/\n",
    "- Twitter Enterprise: https://developer.twitter.com/en/enterprise\n",
    "\n",
    "IMPORTANT to note that free APIs cover 7 days Tweets; Premium APIs exist for 30-day search and beyond. If you have an Academic Research access level, you can access even more data with full-archive search endpoint. There are changes to APIs policies over time, such as functionalities and user agreements. Also, limitations on volume and functions should be considered. \n",
    "\n",
    "Before we start with our first project on Twitter, first you need to sign up for Twitter and then, create a Developer account: \n",
    "\n",
    "- Sign up from [here.](https://help.twitter.com/en/using-twitter/create-twitter-account)\n",
    "- Create a Developer Account from [here.](https://developer.twitter.com)\n",
    "\n",
    "\n",
    "**https://www.statista.com/statistics/272014/global-social-networks-ranked-by-number-of-users/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "193473b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Let's get started with our first project of colleting Tweets.\n",
    "    Import libraries if you install them before.\n",
    "    If you have not installed them, then install with pip on your command prompt or your jupyter notebook with !pip.\n",
    "'''\n",
    "# import relevant packages\n",
    "import pandas as pd # data manipulation library\n",
    "import datetime # human readable date formats\n",
    "import tweepy as tw # wrapper around Twitter API \n",
    "# Please make sure you have installed all of these libraries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e4f9a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your keys registered with Twitter\n",
    "# Obtain the access token and access token secret\n",
    "# These can be generated in your Developer Portal, under the “Keys and tokens” tab for your Developer App.\n",
    "\n",
    "apikey = 'YOURapikey' #25 alphanumeric characters\n",
    "apisecretkey = 'YOURapisecretkey'\n",
    "accesstoken = 'YOURaccesstoken'\n",
    "accesstokensecret = 'YOURaccesstokensecret'\n",
    "bearertoken = 'YOURbearertoken'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39538a76",
   "metadata": {},
   "source": [
    "<img src=\"./images/developer_portal.png\"  width=\"500\" height = \"500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b7068dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say you are sharing your scripts with others, and do not want to show your keys. What can you do?\n",
    "# We first can create a simple Python script called keys.py in which we store all passwords. \n",
    "# Save this script in the same folder as this notebook and import your keys\n",
    "\n",
    "from keys import *\n",
    "\n",
    "# Make sure you name your variable names for the keys in the keys.py script are the same as your variables here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff9e184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your access with search terms\n",
    "auth = tw.OAuthHandler(apikey, apisecretkey)\n",
    "auth.set_access_token(accesstoken, accesstokensecret)\n",
    "api = tw.API(auth, wait_on_rate_limit = True)\n",
    "search_words = \"ComputationalSocialScience OR GESIS OR SocialComQuant\" # Words should be changed according to your search.\n",
    "# If you want to remove retweets, then include -filter:retweets to the search_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e7fc12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Tweets and be aware of the attribute names from the new version of the packages, which may change in time\n",
    "# For a standart search, we use Tweepy.\n",
    "tweets = tw.Cursor(api.search_tweets,  q=search_words, lang=\"en\").items() # Possible to limit the number of search items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06363e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         user_name              user_id             tweet_id  \\\n",
      "0      CompCommLab   798181898031943680  1519270689823498241   \n",
      "1         emcr_sna  1491847980617547779  1519231234358095872   \n",
      "2        gesis_org            145554242  1519224965555535873   \n",
      "3  ReligionFESTHD1  1186731643505184768  1519220526677467136   \n",
      "4        gesis_org            145554242  1519211980896317440   \n",
      "\n",
      "                 tweet_date  \\\n",
      "0 2022-04-27 11:02:13+00:00   \n",
      "1 2022-04-27 08:25:26+00:00   \n",
      "2 2022-04-27 08:00:32+00:00   \n",
      "3 2022-04-27 07:42:53+00:00   \n",
      "4 2022-04-27 07:08:56+00:00   \n",
      "\n",
      "                                               tweet  \\\n",
      "0  RT @clauwa: Want to join @gesis_org and @HHU_d...   \n",
      "1  RT @clauwa: Want to join @gesis_org and @HHU_d...   \n",
      "2  Lights! Camera! Action! Teach! A #Handbook for...   \n",
      "3  RT @gesis_org: #stellenangebot #job #openposit...   \n",
      "4  RT @trovdimi: In case you missed my talk on Ge...   \n",
      "\n",
      "                                          user_image            user_location  \n",
      "0  http://pbs.twimg.com/profile_images/8390499541...          Vienna, Austria  \n",
      "1  http://pbs.twimg.com/profile_images/1496253854...                           \n",
      "2  http://pbs.twimg.com/profile_images/2840291739...                 Mannheim  \n",
      "3  http://pbs.twimg.com/profile_images/1471199487...  Heidelberg, Deutschland  \n",
      "4  http://pbs.twimg.com/profile_images/2840291739...                 Mannheim  \n",
      "---------------------------------------------\n",
      "The length of the dataframe: 259\n"
     ]
    }
   ],
   "source": [
    "# First, check which Twitter attributes you collected from this search, visit:\n",
    "# https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet\n",
    "# Then, create a dataframe with the columns you might need for your analysis\n",
    "\n",
    "tweet_details = [[ tweet.user.screen_name, tweet.user.id, tweet.id_str, \n",
    "                  tweet.created_at, tweet.text, tweet.user.profile_image_url, tweet.user.location] \n",
    "                  for tweet in tweets]\n",
    "tweet_df = pd.DataFrame(data=tweet_details, \n",
    "                        columns = [\"user_name\",\"user_id\", \"tweet_id\", \"tweet_date\",\"tweet\",\"user_image\",\n",
    "                                   \"user_location\"])\n",
    "\n",
    "# For instance, you can see the values of one specific column with a code like this: tweet_df['user_image'].values\n",
    "\n",
    "# Save df\n",
    "tweet_df.to_csv(\"./data/test_tweets.csv\", index = False)\n",
    "print(tweet_df.head())\n",
    "print('---------------------------------------------')\n",
    "\n",
    "# print the length of the dataset\n",
    "print('The length of the dataframe:', len(tweet_df['tweet_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57270ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['http://pbs.twimg.com/profile_images/839049954144436225/iZvx4Nbr_normal.jpg',\n",
       "       'http://pbs.twimg.com/profile_images/1496253854937141257/J4Xdl0YN_normal.jpg',\n",
       "       'http://pbs.twimg.com/profile_images/2840291739/926f900a36e46987ff8ac10c060f2c07_normal.png',\n",
       "       'http://pbs.twimg.com/profile_images/1471199487288913922/wcvkmu9V_normal.jpg',\n",
       "       'http://pbs.twimg.com/profile_images/2840291739/926f900a36e46987ff8ac10c060f2c07_normal.png'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the first five user images with searching the link on the browser\n",
    "tweet_df.user_image.values[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f49e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API v2 (if you have a full access)\n",
    "client = tw.Client(bearer_token=bearer_token)\n",
    "\n",
    "# Replace with your own search query\n",
    "query = 'from:SocialComquant -is:retweet' # you can change from with your own choice of username (without retweets)\n",
    "\n",
    "# Replace with time period of your choice\n",
    "start_time = '2021-01-01T00:00:00Z'\n",
    "\n",
    "# Replace with time period of your choice\n",
    "end_time = '2022-01-01T00:00:00Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e314ad13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-01-01T00:00:00Z'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the start_time by yourself with writing\n",
    "start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "175eb30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# You can search Tweets from the last 7 days or all Tweets with different functions. Check available functions in Tweepy!\n",
    "Tweepy: https://docs.tweepy.org/en/stable/client.html#search-tweets\n",
    "# A helpful link for setting up your query: \n",
    "https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research/blob/main/modules/5-how-to-write-search-queries.md\n",
    "'''\n",
    "# Connect to Twitter API and search all tweets if you have a full academic access\n",
    "tweets = client.search_all_tweets(query=query, tweet_fields=['created_at','text', 'context_annotations','entities'],\n",
    "                                  start_time=start_time,\n",
    "                                  end_time=end_time, max_results=10) #set your max results between 10 and 500\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09368050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-08 10:26:14+00:00\n",
      "[{'domain': {'id': '30', 'name': 'Entities [Entity Service]', 'description': 'Entity Service top level domain, every item that is in Entity Service should be in this domain'}, 'entity': {'id': '848920371311001600', 'name': 'Technology', 'description': 'Technology and computing'}}, {'domain': {'id': '66', 'name': 'Interests and Hobbies Category', 'description': 'A grouping of interests and hobbies entities, like Novelty Food or Destinations'}, 'entity': {'id': '848921413196984320', 'name': 'Computer programming', 'description': 'Computer programming'}}, {'domain': {'id': '66', 'name': 'Interests and Hobbies Category', 'description': 'A grouping of interests and hobbies entities, like Novelty Food or Destinations'}, 'entity': {'id': '898673391980261376', 'name': 'Web development', 'description': 'Web Development'}}]\n",
      "2021-11-24 14:10:43+00:00\n",
      "[{'domain': {'id': '30', 'name': 'Entities [Entity Service]', 'description': 'Entity Service top level domain, every item that is in Entity Service should be in this domain'}, 'entity': {'id': '848920371311001600', 'name': 'Technology', 'description': 'Technology and computing'}}, {'domain': {'id': '66', 'name': 'Interests and Hobbies Category', 'description': 'A grouping of interests and hobbies entities, like Novelty Food or Destinations'}, 'entity': {'id': '848921413196984320', 'name': 'Computer programming', 'description': 'Computer programming'}}, {'domain': {'id': '66', 'name': 'Interests and Hobbies Category', 'description': 'A grouping of interests and hobbies entities, like Novelty Food or Destinations'}, 'entity': {'id': '898673391980261376', 'name': 'Web development', 'description': 'Web Development'}}]\n",
      "2021-10-08 14:07:43+00:00\n",
      "[]\n",
      "2021-09-08 13:35:19+00:00\n",
      "[]\n",
      "2021-09-08 13:32:39+00:00\n",
      "[{'domain': {'id': '30', 'name': 'Entities [Entity Service]', 'description': 'Entity Service top level domain, every item that is in Entity Service should be in this domain'}, 'entity': {'id': '848920371311001600', 'name': 'Technology', 'description': 'Technology and computing'}}, {'domain': {'id': '66', 'name': 'Interests and Hobbies Category', 'description': 'A grouping of interests and hobbies entities, like Novelty Food or Destinations'}, 'entity': {'id': '848921413196984320', 'name': 'Computer programming', 'description': 'Computer programming'}}]\n",
      "2021-09-08 13:31:04+00:00\n",
      "[{'domain': {'id': '46', 'name': 'Brand Category', 'description': 'Categories within Brand Verticals that narrow down the scope of Brands'}, 'entity': {'id': '781974596752842752', 'name': 'Services'}}, {'domain': {'id': '48', 'name': 'Product', 'description': 'Products created by Brands.  Examples: Ford Explorer, Apple iPhone.'}, 'entity': {'id': '1412579054855671809', 'name': 'Google Innovation'}}, {'domain': {'id': '30', 'name': 'Entities [Entity Service]', 'description': 'Entity Service top level domain, every item that is in Entity Service should be in this domain'}, 'entity': {'id': '854692455005921281', 'name': 'Science', 'description': 'Science'}}]\n",
      "2021-07-05 11:00:38+00:00\n",
      "[{'domain': {'id': '30', 'name': 'Entities [Entity Service]', 'description': 'Entity Service top level domain, every item that is in Entity Service should be in this domain'}, 'entity': {'id': '848920371311001600', 'name': 'Technology', 'description': 'Technology and computing'}}, {'domain': {'id': '30', 'name': 'Entities [Entity Service]', 'description': 'Entity Service top level domain, every item that is in Entity Service should be in this domain'}, 'entity': {'id': '930484568305541120', 'name': 'Data science', 'description': 'Data Science'}}]\n",
      "2021-06-21 10:29:38+00:00\n",
      "[{'domain': {'id': '30', 'name': 'Entities [Entity Service]', 'description': 'Entity Service top level domain, every item that is in Entity Service should be in this domain'}, 'entity': {'id': '848920371311001600', 'name': 'Technology', 'description': 'Technology and computing'}}, {'domain': {'id': '66', 'name': 'Interests and Hobbies Category', 'description': 'A grouping of interests and hobbies entities, like Novelty Food or Destinations'}, 'entity': {'id': '898673391980261376', 'name': 'Web development', 'description': 'Web Development'}}]\n",
      "2021-06-14 10:54:18+00:00\n",
      "[{'domain': {'id': '30', 'name': 'Entities [Entity Service]', 'description': 'Entity Service top level domain, every item that is in Entity Service should be in this domain'}, 'entity': {'id': '848920371311001600', 'name': 'Technology', 'description': 'Technology and computing'}}, {'domain': {'id': '66', 'name': 'Interests and Hobbies Category', 'description': 'A grouping of interests and hobbies entities, like Novelty Food or Destinations'}, 'entity': {'id': '898673391980261376', 'name': 'Web development', 'description': 'Web Development'}}]\n",
      "2021-05-04 12:29:13+00:00\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Let's see a fairly new field for context annotations.\n",
    "for tweet in tweets.data:\n",
    "    print(tweet.created_at)\n",
    "    print(tweet.context_annotations) #context annotations (https://developer.twitter.com/en/docs/twitter-api/annotations/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6182d3a1",
   "metadata": {},
   "source": [
    "## 2.2. A demonstration using Python to collect Reddit comments <img src=\"./images/reddit.svg\"  width=\"150\" height = \"150\" align=\"right\"/>\n",
    "\n",
    "Reddit is one of the oldest social media platforms which is still generating content with its users. Millions of users are creating on a daily basis in the form of questions and comments. Reddit also offers such API which is easy to access this vast amount of data.\n",
    "\n",
    "First thing you need to do is to have a Reddit account. You should create it from [here.](https://www.reddit.com/)\n",
    "- [Official Reddit API](https://www.reddit.com/dev/api/)\n",
    "    - [Collecting Reddit data](https://towardsdatascience.com/scrape-reddit-data-using-python-and-google-bigquery-44180b579892)\n",
    "    \n",
    "Alternative ways of getting Reddit data:\n",
    "- [Google BigQuery](https://cloud.google.com/bigquery) (GBQ)\n",
    "    - [Scraping Reddit data with GBQ](https://towardsdatascience.com/scrape-reddit-data-using-python-and-google-bigquery-44180b579892)\n",
    "- [Pushshift.io](https://medium.com/@RareLoot/using-pushshifts-api-to-extract-reddit-submissions-fb517b286563)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2deb3d30",
   "metadata": {},
   "source": [
    "''' \n",
    "    Let's get started with our first project of colleting data from Reddit.\n",
    "    Import libraries if you install them before.\n",
    "    If you have not installed them, then install with pip on your command prompt or your jupyter notebook with !pip.\n",
    "'''\n",
    "# import relevant packages\n",
    "import pandas as pd # data manipulation library\n",
    "import praw # Python Reddit API Wrapper libray\n",
    "# Please make sure you have installed all of these libraries!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66c761a3",
   "metadata": {},
   "source": [
    "# Enter your keys registered with Reddit\n",
    "# Client_id: your 14 characters personal use script key\n",
    "# Client_secret: your 27 characters secret key\n",
    "# Username and password are your Reddit account credentials\n",
    "reddit = praw.Reddit(user_agent='Comment Extraction (by /u/USERNAME)',\n",
    "                     client_id='**********',client_secret=\"***********\",username='********', password='*******')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e82a004",
   "metadata": {},
   "source": [
    "We need to decide which subreddit you would like to focus on getting the data: Let's say \"Computational Social Science\" and be creative :)\n",
    "\n",
    "title, score, url, id, number of comments, date of creation, body text are the fields that are available from Reddit API. \n",
    "Here, we will focus on getting the bodytext(comments) from the subreddit. Refer to [praw documentation](https://praw.readthedocs.io/en/latest/code_overview/models/subreddit.html) for different kinds of implementations. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca12bd79",
   "metadata": {},
   "source": [
    "comm_list = [] # empty list for the comments\n",
    "header_list = [] # empty list for header_list\n",
    "i = 0\n",
    "for submission in reddit.subreddit('Computational Social Science').hot(limit=2): # .hot(limit) can be any number\n",
    "    submission.comments.replace_more(limit=None) # Getting the list of comment ids\n",
    "    comment_queue = submission.comments[:]  # Seed with top-level\n",
    "    while comment_queue:\n",
    "        header_list.append(submission.title)\n",
    "        comment = comment_queue.pop(0)\n",
    "        comm_list.append(comment.body)\n",
    "        t = []\n",
    "        t.extend(comment.replies)\n",
    "        while t: # If the comment has nested replies, it will enter into the next loop and will extract information.\n",
    "            header_list.append(submission.title)\n",
    "            reply = t.pop(0)\n",
    "            comm_list.append(reply.body)\n",
    "            \n",
    "df = pd.DataFrame(header_list)\n",
    "df['comm_list'] = comm_list\n",
    "df.columns = ['header','comments']\n",
    "df['comments'] = df['comments'].apply(lambda x : x.replace('\\n',''))\n",
    "df.to_csv('CSS_comments.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c76de5",
   "metadata": {},
   "source": [
    "# 2.3. More APIs and precollected datasets \n",
    "\n",
    "<img src=\"./images/datasets.jpg\" width=\"500\" height = \"900\" align=\"left\"/>  \n",
    "\n",
    "- __More APIs__\n",
    "\n",
    "    [Facebook for Developers](https://developers.facebook.com/)  \n",
    "    [Facebook Ads API](https://developers.facebook.com/docs/marketing-apis/)  \n",
    "    [Instagram Developer](https://developers.facebook.com/docs/instagram-basic-display-api)  \n",
    "    [YouTube Developers](https://developers.google.com/youtube/)  \n",
    "    [Weibo API](http://open.weibo.com/wiki/API%E6%96%87%E6%A1%A3/en)  \n",
    "    [CrowdTangle](https://www.crowdtangle.com/request)  \n",
    "    [4chan](https://github.com/4chan/4chan-API)  \n",
    "    [Gab](https://github.com/a-tal/gab)  \n",
    "    [Github REST API](https://docs.github.com/en/rest)  \n",
    "    [Github GraphQL](https://docs.github.com/en/graphql)  \n",
    "    [Stackoverflow](https://api.stackexchange.com/docs)  \n",
    "    [Facepager](https://github.com/strohne/Facepager)  \n",
    "\n",
    "\n",
    "- __Precollected datasets__  \n",
    "    https://datasetsearch.research.google.com  \n",
    "    https://www.kaggle.com/datasets  \n",
    "    https://data.gesis.org/sharing/#!Search  \n",
    "\n",
    "\n",
    "- __Locating or Requesting Social Media Data__\n",
    "    https://www.programmableweb.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425df720",
   "metadata": {},
   "source": [
    "# 3. Challanges\n",
    "\n",
    "Facebook completely closed down many of it’s APIs and it is not very hard to get Facebook data besides CrowdTangle or FB Ads.\n",
    "\n",
    "Twitter’s API now has the version 2 with substantial changes. \n",
    "\n",
    "These challanges make us stay vigilant and continuously update our code to keep up with the APIs.\n",
    "\n",
    "- More on Social Media data collection and data quality:\n",
    "https://www.slideshare.net/suchprettyeyes/working-with-socialmedia-data-ethics-good-practice-around-collecting-using-and-storing-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe29c81",
   "metadata": {},
   "source": [
    "# 4. References\n",
    "\n",
    "Zenk-Möltgen, Wolfgang (GESIS - Leibniz Institute for the Social Sciences), Python Script to rehydrate Tweets from Tweet IDs https://doi.org/10.7802/1504\n",
    "\n",
    "Pfeffer, Morstatter (2016): Geotagged Twitter posts from the United States: A tweet collection to investigate representativeness. Dataset. http://dx.doi.org/10.7802/1166\n",
    "\n",
    "Do not miss checking out the Social Comquant Workshop 10 at:https://github.com/strohne/autocol\n",
    "\n",
    "- Useful links for getting started with Twitter API v2\n",
    "    - [Comprehensive Guide for Using the Twitter API v2](https://dev.to/twitterdev/a-comprehensive-guide-for-using-the-twitter-api-v2-using-tweepy-in-python-15d9#:~:text=Tweepy%20is%20a%20popular%20package,the%20academic%20research%20product%20track)\n",
    "    - [Step by Step Guide to Making Your First Request to the Twitter API v2](https://developer.twitter.com/en/docs/tutorials/step-by-step-guide-to-making-your-first-request-to-the-twitter-api-v2)\n",
    "    - [Getting Started with Data Collection Using Twitter API v2](https://towardsdatascience.com/getting-started-with-data-collection-using-twitter-api-v2-in-less-than-an-hour-600fbd5b5558#39c4)\n",
    "    - [An Extensive Guide to Collecting Tweets from Twitter API v2 for Academic REsearch Using Python 3](https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a)\n",
    "    - [What Pythong package is best for getting data from Twitter](https://towardsdatascience.com/what-python-package-is-best-for-getting-data-from-twitter-comparing-tweepy-and-twint-f481005eccc9)\n",
    "\n",
    "- Useful links for getting started with Reddit API\n",
    "    - https://www.reddit.com/r/TheoryOfReddit/wiki/collecting_data/- \n",
    "    - https://towardsdatascience.com/scrape-reddit-data-using-python-and-google-bigquery-44180b579892\n",
    "    - https://github.com/akhilesh-reddy/Cable-cord-cutter-Sentiment-analysis-using-Reddit-data\n",
    "    \n",
    "<a href=\"https://www.flaticon.com/free-icons/database\" title=\"database icons\">Database icons created by Smashicons - Flaticon</a>\n",
    "\n",
    "<a href=\"https://de.freepik.com/vektoren/logo\">Logo Vektor erstellt von rawpixel.com - de.freepik.com</a>\n",
    "\n",
    "<a href=\"http://www.freepik.com\">Designed by stories / Freepik</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3932b9e5",
   "metadata": {},
   "source": [
    "### Note: Alternative Ways for Twitter Academic API or Premium Account\n",
    "\n",
    "The search function mandatorily requires environment label and query argument. Label your Application on Twitter Developer page: https://developer.twitter.com/en/account/environments\n",
    "\n",
    "You can optionally add the fromDate and toDate fields to filter search results by time.\n",
    "\n",
    "The format of dates should \"YYYYMMDDHHMM\".\n",
    "\n",
    "tweets_month = api.search_30_day(label='teaching', query=search_words, \n",
    "                                 fromDate=\"202202201000\", toDate=\"202203010000\")\n",
    "\n",
    "Now, you can dump your results into json format *don't forget to import json*: print(json.dumps(tweet_results[0]._json, indent=4, sort_keys=True))\n",
    "                                 \n",
    "For further interest, visit: https://towardsdatascience.com/how-to-use-twitter-premium-search-apis-for-mining-tweets-2705bbaddca\n",
    "\n",
    "Also, there is another library called Twarc2 to explore for further data collection with Twitter v2 API:\n",
    "https://twarc-project.readthedocs.io/en/latest/api/client2/\n",
    "\n",
    "An academic research product:\n",
    "https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research/blob/main/modules/6a-labs-code-academic-python.md\n",
    "\n",
    "A standart product: \n",
    "https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research/blob/main/modules/6b-labs-code-standard-python.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
