{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e607ca26",
   "metadata": {},
   "source": [
    "<img src='images/gesis.png' style='height: 50px; float: left'>\n",
    "<img src='images/social_comquant.png' style='height: 50px; float: left; margin-left: 40px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70bc56f",
   "metadata": {},
   "source": [
    "## Introduction to Computational Social Science methods with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71218328",
   "metadata": {},
   "source": [
    "# Session 8. Unsupervised machine learning\n",
    "\n",
    "A dataset is a collection of values that are organized in two ways (observations and features). A feature contains all vallues that measure the same attribute (e.g., temparature) across all instances. In other words, feature froms a column. An observation contains all values measured on the same instance (e.g, city) across all attributes. In other words, it forms a row. All these obervational instances forms a table that is called feature matrix in machine learning. \n",
    "\n",
    "It has to be mentioned, that features (in machine learning) and varibles (in statistics) are the same concepts. Nevertheles, there is a difference, feature matrix has to contain the same type of data in every cell. The table can contain both types, categorical and continuous data but the first one has to be encoded.\n",
    "\n",
    "Machine learning is a field that studies, develops and utilizes methods (and algorithms) that utilize data (feature matrix) to to improve performance on some set of tasks (Mitchell, 1997). In the simplified classification, machine learning can be divided into supervised and unsupervised learning (Figure 1). \n",
    "\n",
    "TODO: explain differences supervised and unsupervised \n",
    "\n",
    "|<img src='images/Machine_learning.svg' style='float: none; width: 640px'>|\n",
    "|:--|\n",
    "|<em style='float: center'>**Figure 1**: Machine learning structure</em>|\n",
    "\n",
    "...\n",
    "\n",
    "<div class='alert alert-block alert-success'>\n",
    "<b>In this session</b>, \n",
    "\n",
    "you will learn differences between supervised and unsupervised learning. In Subsection **8.1** we will explain what dimentionality reduction is usefull for. The V-DEM dataset will function as an example that showcase usage of Principal Component Analysis (PCA) and how it related to Factor Analysis. (LDA?) In Subsection **8.2** you will learn how to perform clustering analysis on the example of K-Means algorithm. You will find out how to find optimal number of clusters. After finishing it you will be able to describe Agglomarative clustering and why to use it. In Subsection **8.3** you will be introduced to Association Rule mining  and ... Python packages. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee8e7e",
   "metadata": {},
   "source": [
    "2-dimensional data paradigm: observations and features (variables)\n",
    "\n",
    "Well-behaved (dense and balanced) vs. badly-behaved data (sparse, binary, and/or non-linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb89bd",
   "metadata": {},
   "source": [
    "## 8.1. Dimensionality reduction\n",
    "\n",
    "https://scikit-learn.org/stable/modules/decomposition.html#decompositions\n",
    "\n",
    "Hovy textbook chapter 6\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0aec46",
   "metadata": {},
   "source": [
    "### 8.1.1. Structured data\n",
    "\n",
    "Introduce Principal Component Analysis (PCA) using the VDEM dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8db0c4",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab24781",
   "metadata": {},
   "source": [
    "Explain factor analysis and PCA in easy terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec463fb6",
   "metadata": {},
   "source": [
    "Infer two factors and visualize them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687abc24",
   "metadata": {},
   "source": [
    "How to interpret the results\n",
    "\n",
    "Hovy chapter 6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9acf67",
   "metadata": {},
   "source": [
    "caveats re PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa36ea43",
   "metadata": {},
   "source": [
    "\n",
    "Assumptions of factor analysis:\n",
    "features are metric.\n",
    "features are continious or ordinal\n",
    "there is r>0.3 between the features \n",
    "sample is homogenous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c30675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16577910",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdem = pd.read_csv('../data/V-Dem/V-Dem-CY-Core-v13_2019.csv', encoding='utf-8', low_memory=False)\n",
    "#vdem = pd.read_csv('../data/V-Dem/V-Dem-CY-Full+Others-v10.csv', encoding='utf-8', low_memory=False)\n",
    "\n",
    "#vdem = pd.read_csv('../data/V-Dem/V-Dem-CY-Core-v13.csv', encoding='utf-8', low_memory=False)\n",
    "#vdem = vdem[vdem['year'] == 2019].reset_index(drop=True)\n",
    "#vdem.head()\n",
    "#vdem.to_csv('../data/V-Dem/V-Dem-CY-Core-v13_2019.csv', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b532d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdem.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881efb78",
   "metadata": {},
   "source": [
    "old \n",
    "\n",
    "indicators = [\n",
    "    'v2dlreason', 'v2dlcommon', 'v2dlcountr', 'v2dlconslt', 'v2dlengage', \n",
    "    'v2dlencmps', 'v2dlunivl', 'v2cseeorgs', 'v2csreprss', 'v2cscnsult', \n",
    "    'v2csprtcpt', 'v2csgender', 'v2csantimv', 'v2csrlgrep', 'v2csrlgcon', \n",
    "    'v2mecenefm', 'v2mecenefi', 'v2mecenefibin', 'v2mecrit', 'v2merange', \n",
    "    'v2mefemjrn', 'v2meharjrn', 'v2meslfcen', 'v2mebias', 'v2mecorrpt', \n",
    "    'v2exrescon', 'v2exbribe', 'v2exembez', 'v2excrptps', 'v2exthftps', \n",
    "    'v2cldiscm', 'v2cldiscw', 'v2clacfree', 'v2clrelig', 'v2clfmove'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d636789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = [\n",
    "    'v2mecenefm', 'v2meharjrn', 'v2meslfcen', 'v2mebias', 'v2merange', 'v2mecrit', 'v2cldiscm', 'v2cldiscw', 'v2clacfree',\n",
    "    \n",
    "    'v2psparban', 'v2psbars', 'v2psoppaut', 'v2elmulpar', 'v2cseeorgs', 'v2csreprss',\n",
    "    'v2elsuffrage',\n",
    "    'v2elembcap', 'v2elrgstry', 'v2elvotbuy', 'v2elirreg', 'v2elintim', 'v2elpeace', 'v2elfrfair',#'v2elembaut', \n",
    "    'v2lgbicam', 'v2expathhs', 'v2exhoshog','v2exdfcbhs','v2exdfdmhs',\n",
    "    #'v2lginello','v2lgelecup','v2lginelup','v2expathhg','v2lgello',\n",
    "   # 'v2mefemjrn'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ffe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = vdem['country_name'].tolist()\n",
    "vdem = vdem.set_index('country_name')[indicators]\n",
    "vdem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2cfad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdem.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7530c2d9",
   "metadata": {},
   "source": [
    "Are there missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a5467",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdem.isna().sum().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df14470",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdem[vdem.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_countries_na=vdem[vdem.isna().any(axis=1)].index.tolist()\n",
    "vdem=vdem[~vdem.index.isin(list_countries_na)]\n",
    "countries = vdem.index.tolist()\n",
    "vdem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a3ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdem.isna().sum().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29994758",
   "metadata": {},
   "source": [
    "How are the variables distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e01ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdem.loc[:, :].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3bac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a554c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12, 2])\n",
    "sns.boxplot(\n",
    "    x = 'variable', \n",
    "    y = 'value', \n",
    "    data = vdem.loc[:, ~vdem.columns.isin(['v2elsuffrage','v2expathhs'])].melt()#,'v2lgello','v2mefemjrn'\n",
    ")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[1, 2])\n",
    "sns.boxplot(\n",
    "    x = 'variable', \n",
    "    y = 'value', \n",
    "    data = vdem.loc[:, vdem.columns.isin(['v2expathhs'])].melt()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71edf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[4, 2])\n",
    "sns.boxplot(\n",
    "    x = 'variable', \n",
    "    y = 'value', \n",
    "    data = vdem.loc[:, vdem.columns.isin(['v2elsuffrage'])].melt()#'v2lgello'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6149824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vdem = vdem.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed510126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a156207",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vdem = StandardScaler().fit_transform(X_vdem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c932dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12, 2])\n",
    "sns.boxplot(\n",
    "    x = 'variable', \n",
    "    y = 'value', \n",
    "    data = pd.DataFrame(data=X_vdem, columns=indicators).melt()\n",
    ")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdem_z=pd.DataFrame(data=X_vdem, columns=indicators,index=countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f8f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdem_z.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41013c1",
   "metadata": {},
   "source": [
    "<b>Factor Analysis</b>\n",
    "\n",
    "In social sciences one deals a lot with abstract concepts that one can't directly observe and are difficult to measure empirically. For example, imagine we want to measure electoral democracies of given countries. Since electoral democracy is not a material thing, one can't look at country and measure phisically the amount of this democracy. In other words, this is a concept developed by political and social scientists and such a variable that represents it is called 'unobserved' or 'latent' variable (McLevey, 2022).    \n",
    "\n",
    "In order to succied, we would need to find/develop measurement strategies and models that are transparently validated. First, we will need to develop conceptual and operational definition of electoral democracy. A conceptiual definition includes describing how abstarct concept relates to other concepts. It can be done by dividing it into more specific aspects/dimensions. Second, we would need to define operational definition that describes \"specific operations that one would have to perform to generate empirical obeservations(i.e. data) for each of the various dimension.\"(MCLevey, 2022) These variables that are observed are called indicator variables.   \n",
    "\n",
    "According to Coppedge et. al (2021), electoral democracy is divided into five concepts (Figure, 2):  \n",
    "1) Freedom of expression,\n",
    "2) Freedom of association,\n",
    "3) Share of adults citizens with suffrage,\n",
    "4) Officials are elected.\n",
    "\n",
    "Where each of this concepts is composed of many other features (Figure 2, top-level). Since we have these observations for many countries in our dataset. Let us test this theoretical model of latent factors using factor analysis.\n",
    "\n",
    "|<img src='images/Latent_variables.jpg' style='float: none; width: 640px'>|\n",
    "|:--|\n",
    "|<em style='float: center'>**Figure 2**: Latent variables - at the top are observable features... (MCLevey, 2022)</em>|\n",
    "\n",
    "In broader sence, factor analysis objectives are 1) to reduce the number of features, 2) find latent variables. In other words, we will try to cluster features in a meaningful way. This can be achived by checking correlations between features and finding groups of features that are highly correlated between each other and are not correlated with leftover variables.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112725f4",
   "metadata": {},
   "source": [
    "We would need to import neccesary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a7ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install factor-analyzer\n",
    "# Instantiate factor analysis object\n",
    "from factor_analyzer import FactorAnalyzer \n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity,calculate_kmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c5b760",
   "metadata": {},
   "source": [
    "First, we would need to investigate our correlation matrix. For that perpose we would display correlation matrix as heatmap using seaborn package. Parameters xticklabels and  yticklabels we use in order to display all variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b3ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c = pd.DataFrame(FactorAnalyzer(n_factors=vdem_z.shape[1]).fit(vdem_z).corr_,index=indicators,columns=indicators)\n",
    "c = vdem_z.corr() #or you can calculate correlationmatrix using pandas\n",
    "sns.heatmap(c,xticklabels=True, yticklabels=True); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcae19b2",
   "metadata": {},
   "source": [
    "According to the plot we see 3 highly correlated groups. First fifteen variables, then from v2elembcap to v2elfrfair and last four.  \n",
    "\n",
    "Should we drop some variables?\n",
    "\n",
    "On this stage we can also remove variables that have low correlations. As a rule of thumb, we will drop variables with all correlations lower than 0.3. We will investigate values \"v2elsuffrage\",\"v2lgbicam\",\"v2expathhs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc64210",
   "metadata": {},
   "outputs": [],
   "source": [
    "c[[\"v2elsuffrage\",\"v2lgbicam\",\"v2expathhs\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d8cc9",
   "metadata": {},
   "source": [
    "Variables \"v2elsuffrage\" (percent of population with suffrage) and \"v2lgbicam\" (legislature bicameral) do not have even moderate correlations with outher variables. If we return back to Figure 2. We will see that in or theoretical model \"Suffrage\" is one separate indices that can contribute to Electoral democracy latent variable. \n",
    "\n",
    "We will remove both these variables from our model and thus decrease our model to 4 subgroups, i.e., factors. But before dong that we will perform two test: \n",
    "1) Bartlett's test - helps us to judge if every variable correlates badly with all other variables. If it is not then we can apply factor analysis. \n",
    "2) Kaiser-Meyer-Olkin (KMO) - esimates suitability of our data for factor analysis. It will give us as result two parameters: adequacy for each observed variable and complete model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8379ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bartlett's test \n",
    "chi_square_value,p_value=calculate_bartlett_sphericity(vdem_z)\n",
    "print (chi_square_value, p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c471fa4",
   "metadata": {},
   "source": [
    "According to p-value the test was statistically signifficant, i.e., it is suitable for factor analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7768a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMO\n",
    "kmo_all,kmo_model=calculate_kmo(vdem_z)\n",
    "print (kmo_all)\n",
    "print (kmo_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb404f7",
   "metadata": {},
   "source": [
    "Here, values lower than 0.6 are considered inadequate. In our case those are veraibles 16 and 24 that correspond to \"v2elsuffrage\" and \"v2lgbicam\". We will remove them from our model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7966e1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators.remove('v2lgbicam')\n",
    "indicators.remove('v2elsuffrage')\n",
    "vdem_z=vdem_z.drop(['v2lgbicam','v2elsuffrage'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4457439",
   "metadata": {},
   "source": [
    "After removing variables we left with our theoretical model that have 4 factors. So, let us fit this model. \n",
    "\n",
    "In order to improve interpetability we will add rotation parameter, in this case Varimax rotation. More on that you can find here ...  \n",
    "\n",
    "\"Create an optimal number of factors i.e. 5 in our case. Then, we have to interpret the factors by making use of loadings, variance, and commonalities.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54559523",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_an = FactorAnalyzer(n_factors=4,rotation=\"varimax\")\n",
    "factor_an.fit(vdem_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcbda4",
   "metadata": {},
   "source": [
    "Now we can interpret the factors using loading, variance and commonalities. \n",
    "\n",
    "<b>Loadings</b>. These values show the relative contribution of variable towards the factor. In other words, to what extend the factor explains each variable. Values close to 1 or -1 show that certain factor has an influence on a variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0713b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(factor_an.loadings_, columns=[\"Factor1\",\"Factor2\",\"Factor3\",\"Factor4\"], index=[vdem_z.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eeecd7",
   "metadata": {},
   "source": [
    "For example, Factor 1 have high loadings for first fifteen variables. \n",
    "\n",
    "\n",
    "\n",
    "<b>Variance.</b> Let's check the amount of variance that can be explained by each factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed918343",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(factor_an.get_factor_variance(), columns=[\"Factor1\",\"Factor2\",\"Factor3\",\"Factor4\"], \n",
    "             index=[\"SumSquare Loadings\",\"Proportional Variance\",\"Cumulative Variance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb58f4b4",
   "metadata": {},
   "source": [
    "We can say that about 78% of the variance is explained by five factors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9431654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ededee9",
   "metadata": {},
   "source": [
    "\"Communality is the proportion of each variable’s variance that can be explained by the factors. \"\n",
    "\n",
    "\"The proportion of common variance present in a variable is known as the communality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6463c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(factor_an.get_communalities(),index=vdem_z.columns,columns=['Communalities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b506c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_variables = factor_an.fit_transform(vdem_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f509206c",
   "metadata": {},
   "source": [
    "\"We could use these new factors as variable for other analysis or for prediction. Here is the code to apply the factors to the entire dataframe and create the 3 new variables, that could be used as replacement for the \" 35 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42f43d9",
   "metadata": {},
   "source": [
    "\"To figure out how many factors we would need, we can look at eigenvalues, which is a measure of how much of the variance of the variables does a factor explain. An eigenvalue of more than one means that the factor explains more variance than a unique variable. An eigenvalue of 2.5 means that the factor would explain the variance of 2.5 variables, and so on.\"\n",
    "\n",
    "\n",
    "Calculate the eigenvalues, given the factor correlation matrix.\n",
    "\n",
    "\"In factor analysis, eigenvalues are used to condense the variance in a correlation matrix.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2875027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_an = FactorAnalyzer(n_factors=vdem_z.shape[1],rotation='varimax').fit(vdem_z)\n",
    "\n",
    "# Check Eigenvalues\n",
    "ev, v = factor_an.get_eigenvalues()\n",
    "ev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e9548f",
   "metadata": {},
   "source": [
    "\"Eigenvalues are nothing but the amount of variance the factor explains. We will select the number of factors whose eigenvalues are greater than 1.\"\n",
    "We select 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7b8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27095356",
   "metadata": {},
   "source": [
    "\"These techniques differ in the communality estimates that are used. Simplistically, though, factor analysis derives a mathematical model from which factors are estimated, whereas principal components analysis merely decomposes the original data into a set of linear variates\"\n",
    "\n",
    "\n",
    "\"only factor analysis can estimate the underlying factors, and it relies on various assumptions for these estimates to be accurate. Principal components analysis is concerned only with establishing which linear components exist within the data and how a particular variable might contribute to that component. \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ef76a1",
   "metadata": {},
   "source": [
    "\"The first is to assume that all of the variance is common variance. As such, we assume that the communality of every variable is 1. By making this assumption we merely transpose our original data into constituent linear components (known as principal components analysis)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e408d20",
   "metadata": {},
   "source": [
    "\" The second approach is to estimate the mount of common variance by estimating communality values for each variable. There\n",
    "are various methods of estimating communalities, but the most widely used (including\n",
    "alpha factoring) is to use the squared multiple correlation (SMC) of each variable with all\n",
    "others. So, for the popularity data, imagine you ran a multiple regression using one meas-\n",
    "ure (Selfish) as the outcome and the other five measures as predictors: the resulting multi-\n",
    "ple R2 (see section 7.6.2) would be used as an estimate of the communality for the variable\n",
    "Selfish. This second approach is used in factor analysis. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b0258f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################### ------ PCA ---- \n",
    "\n",
    "#no that is pca terminology \n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "transformer = FactorAnalysis(n_components=vdem_z.shape[1], random_state=0,rotation='varimax')\n",
    "#z = transformer.fit_transform(vdem)\n",
    "X= transformer.fit(vdem_z)\n",
    "\n",
    "pd.DataFrame(transformer.components_,columns=indicators)\n",
    "#get_feature_names_out()\n",
    "#transformer.mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c88e5a",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5fce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d0c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_vdem = PCA(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84afa8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vdem_pca = pca_vdem.fit_transform(X_vdem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b52bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vdem_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d656137",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdem_pc = pd.DataFrame(X_vdem_pca, index=countries)\n",
    "vdem_pc.columns=[f'PC {i}' for i in vdem_pc.columns]\n",
    "vdem_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee436879",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pca_vdem.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cc9e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[3, 3])\n",
    "plt.plot(np.cumsum(pca_vdem.explained_variance_ratio_))\n",
    "plt.xlabel('Principal component')\n",
    "plt.ylabel('Cumulative explained variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4690ae",
   "metadata": {},
   "source": [
    "PCA describes as many components as there are variables; the `n_components` parameter just means how many of those are kept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cc8bbc",
   "metadata": {},
   "source": [
    "How many components to keep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d203671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues_vdem = pd.Series(pca_vdem.explained_variance_)\n",
    "eigenvalues_vdem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d163cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[3, 3])\n",
    "plt.plot(eigenvalues_vdem)\n",
    "plt.xlabel('Principal component')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc5a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(X_vdem_pca[:, 0], X_vdem_pca[:, 1]), columns=['PC1', 'PC2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdem_pc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca89f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[9, 6])\n",
    "ax = sns.kdeplot(data=vdem_pc, x='PC 0', y='PC 1', alpha=.8, fill=True)\n",
    "for i, country in enumerate(vdem_pc.index):\n",
    "    ax.text(\n",
    "        x = vdem_pc['PC 0'][i], \n",
    "        y = vdem_pc['PC 1'][i], \n",
    "        s = country, \n",
    "        horizontalalignment = 'left', \n",
    "        size = 6, \n",
    "        color = 'black', \n",
    "        weight = 'normal'\n",
    "    )\n",
    "ax.set(xticklabels=[], yticklabels=[])\n",
    "ax.set(\n",
    "    xlabel = f'$\\longleftarrow$ PC 0 (eigenvalue: {np.round(eigenvalues_vdem.loc[0], 2)}) $\\longrightarrow$', \n",
    "    ylabel = f'$\\longleftarrow$ PC 1 (eigenvalue: {np.round(eigenvalues_vdem.loc[1], 2)}) $\\longrightarrow$'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322358d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b40757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a402d8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a60643dd",
   "metadata": {},
   "source": [
    "\"Run factor analysis if you assume or wish to test a theoretical model of latent factors causing observed variables. Run principal component analysis If you want to simply reduce your correlated observed variables to a smaller set of important independent composite variables.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a900371",
   "metadata": {},
   "source": [
    "BOX: How is PCA related to FactorAnalysis?\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd5021",
   "metadata": {},
   "source": [
    "BOX: Why is that machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d23009a",
   "metadata": {},
   "source": [
    "### 8.1.2. Unstructured (text) data\n",
    "\n",
    "Introduce other methods (Non-negative matrix factorization, Sparse PCA, Kernel PCA, Truncated Singular Value Decomposition aka LSI, Latent Dirichlet Allocation, etc.): https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25903958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20c2f8be",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "**Latent Dirichlet Allocation (LDA)** is a generative probabilistic model which is generally used for topic modeling. \n",
    "\n",
    "**Topic modeling** is a technique in natural language processing (NLP) used to uncover underlying themes in a large corpus of text. The goal of topic modeling is to identify the most important topics in a collection of documents and to extract the most important words and phrases associated with each topic.\n",
    "\n",
    "There are several popular topic modeling algorithms, including Latent Dirichlet Allocation (LDA) and Non-Negative Matrix Factorization (NMF). These algorithms are **unsupervised**, meaning that they do not require labeled data. On the contrary, they are based on the assumption that words that frequently occur together in a document are likely to be associated with the same topic.\n",
    "\n",
    "Topic modeling typically involves the following steps:\n",
    "\n",
    "- Preprocessing the data: The text data is cleaned and preprocessed to remove unnecessary information such as stop words, punctuation, and special characters.\n",
    "- Vectorization: the preprocessed text data is converted into a numerical vector representation, such as a bag-of-words or TF-IDF (Term Frequency-Inverse Document Frequency) matrix.\n",
    "- Model training: a topic modeling algorithm is applied to the vectorized text data to identify the main topics or themes. This includes identifying the words or phrases most strongly associated with each topic.\n",
    "- Theme Interpretation: the resulting themes are interpreted by examining the most representative words or phrases for each theme. This may involve manually examining the most important words for each theme or using automated techniques to summarize the themes.\n",
    "\n",
    "LDA assumes that each document is a mixture of different topics, and each topic is a probability distribution over words. The algorithm takes a collection of documents as input and returns a set of topics, each represented by a probability distribution over words.\n",
    "\n",
    "The basic idea behind LDA is that each document is represented as a distribution of topics and each topic is represented as a distribution of words. The algorithm assumes that the documents are generated in the following way:\n",
    "\n",
    "- For each document, a distribution of topics is selected from a Dirichlet distribution.\n",
    "- For each word in the document:\n",
    "    - Select a topic from the distribution of topics of the document.\n",
    "    - Select a word from the distribution of words for the selected topic.\n",
    "\n",
    "The algorithm iteratively learns the topic distributions for each document and the word distributions for each topic by maximizing the probability of the observed data. This optimization is usually done using the expectation maximization algorithm (EM).\n",
    "\n",
    "Once the algorithm has converged, it returns a set of topics, each represented by a distribution over words. These themes can be interpreted by examining the most likely words for each theme. \n",
    "\n",
    "Here we present an example of LDA applied to a set of documents. We will use the New York Times articles that we preprocessed in Session 6. Let's import the TF-IDF matrix that we previously extracted from this corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fad02c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WASHINGTON — Stellar pitching kept the Mets af...</td>\n",
       "      <td>URL: http://www.nytimes.com/2016/06/30/sports/...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mayor Bill de Blasio’s counsel and chief legal...</td>\n",
       "      <td>URL: http://www.nytimes.com/2016/06/30/nyregio...</td>\n",
       "      <td>nyregion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the early morning hours of Labor Day last y...</td>\n",
       "      <td>URL: http://www.nytimes.com/2016/06/30/nyregio...</td>\n",
       "      <td>nyregion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It was the Apple Store in New York City before...</td>\n",
       "      <td>URL: http://www.nytimes.com/2016/06/30/nyregio...</td>\n",
       "      <td>nyregion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OMAHA — The United States Olympic swimming tri...</td>\n",
       "      <td>URL: http://www.nytimes.com/2016/06/30/sports/...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  WASHINGTON — Stellar pitching kept the Mets af...   \n",
       "1  Mayor Bill de Blasio’s counsel and chief legal...   \n",
       "2  In the early morning hours of Labor Day last y...   \n",
       "3  It was the Apple Store in New York City before...   \n",
       "4  OMAHA — The United States Olympic swimming tri...   \n",
       "\n",
       "                                                 url  category  \n",
       "0  URL: http://www.nytimes.com/2016/06/30/sports/...    sports  \n",
       "1  URL: http://www.nytimes.com/2016/06/30/nyregio...  nyregion  \n",
       "2  URL: http://www.nytimes.com/2016/06/30/nyregio...  nyregion  \n",
       "3  URL: http://www.nytimes.com/2016/06/30/nyregio...  nyregion  \n",
       "4  URL: http://www.nytimes.com/2016/06/30/sports/...    sports  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install gensim\n",
    "import pandas as pd\n",
    "news = pd.read_csv(\"../c_data_preprocessing_methods/data/nyt_articles.csv\")\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c2fde84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "tf_idf_matrix = load_npz(\"../c_data_preprocessing_methods/output/tf_idf_matrix.npz\")\n",
    "feature_names = np.load(\"../c_data_preprocessing_methods/output/feature_names.npz\")[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61d9319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.000*\"bonds.\" + 0.000*\"refinancing\" + 0.000*\"competitive.\" + 0.000*\"securities.\" + 0.000*\"crosby\" + 0.000*\"obligation\" + 0.000*\"lynch.\" + 0.000*\"mr.\" + 0.000*\"sullivan\" + 0.000*\"microbes\"')\n",
      "(1, '0.001*\"mr.\" + 0.001*\"i\" + 0.001*\"game\" + 0.001*\"her\" + 0.001*\"you\" + 0.001*\"she\" + 0.001*\"his\" + 0.001*\"he\" + 0.001*\"ms.\" + 0.001*\"police\"')\n",
      "(2, '0.000*\"gawker\" + 0.000*\"discusses\" + 0.000*\"sehgal\" + 0.000*\"stitcher\" + 0.000*\"archive:\" + 0.000*\"parul\" + 0.000*\"cowles\" + 0.000*\"writes:\" + 0.000*\"podcast,\" + 0.000*\"thiel\"')\n",
      "(3, '0.000*\"coal\" + 0.000*\"tanaka\" + 0.000*\"trump\" + 0.000*\"mr.\" + 0.000*\"fed\" + 0.000*\"dell\" + 0.000*\"percent\" + 0.000*\"ballet\" + 0.000*\"clinton\" + 0.000*\"growth\"')\n",
      "(4, '0.000*\"rohingya\" + 0.000*\"she\" + 0.000*\"her\" + 0.000*\"mr.\" + 0.000*\"european\" + 0.000*\"gay\" + 0.000*\"goddard\" + 0.000*\"i\" + 0.000*\"you\" + 0.000*\"ms.\"')\n",
      "(5, '0.001*\"mr.\" + 0.001*\"she\" + 0.001*\"her\" + 0.001*\"trump\" + 0.001*\"ms.\" + 0.001*\"he\" + 0.001*\"i\" + 0.001*\"his\" + 0.001*\"mrs.\" + 0.001*\"clinton\"')\n",
      "(6, '0.000*\"misstated\" + 0.000*\"article\" + 0.000*\"e-mailed\" + 0.000*\"(212)\" + 0.000*\"incorrectly\" + 0.000*\"misidentified\" + 0.000*\"correction.\" + 0.000*\"error,\" + 0.000*\"editing\" + 0.000*\"indonesian\"')\n",
      "(7, '0.000*\"percent,\" + 0.000*\"cents,\" + 0.000*\"viacom\" + 0.000*\"index\" + 0.000*\"rose\" + 0.000*\"stocks\" + 0.000*\"missile\" + 0.000*\"s.&p.\" + 0.000*\"nadal\" + 0.000*\"dow\"')\n",
      "(8, '0.000*\"redstone\" + 0.000*\"mr.\" + 0.000*\"police\" + 0.000*\"hamas\" + 0.000*\"redstone’s\" + 0.000*\"erdogan\" + 0.000*\"israeli\" + 0.000*\"redstone,\" + 0.000*\"kurdish\" + 0.000*\"bangladesh\"')\n",
      "(9, '0.000*\"rousseff\" + 0.000*\"impeachment\" + 0.000*\"temer\" + 0.000*\"temer,\" + 0.000*\"sanders\" + 0.000*\"clinton\" + 0.000*\"brazil’s\" + 0.000*\"trump\" + 0.000*\"mr.\" + 0.000*\"petrobras\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# create a sample corpus of documents\n",
    "corpus = news.article.values\n",
    "\n",
    "# create a dictionary and bag-of-words representation of the corpus\n",
    "dictionary = corpora.Dictionary([doc.lower().split() for doc in corpus])\n",
    "bow_corpus = [dictionary.doc2bow(doc.lower().split()) for doc in corpus]\n",
    "\n",
    "# create a TF-IDF representation of the bag-of-words corpus\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "tfidf_corpus = tfidf[bow_corpus]\n",
    "\n",
    "#dictionary = corpora.Dictionary([feature_names])\n",
    "#tfidf_corpus = gensim.matutils.Sparse2Corpus(tf_idf_matrix)\n",
    "\n",
    "# train an LDA model on the TF-IDF corpus\n",
    "num_topics = 10\n",
    "lda_model = LdaModel(tfidf_corpus, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "# print the topics and associated keywords\n",
    "for topic in lda_model.print_topics():\n",
    "    print(topic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "572caa29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"501\" + 0.003*\"alpe\" + 0.002*\"auld\" + 0.002*\"bathe\" + 0.002*\"anthropomorphically\" + 0.002*\"afeni\" + 0.002*\"behary\" + 0.002*\"bewilderingly\" + 0.000*\"5c\" + 0.000*\"actuation\"'),\n",
       " (1,\n",
       "  '0.000*\"azerbaijani\" + 0.000*\"ambro\" + 0.000*\"amico\" + 0.000*\"active\" + 0.000*\"anfield\" + 0.000*\"ashamed\" + 0.000*\"1028\" + 0.000*\"ashun\" + 0.000*\"acupressure\" + 0.000*\"antideficiency\"'),\n",
       " (2,\n",
       "  '0.003*\"501\" + 0.003*\"alpe\" + 0.003*\"bathe\" + 0.003*\"auld\" + 0.002*\"anthropomorphically\" + 0.002*\"afeni\" + 0.002*\"bewilderingly\" + 0.002*\"behary\" + 0.000*\"5c\" + 0.000*\"bend\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc1adb5",
   "metadata": {},
   "source": [
    "Bayesian method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86a2b5a",
   "metadata": {},
   "source": [
    "LDA vs. clustering self-trained word embeddings from session 6 (Hovy chapter 9.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c024d",
   "metadata": {},
   "source": [
    "Caveats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6f7cc3",
   "metadata": {},
   "source": [
    "Data: the same corpus that was prepared in session 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c5415",
   "metadata": {},
   "source": [
    "Shortly mention the sklearn LDA pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125d89c5",
   "metadata": {},
   "source": [
    "gensim for LDA (Hovy chapter 9.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00c4edfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0001', ..., '힘들어지고', '힘을', '힘이'], dtype='<U30')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c15f48",
   "metadata": {},
   "source": [
    "BOX: Structural topic model: Hovy chapter 9.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c58e405",
   "metadata": {},
   "source": [
    "## 8.2. Clustering\n",
    "\n",
    "https://scikit-learn.org/stable/modules/clustering.html#clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b208157",
   "metadata": {},
   "source": [
    "### 8.2.1. Structured data\n",
    "\n",
    "VDEM dataset\n",
    "\n",
    "#### K-Means\n",
    "\n",
    "How it works, when to use it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5dad05",
   "metadata": {},
   "source": [
    "Silhouette score to find optimal number of clusters: https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation and https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766fdfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c3455b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f57c04f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d403a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ecd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import colors\n",
    "\n",
    "def plot_vectors(vectors, title='VIZ', labels=None, dimensions=3):\n",
    "    # set up graph\n",
    "    fig = plt.figure(figsize=(3, 3))\n",
    "    \n",
    "    # create data frame\n",
    "    df = pd.DataFrame(data={'x': vectors[:, 0], 'y': vectors[:, 1]})\n",
    "    # add labels, if supplied\n",
    "    if labels is not None:\n",
    "        df['label'] = labels\n",
    "        #print(df.label)\n",
    "    else:\n",
    "        df['label'] = [''] * len(df)\n",
    "    \n",
    "    # assign colors to labels\n",
    "    cm = plt.get_cmap('Set1') # choose the color palette\n",
    "    n_labels = len(df.label.unique())\n",
    "    label_colors = [cm(1. * i / n_labels) for i in range(n_labels)]\n",
    "    cMap = colors.ListedColormap(label_colors)\n",
    "    \n",
    "    # plot in 3 dimensions\n",
    "    if dimensions == 3:\n",
    "        # add z-axis information\n",
    "        df['z'] = vectors[:, 2]\n",
    "        # define plot\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        frame1 = plt.gca()\n",
    "        # remove axis ticks\n",
    "        frame1.axes.xaxis.set_ticklabels([])\n",
    "        frame1.axes.yaxis.set_ticklabels([])\n",
    "        frame1.axes.zaxis.set_ticklabels([])\n",
    "        \n",
    "        # plot each label as scatter plot in its own color\n",
    "        for l, label in enumerate(df.label.unique()):\n",
    "            df2 = df[df.label == label]\n",
    "            ax.scatter(df2['x'], df2['y'], df2['z'], c=label_colors[l], cmap=cMap, edgecolor=None, label=label, alpha=0.3, s=5)\n",
    "        \n",
    "    # plot in 2 dimensions\n",
    "    elif dimensions == 2:\n",
    "        ax = fig.add_subplot(111)\n",
    "        frame1 = plt.gca()\n",
    "        frame1.axes.xaxis.set_ticklabels([])\n",
    "        frame1.axes.yaxis.set_ticklabels([])\n",
    "        \n",
    "        for l, label in enumerate(df.label.unique()):\n",
    "            df2 = df[df.label == label]\n",
    "            ax.scatter(df2['x'], df2['y'], c=label_colors[l], cmap=cMap, edgecolor=None, label=label, alpha=0.3, s=5)\n",
    "    \n",
    "    else:\n",
    "         raise NotImplementedError()\n",
    "    \n",
    "    plt.title(title)\n",
    "    #plt.show()\n",
    "    \n",
    "    if dimensions == 3:\n",
    "        ax.set_xlabel(f'$\\longleftarrow$ PC 0 $\\longrightarrow$')\n",
    "        ax.set_ylabel(f'$\\longleftarrow$ PC 1 $\\longrightarrow$')\n",
    "        ax.set_zlabel(f'$\\longleftarrow$ PC 2 $\\longrightarrow$')\n",
    "    elif dimensions == 2:\n",
    "        ax.set_xlabel(f'$\\longleftarrow$ PC 0 (eigenvalue: {np.round(eigenvalues_vdem.loc[0], 2)}) $\\longrightarrow$')\n",
    "        ax.set_ylabel(f'$\\longleftarrow$ PC 1 (eigenvalue: {np.round(eigenvalues_vdem.loc[1], 2)}) $\\longrightarrow$')        \n",
    "    else:\n",
    "         raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78568233",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "\n",
    "plot_vectors(vectors=X_vdem_pca[:, :n_components], title='V-Dem', labels=None, dimensions=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c38eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "107dd1be",
   "metadata": {},
   "source": [
    "BOX: Other methods for identifying optimal number of clusters; https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3a358",
   "metadata": {},
   "source": [
    "#### Agglomarative clustering\n",
    "\n",
    "How it works, when to use it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42110e4",
   "metadata": {},
   "source": [
    "Silhouette score from the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a3a7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6390234b",
   "metadata": {},
   "source": [
    "### 8.2.2. Unstructured (text) data\n",
    "\n",
    "Mention available methods: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b35abe",
   "metadata": {},
   "source": [
    "Introduce trick to use dimensionality reduction before (K-Means) clustering; https://datascience.stackexchange.com/questions/23591/clustering-algorithms-for-high-dimensional-binary-sparse-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a8fe9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b2e3109",
   "metadata": {},
   "source": [
    "## 8.3. Association rule mining\n",
    "\n",
    "https://sherbold.github.io/intro-to-data-science/05_Association-Rule-Mining.html\n",
    "\n",
    "use http://rasbt.github.io/mlxtend/?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6464ad",
   "metadata": {},
   "source": [
    "Describe associations for TweetsCOV19 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e550a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b98d082a",
   "metadata": {},
   "source": [
    "## Commented references\n",
    "\n",
    "### Recommended textbooks\n",
    "\n",
    "Hovy, D. (2020). *Text Analysis in Python for Social Scientists: Discovery and Exploration*. Cambridge University Press. https://doi.org/10.1017/9781108873352. *ADD COMMENT*\n",
    "\n",
    "### Other cited references\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758d9eec",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "<b>Document information</b>\n",
    "\n",
    "Contact and main author: ...\n",
    "\n",
    "Contributors: ...\n",
    "\n",
    "Acknowledgements: ...\n",
    "\n",
    "Version date: XX. February 2023\n",
    "\n",
    "License: ...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662b5253",
   "metadata": {},
   "source": [
    "#### Notes to be removed before publication\n",
    "\n",
    "- ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
