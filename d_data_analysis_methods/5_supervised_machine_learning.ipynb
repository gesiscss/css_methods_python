{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5d73552",
   "metadata": {},
   "source": [
    "<img src='images/gesis.png' style='height: 50px; float: left'>\n",
    "<img src='images/social_comquant.png' style='height: 50px; float: left; margin-left: 40px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583054cb",
   "metadata": {},
   "source": [
    "# D5. Supervised machine learning\n",
    "\n",
    "Machine learning\n",
    "\n",
    "Unsupervised and supervised\n",
    "\n",
    "Supervised: regression (from linear regression to neural networks) and classification\n",
    "\n",
    "Explanation vs. prediction; induction and deduction\n",
    "\n",
    "Size of data and patterns in it; primary objective is to avoid under and overfitting (where it may be advised to not use all the information data you have)\n",
    "\n",
    "Data leakage\n",
    "\n",
    "standardization and cross-validation\n",
    "\n",
    "Interpretability and use of prediction in CSS\n",
    "\n",
    "\n",
    "> Varieties of Democracy (V-Dem) is a unique approach to conceptualizing and measuring democracy. V-Dem distinguishes between five high-level principles of democracy: electoral, liberal, participatory, deliberative, and egalitarian, and collects data to measure these principles. ([Source](https://www.v-dem.net/about/))\n",
    "\n",
    "The V-Dem dataset consists of countries (observations in rows) described by several hundred of variables (in columns) obtained by surveys, from country websites, and from other sources. We use data about electoral democracy for the year 2019. The following figure shows that several **indicators** (shown in the top row) are supposed to measure five latent low-level **indices** (from \"Freedom of expression\" to \"Elected officials\") which explain the high-level **index** of \"Electoral democracy\". In the regression part, we will reduce the number of variables and cluster the countries. In the classification part, we will predict a continuous numerical measure of a nation's internet freedoms and a four-part regime classification from the indices and indicators.\n",
    "\n",
    "<img src='images/vdem.jpg' style='width: 800px'>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b9299d",
   "metadata": {},
   "source": [
    "## D5.1. Explanation (statistical modeling)\n",
    "\n",
    "- model fit r^2\n",
    "- hypothesis and significance testing\n",
    "- generalized linear model: linear and logistic regression\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b6bdc6",
   "metadata": {},
   "source": [
    "### D5.1.1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e67f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab467b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdem = pd.read_csv(filepath_or_buffer='../data/vdem.tsv', sep='\\t', encoding='utf-8', low_memory=False)\n",
    "vdem = vdem.set_index('country_name')\n",
    "vdem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6125a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high-level index\n",
    "index = ['v2x_polyarchy']\n",
    "\n",
    "# low-level indices\n",
    "indices = ['v2x_freexp_altinf', 'v2x_frassoc_thick', 'v2x_suffr', 'v2xel_frefair', 'v2x_elecoff']\n",
    "\n",
    "# indicator variables\n",
    "indicators = ['v2mecenefm', 'v2meharjrn', 'v2meslfcen', 'v2mebias', 'v2merange', 'v2mecrit', 'v2cldiscm', 'v2cldiscw', 'v2clacfree', 'v2psparban', 'v2psbars', 'v2cseeorgs', 'v2csreprss', 'v2elsuffrage', 'v2elembaut', 'v2elembcap', 'v2lgbicam', 'v2lgello', 'v2lginello', 'v2expathhs', 'v2exdfcbhs', 'v2exdfdmhs', 'v2exhoshog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9202ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29857b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get correlations\n",
    "vdem[indices].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267322b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec3af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot correlations\n",
    "fig = sns.pairplot(\n",
    "    data = vdem[indices], \n",
    "    height = 2, \n",
    "    kind = 'reg', \n",
    "    diag_kind = 'hist', \n",
    "    plot_kws = {'scatter_kws': {'alpha': .2}}\n",
    ")\n",
    "#fig.savefig('results/...vdem_fh_pairplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d07e682",
   "metadata": {},
   "source": [
    "### D5.1.2. Linear regression\n",
    "\n",
    "#### Ordinary-Least-Squares regression\n",
    "\n",
    "We will predict `total_score`, a continuous numerical measure of a nation's internet freedoms, by the five low-level indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80298780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove missing values\n",
    "vdem_regression = vdem[~vdem['total_score'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed150911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get array of independent variables\n",
    "X_regression_sm = vdem_regression[indices].to_numpy()\n",
    "X_regression_sm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c9ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get numerical dependent variable\n",
    "y_regression_sm = vdem_regression[['total_score']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c962d",
   "metadata": {},
   "source": [
    "sklearn does not report p values. Use: statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcef025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.api import OLS, add_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ed12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column of ones to model the intercept\n",
    "X_regression_sm = add_constant(X_regression_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c7a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model to data\n",
    "ols_sm = OLS(y_regression_sm, X_regression_sm) # equivalent to fit_intercept=True in sklearn (https://stackoverflow.com/questions/70179307/why-is-sklearn-r-squared-different-from-that-of-statsmodels-when-fit-intercept-f/70180217#70180217)\n",
    "ols_sm = ols_sm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_sm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40485e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# goodness of fit (R^2)\n",
    "ols_sm.rsquared.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60917fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intercept and coefficients\n",
    "ols_sm.params.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f3c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# significance scores\n",
    "ols_sm.pvalues.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5f5c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e20ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_coef(df, coef, pvalues=None):\n",
    "    df[coef] = df[coef].apply(lambda x: '{0:.4f}'.format(x))\n",
    "    if pvalues is not None:\n",
    "        def stars(cell):\n",
    "            if cell <= .001:\n",
    "                cell = '***'\n",
    "            elif cell <= .01:\n",
    "                cell = '**'\n",
    "            elif cell <= .05:\n",
    "                cell = '*'\n",
    "            else:\n",
    "                cell = ''\n",
    "            return cell\n",
    "        df['pvalues'] = pvalues\n",
    "        df['stars'] = df['pvalues'].apply(stars)\n",
    "        df[coef] = df[coef] + df['stars']\n",
    "        del df['pvalues']\n",
    "        del df['stars']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26262c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdem_coef = pd.DataFrame(\n",
    "    data = ols_sm.params, \n",
    "    index = np.concatenate([['intercept'], indices]), \n",
    "    columns = ['OLS']\n",
    ")\n",
    "vdem_coef = format_coef(df=vdem_coef, coef='OLS', pvalues=ols_sm.pvalues)\n",
    "vdem_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e0a8e3",
   "metadata": {},
   "source": [
    "#### Ridge regression to deal with collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84475403",
   "metadata": {},
   "source": [
    "Significance scores are meaningless with regularization (https://stats.stackexchange.com/questions/224796/why-are-confidence-intervals-and-p-values-not-reported-as-default-for-penalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c5b048",
   "metadata": {},
   "source": [
    "Ridge regression requires setting scaling penalties: https://stackoverflow.com/questions/72260808/mismatch-between-statsmodels-and-sklearn-ridge-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d9f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model to data\n",
    "n = vdem_regression.shape[0]\n",
    "penalty_ridge = np.concatenate([[0.], np.ones(len(indices))]) / n\n",
    "ridge_sm = OLS(y_regression_sm, X_regression_sm)\n",
    "ridge_sm = ridge_sm.fit_regularized(alpha=penalty_ridge, L1_wt=0.)\n",
    "\n",
    "# display rsults\n",
    "vdem_coef['Ridge'] = ridge_sm.params\n",
    "vdem_coef = format_coef(df=vdem_coef, coef='Ridge', pvalues=None)\n",
    "vdem_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd918a",
   "metadata": {},
   "source": [
    "#### Lasso regression to automatically select variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4de7b",
   "metadata": {},
   "source": [
    "Lasso regression requires setting scaling penalties: https://stackoverflow.com/questions/72260808/mismatch-between-statsmodels-and-sklearn-ridge-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f649d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model to data\n",
    "alpha_lasso = 1. # if alpha < 1. then Lasso becomes Elastic Net (https://stats.stackexchange.com/questions/319861/how-to-interpret-lasso-shrinking-all-coefficients-to-0)\n",
    "penalty_lasso = np.concatenate([[0.], np.ones(len(indices)) * alpha_lasso])\n",
    "lasso_sm = OLS(y_regression_sm, X_regression_sm)\n",
    "lasso_sm = lasso_sm.fit_regularized(alpha=penalty_lasso, L1_wt=1.) # reuse 'OLS' object\n",
    "\n",
    "# display rsults\n",
    "vdem_coef['Lasso'] = lasso_sm.params\n",
    "vdem_coef = format_coef(df=vdem_coef, coef='Lasso', pvalues=None)\n",
    "vdem_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d0e4b1",
   "metadata": {},
   "source": [
    "#### Ordinary-Least-Squares regression with the variables selected in Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb80c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get array of independent variables\n",
    "variables_select = vdem_coef[vdem_coef['Lasso'].astype(float) != 0].index.tolist()\n",
    "X_regression_sm_select = X_regression_sm[:, [i for i, e in enumerate(vdem_coef.index) if e in variables_select]]\n",
    "\n",
    "# fit model to data\n",
    "ols_sm_select = OLS(y_regression_sm, X_regression_sm_select) # equivalent to fit_intercept=True in sklearn (https://stackoverflow.com/questions/70179307/why-is-sklearn-r-squared-different-from-that-of-statsmodels-when-fit-intercept-f/70180217#70180217)\n",
    "ols_sm_select = ols_sm_select.fit()\n",
    "\n",
    "# display results\n",
    "vdem_coef_select = pd.DataFrame(\n",
    "    data = ols_sm_select.params, \n",
    "    index = variables_select, \n",
    "    columns = ['OLS (select)']\n",
    ")\n",
    "vdem_coef_select = format_coef(df=vdem_coef_select, coef='OLS (select)', pvalues=ols_sm_select.pvalues)\n",
    "vdem_coef = pd.merge(left=vdem_coef, right=vdem_coef_select, left_index=True, right_index=True, how='left')\n",
    "vdem_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c7d8d",
   "metadata": {},
   "source": [
    "### D5.1.3. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b23be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_classification = vdem[indices].to_numpy()\n",
    "X_classification_const = add_constant(X_classification)\n",
    "y_classification_binary = np.where(vdem['v2x_regime'] <= 1, 0, 1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.api import Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ac80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_sm = Logit(y_classification_binary, X_classification_const)\n",
    "logit_sm = logit_sm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc10e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_sm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0079d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# goodness of fit\n",
    "logit_sm.prsquared.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intercept and coefficients\n",
    "logit_sm.params.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc68d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# significance scores\n",
    "logit_sm.pvalues.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09475fd",
   "metadata": {},
   "source": [
    "## D5.2. Prediction\n",
    "\n",
    "### D5.2.1. Regression\n",
    "\n",
    "- Out-of-sample testing\n",
    "- Cross validation\n",
    "- Data leakage\n",
    "- Feature selection\n",
    "- Over- and underfitting\n",
    "\n",
    "- https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44006f94",
   "metadata": {},
   "source": [
    "##### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9cc977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which predictors to use\n",
    "predictors = indices # indices or indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3805277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get array of predictor variables\n",
    "X_regression = vdem_regression[predictors].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a509ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get numerical response variable\n",
    "y_regression = vdem_regression[['total_score']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cf5ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a080f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize variables\n",
    "X_regression = StandardScaler().fit_transform(X_regression)\n",
    "y_regression = np.ravel(StandardScaler().fit_transform(y_regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb718a5",
   "metadata": {},
   "source": [
    "##### Splitting data into sets to train and test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c966dc",
   "metadata": {},
   "source": [
    "For developing machine learning models we need to split our data; some for training the model and others for testing and validating it. Without splitting, our model will simply learn to retrodict data that it has already seen, rather than predicting the data it has not.\n",
    "\n",
    "Every time you split your data, you remove some of the information the model can learn from. If you remove too much, a full train/validation/test split might impact your model’s performance in a negative way.\n",
    "\n",
    "Fortunately, we can sidestep this issue without sacrificing any principles. The process follows these steps:\n",
    "1. Split your data into train and test sets. All of the training data will be fully available\n",
    "to train on. The test set will not be used in any way until a final model has been\n",
    "selected.\n",
    "2. Use cross-validation (explained later in this section) to produce an optimal set of training\n",
    "hyperparameters.\n",
    "3. Select the best cross-validated model and evaluate using test data.\n",
    "\n",
    "We can complete the first step in Sklearn using `train_test_split()` function. Below, we separate our data into two objects: `X` and `y`. Doing so brings us inline with a long-standing convention that the predictor data is stored in an upper-case X, indicating a matrix of covariates, or **design matrix**. The lower-case y, or the **target**, indicates a vector of outcome values. The machine learning models we employ will\n",
    "learn how to predict the y values using the X values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ba1aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test sets\n",
    "X_regression_train, X_regression_test, y_regression_train, y_regression_test = train_test_split(\n",
    "    X_regression, \n",
    "    y_regression, \n",
    "    test_size=.5, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6daefc9",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "\n",
    "The linear regression model is about as transparent and interpretable as machine\n",
    "learning gets. Linear regression models are algorithmically simple, and since they\n",
    "have been widely used in the social sciences, they're rather easier to begin with if you don't have much experience with machine learning.\n",
    "\n",
    "In an OLS regression, we model an outcome of interest (y) as a\n",
    "function of a weighted sum of input features (X) and random error. The *weights* in a\n",
    "linear model are the coefficients, which are *learnt* during training. For example, we\n",
    "might predict the degree of internet freedom in a country as a linear function of some other\n",
    "regime characteristics.\n",
    "\n",
    "In the context of machine learning, the goal with a regression model such as this is to\n",
    "find a line (if you have one feature), a plane (if you have two features), or a hyperplane\n",
    "(if you have three or more features) that best fits the data. When we fit our model to the\n",
    "training data, it *learns* the best value for the intercept and slope by minimizing the\n",
    "mean-squared error (MSE), which is the sum of the squared differences between each\n",
    "observed value in the data and the predicted value from the model.\n",
    "\n",
    "In order to do this, we first need to create an OLS object as follows, and then learn the model parameters by fitting the model to our training data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bff9164",
   "metadata": {},
   "source": [
    "##### Fitting model on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a79435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e645697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LinearRegression object and regress y on X\n",
    "lr = LinearRegression(fit_intercept=False).fit(X=X_regression_train, y=y_regression_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccee3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coefficients\n",
    "lr.coef_.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42323103",
   "metadata": {},
   "source": [
    "We can check to see how well the model managed to to fit our data using R^2 score. It measures how much of the variance in the dependent variable can be explained by the model; a score of 1 indicates a perfect fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb74b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coefficient of determination (model fit)\n",
    "lr.score(X=X_regression_train, y=y_regression_train).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85126fe6",
   "metadata": {},
   "source": [
    "0.85 is a pretty high score for R^2, it means that our trained model is capable of accounting for roughly 85% of the variance in the training data with just six parameters (including the intercept). This could indicate overfitting to the training data, meaning that our model can perform very well on the data it has learned from, but it's not that capable when encountering new data. In order to examine that, we will use cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dfdb94",
   "metadata": {},
   "source": [
    "**Cross-validation** builds on the intuition behind training and testing sets, but does so\n",
    "repeatedly, training and assessing models each time. The most common type of cross-\n",
    "validation in machine learning is **k-fold cross-validation**, which splits our data into k\n",
    "number of equally sized folds. We then use these folds as a sliding window of training and\n",
    "validation splits. If we are doing fivefold cross-validation, we segment our dataset into\n",
    "five folds and fit and assess five models.\n",
    "\n",
    "The first model is trained using the data\n",
    "contained in folds 2 to 5 and then validated on the data on in fold 1. The second model\n",
    "is trained on the data in fold 1 and folds 3 to 5 and validated on the data in fold 2, and\n",
    "so on. The model evaluation scores are computed for all five and then examined\n",
    "together, or summarized as an average. If we are using accuracy as our evaluation\n",
    "score, ideally we would see that all five accuracy measures are high and reliable; if\n",
    "there is a lot of variation in our accuracy scores, then the model is likely over-relying on\n",
    "characteristics of data in some of the folds.\n",
    "\n",
    "\n",
    "Putting OLS and CV together: The code below is going to produce five scores from the five training–validations splits it produces internally. We’re primarily interested in the stability of the score (how much it\n",
    "fluctuates between the folds).\n",
    "\n",
    "If our model is consistent in its performance but not as accurate as we would like, then\n",
    "we have to improve our analysis. We might improve the quality of the input data or\n",
    "make improvements to the model itself. If we see a lot of variation in the model accuracy\n",
    "on different folds, then we have a different problem and we need to change how we\n",
    "segment our data into folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6343ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcebdc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "shufflesplit = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2498ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regress y on X five times using five folds and shuffled splits\n",
    "score_cv_lr = cross_val_score(\n",
    "    estimator = LinearRegression(fit_intercept=False), \n",
    "    X = X_regression_train, \n",
    "    y = y_regression_train, \n",
    "    scoring = 'r2', \n",
    "    cv = shufflesplit\n",
    ")\n",
    "score_cv_lr.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43726c9",
   "metadata": {},
   "source": [
    "Three of the scores are excellent, falling somewhere in the high 0.7 to low 0.8 range.\n",
    "The remaining two are far worse. Our model’s performance seems to depend on which\n",
    "data it trains on (and, equivalently, the data upon which it must validate itself).\n",
    "\n",
    "The gap between our high and low cross-validation scores might indicate that our data\n",
    "is ordered or clustered in some way. It could be that our observations appear in\n",
    "alphabetical order by country name, or something similar. In such cases, it can be\n",
    "useful to shuffle the data before we split it to ensure that we are not simply rolling over\n",
    "one group at a time. Doing this is as simple as using Sklearn’s `ShuffleSplit()`,\n",
    "which takes two arguments: **the number** (supplied as an integer) or **percentage**\n",
    "(supplied as a float) of instances to sample for the training and test sets, and the\n",
    "**number of iterations**, or **splits**, to perform. You can then pass the resulting object into\n",
    "`cross_val_score`’s cv argument, and Sklearn smoothly handles the rest:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9f459c",
   "metadata": {},
   "source": [
    "As you can see, by simply randomizing the order in which our observations appear, we were able to smooth out our R^2 scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d43262",
   "metadata": {},
   "source": [
    "Cheating on the Test: Now let’s take the mean value across all folds and use that as a point of comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6af36a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average coefficient of determination\n",
    "score_cv_lr.mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873d553",
   "metadata": {},
   "source": [
    "The score from our cross-validation (0.61) is ... lower than the one we ...\n",
    "received by training on the training dataset (0.85), but that’s to be expected. We can\n",
    "think of the cross-validation score as a *validation score* in the sense that it measures\n",
    "our model’s performance on data it wasn’t able to train on (averaged across five\n",
    "different configurations of that set-up). Our original OLS model, by comparison, was\n",
    "able to train on the entire dataset at once; its score represents how well it fit the data it\n",
    "trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b75f90a",
   "metadata": {},
   "source": [
    "A remedy to the overfitting issue is often to make your model *less powerful* or to use some kind of regularization technique. Remember, though, that under normal circumstances, we wouldn’t be able to see our model’s test score. In an attempt to wean ourselves off of test scores, we’re going to spend the rest of this section creating\n",
    "regularized models without examining the test scores (we’ll save that for the very end).\n",
    "\n",
    "#### Regularization via Ridge Regression\n",
    "\n",
    "We recognize an overfitting problem when the quality of a model drops when making\n",
    "predictions on the test set. To address this, we could provide some additional\n",
    "constraints to prevent our model from learning too much from the training data. One\n",
    "method is ridge regression, which uses L2 regularization to make the coefficients as\n",
    "close to 0 as possible while still making good predictions. In effect, L2 regularization\n",
    "applies a penalty to model parameters that scales with their magnitude. This means that\n",
    "your model is incentivized to keep each parameter value as small as possible. This\n",
    "tension is useful for preventing overfitting.\n",
    "\n",
    "To fit a ridge regression model, we follow the same process as before, only unlike our\n",
    "OLS model, ridge regression accepts one important hyperparameter: **alpha**. The\n",
    "alpha hyperparameter determines the strength of the regularizing penalty the ridge\n",
    "regression applies to each of our parameters; the higher it is, the stronger it is. It\n",
    "defaults to a value of 1, which is generally a good starting point. We’ll start by creating a\n",
    "fresh set of training and test data (with a new random seed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35067271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab83bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regress y on X five times using five folds and shuffled splits\n",
    "score_cv_ridge = cross_val_score(\n",
    "    estimator = Ridge(alpha=1., fit_intercept=False), \n",
    "    X = X_regression_train, \n",
    "    y = y_regression_train, \n",
    "    scoring = 'r2', \n",
    "    cv = shufflesplit\n",
    ")\n",
    "score_cv_ridge.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb0e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cv_ridge.mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c25e87",
   "metadata": {},
   "source": [
    "We can see that the use of ridge regression has left us very slightly better off than our\n",
    "original OLS regression, but not by much. It might be possible to improve the cross-\n",
    "validation scores by modifying the `alpha` parameter, but let’s try another regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f569ef0b",
   "metadata": {},
   "source": [
    "#### Regularization via Lasso Regression\n",
    "\n",
    "We could use L1 regularization, which penalizes coefficient values that are close to 0\n",
    "much more harshly than the comparatively light treatment that L2 regularization offers.\n",
    "The result is that the model is forced to use only a subset of the available features,\n",
    "which it selects automatically. All other coefficients are set to 0. This approach is called\n",
    "lasso regression. As with ridge, lasso takes an **alpha** parameter that determines\n",
    "how aggressive the regularization is. If we have an underfitting problem, then we want\n",
    "to decrease a to soften the constraints and let the model learn more from the training\n",
    "data. Conversely, if we have an overfitting problem, we want to increase `alpha` to more\n",
    "aggressively push the coefficients towards 0 and learn less from the training data.\n",
    "Creating a lasso regression model is the same process as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df362a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f3a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regress y on X five times using five folds and shuffled splits\n",
    "score_cv_lasso = cross_val_score(\n",
    "    estimator = Lasso(alpha=1., fit_intercept=False), \n",
    "    X = X_regression_train, \n",
    "    y = y_regression_train, \n",
    "    scoring = 'r2', \n",
    "    cv = shufflesplit\n",
    ")\n",
    "score_cv_lasso.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ceb1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cv_lasso.mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0221748b",
   "metadata": {},
   "source": [
    "Our cross-validation R^2 score with default parameter settings (alpha=1) is negative, which means, ...\n",
    "\n",
    "We might be able to squeeze a bit more life out of our regularized models by tweaking\n",
    "the `alpha` hyperparameter. If not specified, `alpha` defaults to 1. As `alpha` increases, the\n",
    "model becomes more simple, more constrained, and more regularized. As it decreases,\n",
    "the model becomes more complex, less constrained, less regularized. Let’s compare the results of a series of ridge and lasso regressions on this data using\n",
    "different `alpha` parameters. We will define a set of `alpha` values, estimate a series of\n",
    "ridge and lasso regressions, and then plot their R^2 scores for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eed0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, 2, 100)\n",
    "\n",
    "scores_cv_ridge = []\n",
    "scores_cv_lasso = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    scores_cv_ridge.append(cross_val_score(estimator=Ridge(alpha=alpha, fit_intercept=False), X=X_regression_train, y=y_regression_train, cv=shufflesplit).mean())\n",
    "    scores_cv_lasso.append(cross_val_score(estimator=Lasso(alpha=alpha, fit_intercept=False), X=X_regression_train, y=y_regression_train, cv=shufflesplit).mean())\n",
    "    \n",
    "scores = pd.DataFrame(\n",
    "    zip(alphas, scores_cv_ridge, scores_cv_lasso), \n",
    "    columns = ['alpha', 'Ridge Regression', 'Lasso Regression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4876689",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.axhline(score_cv_lr.mean(), label='Linear', linestyle='dotted', color='darkgray')\n",
    "sns.lineplot(x='alpha', y='Ridge Regression', data=scores, label='Ridge', linestyle='solid')\n",
    "sns.lineplot(x='alpha', y='Lasso Regression', data=scores, label='Lasso', linestyle='dashed')\n",
    "ax.set(xlabel='alpha values for Ridge and Lasso Regressions', ylabel='Coefficient of determination', xscale='log')\n",
    "sns.despine()\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce85015",
   "metadata": {},
   "source": [
    "As you can see, all three of the model types we’re testing – ridge, lasso, and OLS\n",
    "– converge as `alpha` approaches 0 (we didn’t actually fit any of the models with an\n",
    "`alpha` of zero, since the models only accept non-negative, non-zero values for `alpha`),\n",
    "but rapidly diverge thereafter. As `alpha` increases, lasso regression’s performance\n",
    "increases, falters, and begins a nosedive as `alpha` approaches 0.5. Ridge regression\n",
    "rises and falls like lasso regression, but over a much larger scale of `alpha`.\n",
    "\n",
    "Although the peaks of ridge and lasso are close, it would appear that ridge regression\n",
    "with a haphazardly optimized `alpha` parameter is our best fit for this model. We’ll\n",
    "retrieve that value of `alpha`, fit a new model, and interpret the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92703d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(scores_cv_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf44dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(scores_cv_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4355dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_best = alphas[scores_cv_ridge.index(max(scores_cv_ridge))]\n",
    "alpha_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e5c26b",
   "metadata": {},
   "source": [
    "Let’s use this to fit a ridge regression and get the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d368d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Ridge object and regress y on X\n",
    "ridge = Ridge(alpha=alpha_best, fit_intercept=False).fit(X=X_regression_train, y=y_regression_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c396ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.score(X=X_regression_train, y=y_regression_train).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d43362",
   "metadata": {},
   "source": [
    "##### Evaluating model on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22bc56f",
   "metadata": {},
   "source": [
    "Now there's only one step left: testing the model on the test data. Normally, assessing your validated model’s performance on test data should only be done **once you are completely finished developing your models**. If you use your test data to help you improve your model, you’re causing *data leakage*, wherein knowledge your model shouldn’t have access to is being used to improve it. Here, we will just complete the further steps for the sake of indicating how it's done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9622766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X=X_regression_test, y=y_regression_test).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b184dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.score(X=X_regression_test, y=y_regression_test).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7e15d0",
   "metadata": {},
   "source": [
    "Alternatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c0697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict response variable\n",
    "y_lr_pred = lr.predict(X=X_regression_test)\n",
    "y_ridge_pred = ridge.predict(X=X_regression_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca37f647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985882ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get coefficient of determination\n",
    "#r2_score(y_true=y_regression_test, y_pred=y_lr_pred).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predicted against true response variable\n",
    "plt.scatter(x=y_regression_test, y=y_lr_pred, label='Linear Regression')\n",
    "plt.scatter(x=y_regression_test, y=y_ridge_pred, label='Ridge Regression')\n",
    "plt.xlabel('True response variable')\n",
    "plt.ylabel('Predicted response variable')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336474a9",
   "metadata": {},
   "source": [
    "... the training score (0.85) is substantially higher than the test score (0.67), which is an indication\n",
    "that our model is overfitting the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23536f54",
   "metadata": {},
   "source": [
    "It looks good! Each of the predictions is off by a modest amount, but there’s only one truly\n",
    "off-base prediction (Angola, with a difference of 23). Many predictions are very close!\n",
    "Like most aspects of machine learning, linear regression isn’t a *one-size-fits-all* solution.\n",
    "It’s a family of models with a variety of tweakable hyperparameters that deserve your\n",
    "attention. If you put in the effort, you’ll likely be rewarded with a model that fits the data\n",
    "well and is highly interpretable. That said, linear regression is not suitable for all tasks;\n",
    "we will now take a look at a model better suited to classification tasks: **logistic regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac57d3",
   "metadata": {},
   "source": [
    "#### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391fb717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "cross_val_score(estimator=DecisionTreeRegressor(random_state=42), X=X_regression_train, y=y_regression_train, scoring='r2', cv=shufflesplit).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f0ead1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "938f573e",
   "metadata": {},
   "source": [
    "#### RULES-BASED LEARNING WITH TREES\n",
    "\n",
    "Decision trees can be used for both classification and regression tasks. Here we will focus on a classification example, but the process is more or less the same for the regression tasks.\n",
    "\n",
    "A decision tree is in fact a directed network that starts with a single node including every instance in the data set. Beginning from that node, it *asks* a series of *questions* in order to find the correct labels in a classification problem, or the correct values in a regression problem. A good decision tree should ask the most informative questions in each step; we will see how it can be done shortly. The questions will always concern the value for\n",
    "some specific feature for each instance, like *Does Canada hold free and fair\n",
    "elections?* or *Is Canada’s score for freedom of the press higher than the median\n",
    "score?*\n",
    "\n",
    "Every time the model asks a question, a node containing some subset of instances in\n",
    "our dataset splits off into two new nodes. Depending on the answer to the question,\n",
    "each observation moves from the parent node into one of the two child nodes. This\n",
    "process continues until \n",
    "- All of the observations contained in a node share the same value for the outcome you want the model to be able to predict, or\n",
    "- Your tree model runs out of room to ask more questions.\n",
    "\n",
    "The path from the root node\n",
    "(every instance in the dataset) to each leaf in the tree constitutes a *rule*. We can collect\n",
    "all of these rules into a single hierarchical rule base that is relatively easy for humans to\n",
    "interpret and understand.\n",
    "\n",
    "Now that we know the basics, it’s time to answer a critical question: *How does the\n",
    "model decide which question to ask next?* How does it know what the *most informative*\n",
    "question is? The most common method is to use the concept of entropy from\n",
    "information theory. In information theory, entropy is a measure of how much information\n",
    "something contains, expressed in terms of uncertainty.\n",
    "\n",
    "To use a simplified example, let’s say we want to figure out which of the nations in the\n",
    "VDEM dataset are democracies. If you think elections are all you need to be considered\n",
    "a democracy, then you could just ask one question for each case – do they hold\n",
    "elections? *However*, not all elections are the same, and democracies are about much\n",
    "more than elections. So you keep asking questions until you are confident you can\n",
    "make a good judgement. The more questions you need to ask to arrive at a confident\n",
    "judgement, the more accurate your classification of the observations into \"democracies\"\n",
    "and \"autocracies\" will be. The more purely separated those two classes become, the\n",
    "lower the \"entropy\" in your model. In the context of a decision tree analysis, the model\n",
    "will *always* ask the question that will result in the biggest decrease in entropy, usually\n",
    "expressed in terms of **information gain**, which quantifies the decrease in entropy that\n",
    "resulted from asking the question.\n",
    "\n",
    "At this point, there shouldn’t be much doubt about how easily the VDEM dataset we’ve\n",
    "been using up until here can be classified; nevertheless, we are going to use it\n",
    "here again. We are not going to do so because it will provide us with a better\n",
    "classification (we already achieved very good scores using a logistic regression) but\n",
    "rather because the resultant decision tree model will allow us to easily see what\n",
    "information the model finds most useful when deciding whether a nation is an autocracy\n",
    "or a democracy.\n",
    "\n",
    "As usual, we will start by splitting our dataset into a matrix `X` and and outcome vector `y`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f377cde",
   "metadata": {},
   "source": [
    "The technique we’re using to convert the 4-point `v2x_regime` scale into a binary\n",
    "variable is identical to the one we employed in the previous section.\n",
    "With `X` and `y` created, we can create our training and test sets, and then create and fit\n",
    "our decision tree classifier using cross-validation (in much the same way as we did in\n",
    "the previous section; go back to it for more details on cross-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f538219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor(random_state=42)\n",
    "dtr = dtr.fit(X=X_regression_train, y=y_regression_train)\n",
    "score_dtr = cross_val_score(dtr, X_regression_train, y_regression_train, cv=shufflesplit)\n",
    "print(score_dtr)\n",
    "print(f\"Mean: {score_dtr.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92848aa9",
   "metadata": {},
   "source": [
    "In order to get a sense of what our tree is doing under the hood, the below picture\n",
    "represents our decision tree. You start at the top node, which contains all of the\n",
    "observations (countries in this case). The top line in that node (and every non-leaf node\n",
    "in the remainder of the tree) indicates the rule it will use to split the data. All of the\n",
    "countries for which that statement is true will travel along the **True** path for further\n",
    "subdivision. All of the nations for whom this condition does not apply travel along the\n",
    "**False** path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef67dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretable_names = [\n",
    "    'Freedom of expression and alternative sources of information index',\n",
    "    'Freedom of association index (thick)',\n",
    "    'Share of population with suffrage',\n",
    "    'Clean elections index',\n",
    "    'Elected officials index'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782cd594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "    decision_tree = dtr,\n",
    "    out_file = 'results/dtr.dot', \n",
    "    filled = False,\n",
    "    rounded = True,\n",
    "    feature_names = interpretable_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e6a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygraphviz import AGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345d8516",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = AGraph(path='results/dtr.dot')\n",
    "G.draw(path='results/dtr.png', prog='dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b07776f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b4ad72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7111bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bac99358",
   "metadata": {},
   "source": [
    "##### What About Overfitting?\n",
    "\n",
    "Decision trees are also prone to overfitting. The tree grows bigger with every question, and by the time we’ve reached the leaves, we know\n",
    "everything we need to know to make predictions that are 100% right 100% of the time -*for the data we trained the model on*. This extreme overfitting is sometimes called\n",
    "**memorizing** the training data. We don’t want that!\n",
    "\n",
    "One way to address the overfitting problem with decision trees is to **prune** them.\n",
    "Remember that the model *always* asks the most informative question first. This means\n",
    "that as the trees get deeper and deeper (as we ask more questions), each feature is\n",
    "weaker or less predictive than those that came before it. As we move further and further\n",
    "out, we risk making decisions based on noise and overfitting the model to the data we\n",
    "have. The full tree, then, is typically *worse* than a pruned tree because it includes weak\n",
    "features that could be specific to our dataset and which do not generalize.\n",
    "\n",
    "We constrain the depth of the tree by restricting the number of questions or decisions\n",
    "that the model is allowed to ask, and in doing so, we improve the ability of our model to\n",
    "generalize to data it hasn’t seen before. If we set the maximum depth of our tree to 6,\n",
    "for example, the models can only ask the six most informative questions, at which point\n",
    "it must make its prediction. Obviously, this reduces the accuracy on the training data,\n",
    "but not as much as you might think. It’s the unseen data we care most about, and the\n",
    "pruned model will make much better predictions when it is not overfitted.\n",
    "\n",
    "In Sklearn, we specify the maximum depth of the tree in advance. This can be done using the `max_depth` argument for the `DecisionTreeClassifier()`. Let’s set it to 3. This will produce a very shallow tree, but that’s desirable; we want it to have to make the best decisions it can in broad strokes. This way, the model will be less likely to overfit the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b3efd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtclass_pruned = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "dt_scores = cross_val_score(dtclass_pruned, X_train, y_train, cv=shuffsplit)\n",
    "print(dt_scores)\n",
    "print(f\"Mean: {dt_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0753d4a0",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>TO DOUBLE CHECK:</b>\n",
    "    \n",
    "The output directory! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtclass_pruned.fit(X_train, y_train)\n",
    "\n",
    "export_graphviz(\n",
    "    dtclass_pruned,\n",
    "    out_file='../graphical_models/pruned.gv',\n",
    "    filled=False,\n",
    "    rounded=True,\n",
    "    feature_names=interpretable_names,\n",
    "    class_names=le.classes_,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f76f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtclass_pruned.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354afc04",
   "metadata": {},
   "source": [
    "We’ve already seen a modest improvement, which probably represents a\n",
    "slight reduction in overfitting (something that cross-validation automatically assesses).\n",
    "Let’s examine the tree again:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb2b3f5",
   "metadata": {},
   "source": [
    "<img src='images/decisiontree2.jpg' style='height: 400px; float: left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c3d725",
   "metadata": {},
   "source": [
    "You can see the influence of setting the `max_depth` parameter to 3 in the tree: rather\n",
    "than a sprawling monstrosity, we now have a tree that neatly terminates each branch at\n",
    "the same level. Decision trees have other parameters you can tweak, such as `min_samples_leaf`; it’s worth looking at the documentation to see the options available to you! Using only `max_depth`, we managed to get a good result, but we’re unlikely to be able to do much better using regularization alone. As we saw with ridge\n",
    "and lasso regression, regularization usually reaches a \"sweet spot\" at some modest\n",
    "value, but as the strength of the regularization increases, the model’s performance\n",
    "nosedives. Decision trees have, by their nature, low granularity. You can’t perform fine-grained regularization on a single decision tree the same way you could for an `alpha`\n",
    "parameter on a ridge or lasso regression (what would a `max_depth` of 3.5 even look\n",
    "like?). It’s likely that no regularization of a single-tree model will eliminate overfitting\n",
    "entirely. Instead, we’ll have to turn to a method which will allow us to combine many,\n",
    "many trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814e676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb8014b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22906e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d647ceb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8369f3f",
   "metadata": {},
   "source": [
    "#### Multi-Layer Perceptron Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985c9398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8709be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "cross_val_score(estimator=MLPRegressor(random_state=42), X=X_regression_train, y=y_regression_train, scoring='r2', cv=shufflesplit).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bde7fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ff019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be8e9c04",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d0b4a3",
   "metadata": {},
   "source": [
    "#### Model selection\n",
    "\n",
    "- By interpretability\n",
    "- By predictive accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d02857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2888bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9190addc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db65b5e7",
   "metadata": {},
   "source": [
    "### CLASSIFICATION WITH LOGISTIC REGRESSION\n",
    "\n",
    "When the goal is classification, logistic regression provides better results than the\n",
    "models in the previous section. It’s also highly interpretable and can be used for binary\n",
    "or multi-class classification problems. It’s a very flexible model, in part because it\n",
    "doesn’t assume a linear relationship between the response variable and our explanatory\n",
    "feature matrix. While similar to linear regression in many ways, rather than predict a\n",
    "numerical outcome for a variable, logistic regression describes the probability that an\n",
    "observation would have a particular value in a categorical variable. Logistic regression\n",
    "is typically conducted using two classes, but it can also be extended to multiple classes.\n",
    "\n",
    "Given that logistic regression is designed to answer different kinds of questions than\n",
    "linear regression, we’re going to have to create a new set of training and test data. Let’s\n",
    "say we want to predict whether a given country is governed democratically, as opposed\n",
    "to autocratically. We have a variable from the VDEM dataset that will serve for this\n",
    "purpose: it is a 4-point scale, with\n",
    "\n",
    "- 0 representing closed autocracies,\n",
    "- 1 representing electoral autocracies,\n",
    "- 2 representing electoral democracies, and\n",
    "- 3 representing liberal democracies.\n",
    "\n",
    "We’re going to simplify this down to a 2-point scale, with **0** indicating **autocracies**, and **1**\n",
    "indicating **democracies**. Using this recoding, we can use binary logistic regression to\n",
    "predict the probability that any given country in our dataset belongs to one of the two\n",
    "categories. Our predictions will be based on the five measures of internet freedom\n",
    "drawn from the VDEM dataset, briefly discussed earlier in this notebook. We'll create our new X and y like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e7cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.where(vdem[\"v2x_regime\"] <= 1, 0, 1).copy()\n",
    "\n",
    "X = vdem[[\n",
    "                 'v2smgovdom_osp', \n",
    "                 \"v2smgovfilprc_osp\", \n",
    "                 \"v2smgovsmcenprc_osp\", \n",
    "                 \"v2smonper_osp\", \n",
    "                 \"v2smarrest_osp\", \n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1470e54",
   "metadata": {},
   "source": [
    "Now we perform a new train–test split and estimate our binary logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d971bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=23)\n",
    "shuffsplit = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "log_reg = cross_val_score(\n",
    "    LogisticRegression(), \n",
    "    X_train, \n",
    "    y_train, \n",
    "    cv=shuffsplit)\n",
    "print(log_reg)\n",
    "print(f\"Mean: {log_reg.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b74690",
   "metadata": {},
   "source": [
    "As before, we could use regularization to deal with underfitting and overfitting problems.\n",
    "In this case, the parameter that controls regularization is called `C`. The logic is similar to\n",
    "when we used `alpha`, but it goes in the opposite direction. Increasing the\n",
    "value of `C` reduces regularization and results in more complex models that can learn\n",
    "more from the training data. Decreasing `C` results in more regularization that constrains\n",
    "how much the model can learn from the training data. So when we set a low value for `C`,\n",
    "the logistic regression model will force the coefficients to be closer to 0, but not exactly 0. The code for accomplishing this is like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1854e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_regularized = cross_val_score(\n",
    "    LogisticRegression(C=0.5), \n",
    "    X_train, \n",
    "    y_train, \n",
    "    cv=shuffsplit)\n",
    "print(log_reg_regularized)\n",
    "print(f\"Mean: {log_reg_regularized.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9225bfa4",
   "metadata": {},
   "source": [
    "In this case, altering our regularization parameter didn’t help at all. Rather than bury this\n",
    "result or massage it to produce a desirable outcome, we’re going to preserve this as a\n",
    "reminder that using reasonable techniques in machine learning can often produce\n",
    "uninteresting, uninformative, or confusing results.\n",
    "Despite the roadblock we encountered here, it should be clear that it is relatively\n",
    "straightforward to use linear and logistic regression models, with and without\n",
    "regularization to prevent overfitting, within a supervised machine learning framework.\n",
    "You might also have noticed that we did not need to write a lot of code to do the actual\n",
    "learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4641d0",
   "metadata": {},
   "source": [
    "##### CONCLUSION\n",
    "\n",
    "The key points in this section were as follows:\n",
    "\n",
    "- We used Sklearn to set up, build, fit, and interpret supervised machine learning\n",
    "models.\n",
    "\n",
    "- We learnt how to prepare data by splitting our features into two different arrays, one\n",
    "containing the labels we want to predict (quantitative or categorical) and the other\n",
    "containing the values we want to use in our predictions.\n",
    "\n",
    "- We learnt how to use cross-validation to remove the need for a separate validation\n",
    "split and to harness the entire training set when tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5142cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6d1b5bc",
   "metadata": {},
   "source": [
    "### Classification: Decision trees and random forests\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692d690b",
   "metadata": {},
   "source": [
    "In the previous section, we worked with a few simple and relatively familiar supervised learning models. In this section we will continue that with some other types of models: **decision trees**, **ensemble learning**, **random forests**, and **gradient-boosted\n",
    "machines**. We will finish with a description of model evaluation metrics, comparing\n",
    "accuracy, precision, recall, and some ways we can make better use of these metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cae4c4f",
   "metadata": {},
   "source": [
    "##### The Train-Test Split and Cross-Validation\n",
    "\n",
    "Same as the previous section, we will use the VDEM data on a country’s political and electoral freedoms to predict internet freedoms drawn from the Freedom House dataset. We will begin with splitting the data into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b749ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = vdem_fh[['v2x_polyarchy', 'v2x_libdem', 'v2x_partipdem', 'v2x_delibdem', 'v2x_egaldem',]]\n",
    "y = vdem_fh[['Total Score']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c2be1",
   "metadata": {},
   "source": [
    "#### RULES-BASED LEARNING WITH TREES\n",
    "\n",
    "Decision trees can be used for both classification and regression tasks. Here we will focus on a classification example, but the process is more or less the same for the regression tasks.\n",
    "\n",
    "A decision tree is in fact a directed network that starts with a single node including every instance in the data set. Beginning from that node, it *asks* a series of *questions* in order to find the correct labels in a classification problem, or the correct values in a regression problem. A good decision tree should ask the most informative questions in each step; we will see how it can be done shortly. The questions will always concern the value for\n",
    "some specific feature for each instance, like *Does Canada hold free and fair\n",
    "elections?* or *Is Canada’s score for freedom of the press higher than the median\n",
    "score?*\n",
    "\n",
    "Every time the model asks a question, a node containing some subset of instances in\n",
    "our dataset splits off into two new nodes. Depending on the answer to the question,\n",
    "each observation moves from the parent node into one of the two child nodes. This\n",
    "process continues until \n",
    "- All of the observations contained in a node share the same value for the outcome you want the model to be able to predict, or\n",
    "- Your tree model runs out of room to ask more questions.\n",
    "\n",
    "The path from the root node\n",
    "(every instance in the dataset) to each leaf in the tree constitutes a *rule*. We can collect\n",
    "all of these rules into a single hierarchical rule base that is relatively easy for humans to\n",
    "interpret and understand.\n",
    "\n",
    "Now that we know the basics, it’s time to answer a critical question: *How does the\n",
    "model decide which question to ask next?* How does it know what the *most informative*\n",
    "question is? The most common method is to use the concept of entropy from\n",
    "information theory. In information theory, entropy is a measure of how much information\n",
    "something contains, expressed in terms of uncertainty.\n",
    "\n",
    "To use a simplified example, let’s say we want to figure out which of the nations in the\n",
    "VDEM dataset are democracies. If you think elections are all you need to be considered\n",
    "a democracy, then you could just ask one question for each case – do they hold\n",
    "elections? *However*, not all elections are the same, and democracies are about much\n",
    "more than elections. So you keep asking questions until you are confident you can\n",
    "make a good judgement. The more questions you need to ask to arrive at a confident\n",
    "judgement, the more accurate your classification of the observations into \"democracies\"\n",
    "and \"autocracies\" will be. The more purely separated those two classes become, the\n",
    "lower the \"entropy\" in your model. In the context of a decision tree analysis, the model\n",
    "will *always* ask the question that will result in the biggest decrease in entropy, usually\n",
    "expressed in terms of **information gain**, which quantifies the decrease in entropy that\n",
    "resulted from asking the question.\n",
    "\n",
    "At this point, there shouldn’t be much doubt about how easily the VDEM dataset we’ve\n",
    "been using up until here can be classified; nevertheless, we are going to use it\n",
    "here again. We are not going to do so because it will provide us with a better\n",
    "classification (we already achieved very good scores using a logistic regression) but\n",
    "rather because the resultant decision tree model will allow us to easily see what\n",
    "information the model finds most useful when deciding whether a nation is an autocracy\n",
    "or a democracy.\n",
    "\n",
    "As usual, we will start by splitting our dataset into a matrix `X` and and outcome vector `y`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c9410",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>NOTE:</b>\n",
    "    \n",
    "I used `vdem` instead of `dem_indices` in the follwoing cells. Let me know if it is incorrect, so that I will modify it. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from graphviz import Source\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# dem_indices = pd.read_csv(\"../data/vdem_internet_freedom_combined/dem_indices.csv\")\n",
    "\n",
    "X = vdem[[\n",
    "                 'v2smgovdom_osp', # Government dissemination of false information domestic\n",
    "                 \"v2smgovfilprc_osp\", # Government internet filtering in practice\n",
    "                 \"v2smgovsmcenprc_osp\", # Government social media censorship in practice\n",
    "                 \"v2smonper_osp\", # Diversity of online media perspectives (0 = gov't only, 4 = any perspective)\n",
    "                 \"v2smarrest_osp\", # Arrests for political content disseminated online\n",
    "]]\n",
    "\n",
    "interpretable_names = [\n",
    "    'Domestic Misinformation',\n",
    "    'Internet Filtering',\n",
    "    'Social Media Censorship',\n",
    "    'Online Media Diversity',\n",
    "    'Arrests for Political Content'\n",
    "]\n",
    "\n",
    "regime_types = [\n",
    "    'Autocracy',\n",
    "    'Democracy',\n",
    "]\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(regime_types)\n",
    "\n",
    "y = np.where(vdem[\"v2x_regime\"] <= 1, 0, 1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b13eb",
   "metadata": {},
   "source": [
    "The technique we’re using to convert the 4-point `v2x_regime` scale into a binary\n",
    "variable is identical to the one we employed in the previous section.\n",
    "With `X` and `y` created, we can create our training and test sets, and then create and fit\n",
    "our decision tree classifier using cross-validation (in much the same way as we did in\n",
    "the previous section; go back to it for more details on cross-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c940458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=23)\n",
    "\n",
    "shuffsplit = ShuffleSplit(n_splits=5, test_size=0.3, random_state=42)\n",
    "\n",
    "dtclass = DecisionTreeClassifier(random_state=0)\n",
    "dt_scores = cross_val_score(dtclass, X_train, y_train, cv=shuffsplit)\n",
    "print(dt_scores)\n",
    "print(f\"Mean: {dt_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7230d3eb",
   "metadata": {},
   "source": [
    "In order to get a sense of what our tree is doing under the hood, the below picture\n",
    "represents our decision tree. You start at the top node, which contains all of the\n",
    "observations (countries in this case). The top line in that node (and every non-leaf node\n",
    "in the remainder of the tree) indicates the rule it will use to split the data. All of the\n",
    "countries for which that statement is true will travel along the **True** path for further\n",
    "subdivision. All of the nations for whom this condition does not apply travel along the\n",
    "**False** path.\n",
    "\n",
    "<img src='images/decisiontree1.jpg' style='height: 700px; float: left'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vdem.v2x_regime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3401d89c",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>TO DOUBLE CHECK:</b>\n",
    "    \n",
    "The output directory! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd375b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be added: wrapping in graphviz.Source()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a8e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "dt_fitted = dtclass.fit(X_train, y_train)\n",
    "\n",
    "export_graphviz(\n",
    "    dtclass,\n",
    "    out_file='../graphical_models/classified_1.gv', \n",
    "    filled=False,\n",
    "    rounded=True,\n",
    "    feature_names=interpretable_names,\n",
    "    class_names=le.classes_,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1bdf89",
   "metadata": {},
   "source": [
    "##### What About Overfitting?\n",
    "\n",
    "Decision trees are also prone to overfitting. The tree grows bigger with every question, and by the time we’ve reached the leaves, we know\n",
    "everything we need to know to make predictions that are 100% right 100% of the time -*for the data we trained the model on*. This extreme overfitting is sometimes called\n",
    "**memorizing** the training data. We don’t want that!\n",
    "\n",
    "One way to address the overfitting problem with decision trees is to **prune** them.\n",
    "Remember that the model *always* asks the most informative question first. This means\n",
    "that as the trees get deeper and deeper (as we ask more questions), each feature is\n",
    "weaker or less predictive than those that came before it. As we move further and further\n",
    "out, we risk making decisions based on noise and overfitting the model to the data we\n",
    "have. The full tree, then, is typically *worse* than a pruned tree because it includes weak\n",
    "features that could be specific to our dataset and which do not generalize.\n",
    "\n",
    "We constrain the depth of the tree by restricting the number of questions or decisions\n",
    "that the model is allowed to ask, and in doing so, we improve the ability of our model to\n",
    "generalize to data it hasn’t seen before. If we set the maximum depth of our tree to 6,\n",
    "for example, the models can only ask the six most informative questions, at which point\n",
    "it must make its prediction. Obviously, this reduces the accuracy on the training data,\n",
    "but not as much as you might think. It’s the unseen data we care most about, and the\n",
    "pruned model will make much better predictions when it is not overfitted.\n",
    "\n",
    "In Sklearn, we specify the maximum depth of the tree in advance. This can be done using the `max_depth` argument for the `DecisionTreeClassifier()`. Let’s set it to 3. This will produce a very shallow tree, but that’s desirable; we want it to have to make the best decisions it can in broad strokes. This way, the model will be less likely to overfit the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cbd115",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtclass_pruned = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "dt_scores = cross_val_score(dtclass_pruned, X_train, y_train, cv=shuffsplit)\n",
    "print(dt_scores)\n",
    "print(f\"Mean: {dt_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59789f02",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>TO DOUBLE CHECK:</b>\n",
    "    \n",
    "The output directory! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtclass_pruned.fit(X_train, y_train)\n",
    "\n",
    "export_graphviz(\n",
    "    dtclass_pruned,\n",
    "    out_file='../graphical_models/pruned.gv',\n",
    "    filled=False,\n",
    "    rounded=True,\n",
    "    feature_names=interpretable_names,\n",
    "    class_names=le.classes_,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtclass_pruned.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107a79e8",
   "metadata": {},
   "source": [
    "We’ve already seen a modest improvement, which probably represents a\n",
    "slight reduction in overfitting (something that cross-validation automatically assesses).\n",
    "Let’s examine the tree again:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30467b26",
   "metadata": {},
   "source": [
    "<img src='images/decisiontree2.jpg' style='height: 400px; float: left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8659f",
   "metadata": {},
   "source": [
    "You can see the influence of setting the `max_depth` parameter to 3 in the tree: rather\n",
    "than a sprawling monstrosity, we now have a tree that neatly terminates each branch at\n",
    "the same level. Decision trees have other parameters you can tweak, such as `min_samples_leaf`; it’s worth looking at the documentation to see the options available to you! Using only `max_depth`, we managed to get a good result, but we’re unlikely to be able to do much better using regularization alone. As we saw with ridge\n",
    "and lasso regression, regularization usually reaches a \"sweet spot\" at some modest\n",
    "value, but as the strength of the regularization increases, the model’s performance\n",
    "nosedives. Decision trees have, by their nature, low granularity. You can’t perform fine-grained regularization on a single decision tree the same way you could for an `alpha`\n",
    "parameter on a ridge or lasso regression (what would a `max_depth` of 3.5 even look\n",
    "like?). It’s likely that no regularization of a single-tree model will eliminate overfitting\n",
    "entirely. Instead, we’ll have to turn to a method which will allow us to combine many,\n",
    "many trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f99f372",
   "metadata": {},
   "source": [
    "#### ENSEMBLE LEARNING\n",
    "\n",
    "In order to tackle the overfitting problem, an effective way is to use an ensemble approach, which combines predictions from multiple models into a single prediction that\n",
    "is better than that of any individual model. As you will soon learn, this approach tends to\n",
    "produce excellent results and does not require any pruning. Ensembles of decision\n",
    "trees produce better results than any one decision tree, including any of the individual\n",
    "decision trees in the ensemble.\n",
    "\n",
    "To work with an ensemble of decision trees, we first draw many bootstrapped samples\n",
    "of instances from our overall dataset. In a bootstrapped sample, replacement is allowed,\n",
    "which means that the same instance can be sampled more than once. For each sample,\n",
    "we fit a decision tree and record the model’s predictions. The final predictions are made\n",
    "by an *election* of sorts, where each tree *votes* on the class they think each\n",
    "observation belongs to. If we take 200 samples, we would fit 200 decision trees. These\n",
    "modes are used collectively -as an ensemble- to make predictions on new instances\n",
    "by taking averages of the predictions made by the models that make up the ensemble.\n",
    "This process is called **bagging** or **bootstrapped aggregation**, and it can be applied not\n",
    "only to decision trees but also to a wide variety of the classification models implemented\n",
    "in scikit-learn. For now, we will stick to applying it to decision trees.\n",
    "\n",
    "Bagging or bootstrapped aggregation goes a very long way in addressing the overfitting\n",
    "problem. One major advantage is that we don’t have to prune our decision trees. In fact,\n",
    "it’s better if we don’t! If we let each tree grow to be as deep and complex as it likes, we\n",
    "will end up with an ensemble that has high variance but low bias. That’s exactly what we\n",
    "want when we go to make our final aggregated predictions. The important choice you\n",
    "must make is how many bags to use, or rather, how many bootstrapped samples of\n",
    "instances to draw, and the number of total trees we want to end up with. Let’s see what\n",
    "the combination of 100 trees can bring us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag_of_trees = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                                n_estimators=100,\n",
    "                                bootstrap=True,\n",
    "                                random_state=0)\n",
    "\n",
    "bt_scores = cross_val_score(bag_of_trees, X_train, y_train, cv=shuffsplit)\n",
    "print(bt_scores)\n",
    "print(f\"Mean: {bt_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26424b64",
   "metadata": {},
   "source": [
    "The unregularized bagging classifier has produced an even better score than the\n",
    "regularized decision tree did! There may yet be more room for improvement if we alter\n",
    "how each of the trees functions using a random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba59ca54",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "\n",
    "One issue with the bagging approach is that the resulting trees tend to be correlated\n",
    "with one another, mainly due to the fact that they are all trying to maximize the same\n",
    "thing when they ask questions – information gain. If there are some very powerful\n",
    "attributes in our dataset, as there almost always are, the tree we fit for each bag will\n",
    "lean heavily on those features, which makes the whole ensemble approach a lot less\n",
    "useful and degrades the quality of the final prediction. It would be much better for us if\n",
    "the trees are not correlated, or are at best weakly correlated.\n",
    "\n",
    "Random forests accomplish this with one simple, but highly effective, modification: *they\n",
    "constrain the features that any given node is allowed to ask questions about*. The result\n",
    "is a collection of decision trees that are uncorrelated, or weakly correlated, with one\n",
    "another, which leads to more accurate predictions when they are aggregated.\n",
    "\n",
    "Random forests are straightforward to train, and because of their clever design, they do\n",
    "a good job of dealing with noise and preventing overfitting, so it is not necessary to trim\n",
    "or prune the trees. They also take only two hyperparameters: **the number of trees in the\n",
    "forest** (i.e. the number of samples of instances to draw) and **the size of the random\n",
    "sample** to draw when sampling the features that any given decision tree will select from.\n",
    "You can and should experiment with cross-validation to select values for these\n",
    "hyperparameters that result in the most accurate predictions (we’re not doing so here\n",
    "because space is limited)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80518945",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rforest = RandomForestClassifier(n_estimators=100,\n",
    "                                max_features=2,\n",
    "                                random_state=0)\n",
    "\n",
    "rforest_scores = cross_val_score(rforest, X_train, y_train, cv=shuffsplit)\n",
    "print(rforest_scores)\n",
    "print(f\"Mean: {rforest_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67036d04",
   "metadata": {},
   "source": [
    "It would appear that our random forest model, with modest parameters, is producing the\n",
    "*exact same result* as we got with our bagging classifier.\n",
    "\n",
    "The downside of random forests is that – unlike garden-variety decision trees – the\n",
    "results are not so easy to interpret. For this reason, random forests and other ensemble\n",
    "models are generally considered to be less *interpretable* than simple decision trees,\n",
    "linear and logistic regressions, or k-nearest neighbours. While you can inspect any of\n",
    "the trees in your random forest classifier, this process is complicated somewhat by the\n",
    "fact that our model contains 100 distinct trees, and we can’t easily determine how\n",
    "significant any one tree was to the overall decision-making process. Nevertheless, it’s a\n",
    "good idea to select a a tree at random and take a look at what it did with the data. Of\n",
    "course, you can do this many different times, if you like. Just select different trees each\n",
    "time. One such tree is shown the following picture:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89830ff",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>TO DOUBLE CHECK:</b>\n",
    "    \n",
    "The output directory! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c20c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rforest.fit(X_train, y_train)\n",
    "\n",
    "export_graphviz(\n",
    "    rforest.estimators_[6],\n",
    "    out_file='../graphical_models/rf_classified.gv',\n",
    "    filled=False,\n",
    "    rounded=True,\n",
    "    feature_names=interpretable_names,\n",
    "    class_names=le.classes_,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628fc727",
   "metadata": {},
   "source": [
    "<img src='images/decisiontree3.jpg' style='height: 600px; float: left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2a5a8",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Machines\n",
    "\n",
    "While random forests remain one of the best and most widely used approaches to\n",
    "supervised machine learning, a slightly newer approach to ensembling decision trees\n",
    "has recently started outperforming random forests and is widely considered to be one of\n",
    "the best algorithms for doing machine learning on anything other than image or\n",
    "perception data (Chollet, 2018). This technique is called **gradient boosting**, and it\n",
    "differs from the random forest approach in that rather than allowing all of the decision\n",
    "trees to randomly pursue the best answer possible in isolation (as random forest does),\n",
    "it attempts to fit trees that better account for the misclassified observations from\n",
    "previous trees. In this way, each tree tackles the *room for improvement* left behind by\n",
    "the tree that immediately preceded it. The effect here is that the gradient-boosted trees\n",
    "can reach a remarkably high degree of accuracy using only a small handful of\n",
    "estimators (but are accordingly prone to overfitting). Let’s try creating one now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9903df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gboost = GradientBoostingClassifier(n_estimators=100,\n",
    "                                random_state=0)\n",
    "\n",
    "gboost_scores = cross_val_score(gboost, X_train, y_train, cv=shuffsplit)\n",
    "print(gboost_scores)\n",
    "print(f\"Mean: {gboost_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0ceec7",
   "metadata": {},
   "source": [
    "The gradient-boosted trees achieved worse performance than our previous two models.\n",
    "Usually, we would expect a gradient-boosted trees model to outperform all of our other\n",
    "decision tree models (ensemble or otherwise), but that shouldn’t be interpreted as a\n",
    "good reason to skip straight to gradient boost without bothering to specify and fit any\n",
    "other models. What we’ve seen here is evidence to that point; there is value in fitting\n",
    "*intermediate* models to see how their performance and idiosyncrasies compare to the\n",
    "cutting-edge techniques. There are a few reasons why this is a vital practice.\n",
    "\n",
    "- *Advanced, complicated methods are not intrinsically better than simple methods*: not\n",
    "only is this true in our example – given that one of the most demonstrably powerful and\n",
    "widely applicable algorithms, gradient boosting, failed to outperform random forests –\n",
    "but it is often true in general. Cutting-edge methods are indispensable for their ability to\n",
    "tackle cutting-edge issues, but they’re often overkill for the kinds of problems they get\n",
    "applied to.\n",
    "\n",
    "- *Don’t sacrifice interpretability without good cause*: explicable, interpretable, transparent\n",
    "models that slightly underperform are often more valuable than top-performing ‘black\n",
    "box’ models that appear to be more accurate, but for reasons that are hard to establish.\n",
    "Gradient-boosted models are more difficult to interpret than decision tree models, so the\n",
    "advantages of the former over the latter should be considered in light of the\n",
    "interpretability trade-off.\n",
    "\n",
    "- *Any problem in machine learning should be tackled using multiple approaches*: even if\n",
    "you feel like you can’t improve on your model, there may be undiscovered issues lurking\n",
    "beneath the surface. Applying a multitude of modelling strategies to a problem – even in\n",
    "cases where your first model is performing well – may help confirm the defensibility of\n",
    "your primary approach, give you more inferential insight, or uncover contingencies that\n",
    "need to be addressed.\n",
    "\n",
    "One problem common to all tree-based models (ensemble or otherwise) is that they\n",
    "require an abundance of data and are especially prone to overfitting in cases where\n",
    "such data is not forthcoming. There are different ways to make up for a lack of\n",
    "data.\n",
    "\n",
    "Before we move on, let’s take a moment to compare how each of our tree-based\n",
    "models perform on the test set which we split off from the training data right at the\n",
    "beginning of this section and haven’t touched since:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b29e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [dtclass,\n",
    "dtclass_pruned,\n",
    "bag_of_trees.fit(X_train, y_train),\n",
    "rforest,\n",
    "gboost.fit(X_train, y_train)]\n",
    "\n",
    "for model in model_list:\n",
    "    print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de338429",
   "metadata": {},
   "source": [
    "Looks like the training results match up nicely with the test results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21de5e9e",
   "metadata": {},
   "source": [
    "BOXES: REFER TO SUPPORT VECTOR MACHINES AND NAIVE BAYES CLASSIFICATION\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8629e83d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bfddd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7156deb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb0a698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5842683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2639439f",
   "metadata": {},
   "source": [
    "#### Notes to be removed before publication\n",
    "\n",
    "- Check https://doi.org/10.1145/1150402.1150412\n",
    "- add cvxopt=1.2.6 to environment.yml -- NOT NECSSARY ANYMORE CAUSE WE DON'T USE method=sqrt_lasso IN statsmodels RIDGE REGRESSION ANYMORE\n",
    "- https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#sphx-glr-auto-examples-inspection-plot-linear-model-coefficient-interpretation-py\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c10e78e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
