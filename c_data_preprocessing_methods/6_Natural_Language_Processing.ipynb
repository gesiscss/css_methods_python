{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/gesis.png' style='height: 50px; float: left'>\n",
    "<img src='images/social_comquant.png' style='height: 50px; float: left; margin-left: 40px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Computational Social Science methods with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6: Natural Language Processing\n",
    "The field of study that focuses on the interactions between human language and computers is called natural language processing (NLP).\n",
    "\n",
    "NLP is a field of artificial intelligence in which computers analyze, understand, and derive meaning from human language in a smart and useful way. NLP systems are used exploiting the signals in our language used to predict all of the aforementioned features: people’s age (Nguyen et al., 2011; Rosenthal & McKeown, 2011), gender (Alowibdi et al., 2013; Ciot et al., 2013; Liu & Ruths, 2013), personality (Park et al., 2015), job title (Preoţiuc-Pietro et al., 2015a), income (Preoţiuc-Pietro et al., 2015b), and much more (Volkova et al., 2014, 2015).\n",
    "\n",
    "In NLP, word embeddings have been at the forefront of this progress, which has expanded to include flexible model architectures (Hovy, 2021). The most publicly visible example of this shift is probably the translation quality of services like Google Translate (Wu et al., 2016).\n",
    "\n",
    "A collection of fundamental tasks appear frequently across various NLP projects (Vajjala et al., 2020). Let’s briefly introduce them (Figure 1):\n",
    "\n",
    "<img src='images/nlp_tasks.png' style='height: 400px; float: right'>\n",
    "\n",
    "*Language modeling* is the task of predicting what the next word in a sentence will be based on the history of previous words. The goal of this task is to learn the probability of a sequence of words appearing in a given language. Language modeling is useful for building solutions for a wide variety of problems, such as **speech recognition, optical character recognition, handwriting recognition, machine translation, and spelling correction**.\n",
    "\n",
    "*Text classification* is the task of bucketing the text into a known set of categories based on its content. Text classification is by far the most popular task in NLP and is used in a variety of tools, from **email spam identification** to **sentiment analysis**.\n",
    "\n",
    "*Information extraction* is the task of extracting relevant information from text, such as **calendar events from emails** or the **names of people mentioned** in a social media post.\n",
    "\n",
    "*Information retrieval* is the task of finding documents relevant to a user query from a large collection. Applications like **Google Search** are well-known use cases of information retrieval.\n",
    "\n",
    "*Conversational agent* is the task of building dialogue systems that can converse in human languages. **Alexa** and **Siri** are some common applications of this task.\n",
    "\n",
    "*Text summarization* aims to create short summaries of longer documents while retaining the **core content** and preserving the **overall meaning** of the text.\n",
    "\n",
    "*Question answering* is the task of building a system that can automatically answer questions posed in natural language.\n",
    "\n",
    "*Machine translation* is the task of converting a piece of text from one language to another. Tools like **Google Translate** are common applications of this task.\n",
    "\n",
    "*Topic modeling* is the task of uncovering the topical structure of a large collection of documents. Topic modeling is a common text-mining tool and is used in a wide range of domains, from **literature** to **bioinformatics**.\n",
    "\n",
    "\n",
    "\n",
    "Understanding human language is considered as a difficult task due to its complexity. For example, there is an infinite number of different ways to arrange words in a sentence. \n",
    "\n",
    "Also, words can have several meanings and contextual information is necessary to correctly interpret sentences as every language is unique and ambiguous. The ambiguity can be in lexical and syntactic forms.\n",
    "\n",
    "- In lexical ambiguity, a single word has two or more possible meanings. For example, \"I saw bats\".\n",
    "- In syntactic ambiguity, a single sentence or a sequence of words have multiple possible meanings. For example, \"The chicken is ready to eat\".\n",
    "\n",
    "This session will help you understand the basic and advanced NLP concepts and show you how to implement using the most advanced and popular NLP libraries, such as  <a href=\"https://spacy.io/\">spaCy</a> and <a href=\"https://radimrehurek.com/gensim/\">Gensim</a>.\n",
    "\n",
    "<!-- – <a href=\"https://www.nltk.org/\">NLTK</a>, <a href=\"https://spacy.io/\">spaCy</a>, <a href=\"https://radimrehurek.com/gensim/\">Gensim</a>, and <a href=\"https://huggingface.co/\">Hugging Face</a>. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "<b>In this session</b>, \n",
    "\n",
    "you will learn about basics for the Natural Language Processing. In subsession **6.1**, we will extract useful information / facts (communication symbols) from tweets. In **6.2**, we will show how to implement a text preprocessing pipeline using XXX data at the end of which stands the document-term matrix that is ready for analysis (such as topic modeling). In **6.3**, we will deal with word and document similarities using similarity metrics and word/document embeddings (not the pretrained ones); also Zipf's Law.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<big><b>Reminder</b></big>\n",
    "    \n",
    "ONLY use pip unless there is no Conda option. Please make sure that ALL packages we need are installed. If you need further information how to install and import packages and libraries, please check out \n",
    "<a href=\"https://github.com/gesiscss/css_methods_python/blob/main/a_introduction/1_computing_environment.ipynb\"> Session 1: Computing environment </a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Extracting entities from tweet texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<big><b>Notes from the content document (remove after finalizing it)</b></big>\n",
    "    \n",
    "Extracting information from tweets; start with using regular expressions (Hovy has section; Ali has slides in summer school 2022) to extract tweets with certain properties (e.g., tweets with hashtags, tweets with URLs); demonstrate that it is important to remove punctuation at a certain step in the extraction process (certainly after URL extraction but before hashtag extraction); sensitize users about the order of operations; data: table of tweet texts of the 100 most retweeted tweets in the TweetsCOV19 dataset from session 2\n",
    "mentioned users\n",
    "hashtags\n",
    "urls\n",
    "named entities (with Spacy: https://www.analyticsvidhya.com/blog/2021/06/nlp-application-named-entity-recognition-ner-in-python-with-spacy/; or does gensim have anything?)\n",
    "lexicon-based sentiment (using spacy of pysenti? Or gensim?)\n",
    "emojis\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1. Extracting facts using regular expressions\n",
    "\n",
    "<a href=\"https://docs.python.org/3/howto/regex.html\">Regular expressions</a> (called regex, regexes, regex pattern, regexp, or REs) specify search patterns. Typical examples of regular expressions are the patterns for matching email addresses, phone numbers, and credit card numbers.\n",
    "\n",
    "Regular expressions are essentially a specialized programming language embedded in Python, and you can interact with regular expressions via the built-in `re` module in Python, which has some functions that match a string for a pattern:\n",
    "\n",
    "- `match()`\n",
    "- `search()`\n",
    "- `findall()`\n",
    "- `finditer()`\n",
    "\n",
    "Pattern... character set..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Pouria, can you please add here the image about regex functions, a nicer one maybe you can find online and cite it of course??? Add logos for the libraries, such as pandas, regex, spacy, gensim and if I forget anything, pls check out the notebook and add those logos like we did in other sessions. Thank you!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the top 500 retweeted tweets from the TweetsCOV19 dataset, which was introduced in [Session 2: Data handling and visualization](https://github.com/gesiscss/css_methods_python/blob/main/a_introduction/2_data_handling_and_visualization.ipynb). To read and practice with this data, we need to import neccessary libraries below. If you have some difficulties with importing/installing, please check out the [Session 1: Computing environment](https://github.com/gesiscss/css_methods_python/blob/main/a_introduction/1_computing_environment.ipynb) for further installation information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('./data/TweetsCOV19/top_500_retweeted_tweets.csv', encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1265465820995411973</td>\n",
       "      <td>This was me, and I want to make one thing clea...</td>\n",
       "      <td>257467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1266553959973445639</td>\n",
       "      <td>Mike Pence caught on hot mic delivering empty ...</td>\n",
       "      <td>135818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1258750892448387074</td>\n",
       "      <td>THE PANDEMIC IS STILL HAPPENING. THE PANDEMIC ...</td>\n",
       "      <td>88667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1263579286201446400</td>\n",
       "      <td>This just happened on live tv. Wow, what a dou...</td>\n",
       "      <td>82495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1266546753182056453</td>\n",
       "      <td>Mask on</td>\n",
       "      <td>66604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1265465820995411973  This was me, and I want to make one thing clea...   \n",
       "1  1266553959973445639  Mike Pence caught on hot mic delivering empty ...   \n",
       "2  1258750892448387074  THE PANDEMIC IS STILL HAPPENING. THE PANDEMIC ...   \n",
       "3  1263579286201446400  This just happened on live tv. Wow, what a dou...   \n",
       "4  1266546753182056453                                            Mask on   \n",
       "\n",
       "   retweets  \n",
       "0    257467  \n",
       "1    135818  \n",
       "2     88667  \n",
       "3     82495  \n",
       "4     66604  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start using `findall` function from regex to extract **urls, mentions, and hashtags** in tweets (i.e., the column of text in our dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['urls'] = tweets_df['text'].apply(lambda x: re.findall(\"http[s]*\\S+\", x))\n",
    "\n",
    "tweets_df['mentions'] = tweets_df['text'].apply(lambda x: re.findall(\"@([a-zA-Z0-9_]{1,50})\", x))\n",
    "# if you want to keep mention sign with mentioned string, use this following code\n",
    "#tweets_df['mentions'] = tweets_df['text'].apply(lambda x: re.findall(r'@\\w+ ?', x))\n",
    "\n",
    "tweets_df['hashtags'] = tweets_df['text'].apply(lambda x: re.findall(\"#([a-zA-Z0-9_]{1,50})\", x))\n",
    "# if you want to keep hashtag sign hashtag's string, use this following code\n",
    "#tweets_df['hashtags'] = tweets_df['text'].apply(lambda x: re.findall(r'#\\w+ ?', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>urls</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1265465820995411973</td>\n",
       "      <td>This was me, and I want to make one thing clea...</td>\n",
       "      <td>257467</td>\n",
       "      <td>[https://t.co/349TZijtD8]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1266553959973445639</td>\n",
       "      <td>Mike Pence caught on hot mic delivering empty ...</td>\n",
       "      <td>135818</td>\n",
       "      <td>[https://t.co/IduvGhiPwj]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1258750892448387074</td>\n",
       "      <td>THE PANDEMIC IS STILL HAPPENING. THE PANDEMIC ...</td>\n",
       "      <td>88667</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1263579286201446400</td>\n",
       "      <td>This just happened on live tv. Wow, what a dou...</td>\n",
       "      <td>82495</td>\n",
       "      <td>[https://t.co/dQKheEcCvb]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1266546753182056453</td>\n",
       "      <td>Mask on</td>\n",
       "      <td>66604</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1265465820995411973  This was me, and I want to make one thing clea...   \n",
       "1  1266553959973445639  Mike Pence caught on hot mic delivering empty ...   \n",
       "2  1258750892448387074  THE PANDEMIC IS STILL HAPPENING. THE PANDEMIC ...   \n",
       "3  1263579286201446400  This just happened on live tv. Wow, what a dou...   \n",
       "4  1266546753182056453                                            Mask on   \n",
       "\n",
       "   retweets                       urls mentions hashtags  \n",
       "0    257467  [https://t.co/349TZijtD8]       []       []  \n",
       "1    135818  [https://t.co/IduvGhiPwj]       []       []  \n",
       "2     88667                         []       []       []  \n",
       "3     82495  [https://t.co/dQKheEcCvb]       []       []  \n",
       "4     66604                         []       []       []  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **emoji** extraction, in addition to regex, we will use the library called emoji (if not installed before, please install it before running the following cell). This library helps us transform emojis into the related codes (i.e., texts). Once the emojis are converted to text, we apply the same logic applied so far with regex to find them. \n",
    "\n",
    "The full list of emojis and related codes is available here: https://unicode.org/emoji/charts/full-emoji-list.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>urls</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emoji</th>\n",
       "      <th>emoji_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1264986843948277760</td>\n",
       "      <td>People who say ‘well, he’s doing the best he c...</td>\n",
       "      <td>9033</td>\n",
       "      <td>[https://t.co/5POEhfB6vi]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[COVID]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1260425005483073538</td>\n",
       "      <td>This young woman was killed in her home for no...</td>\n",
       "      <td>9021</td>\n",
       "      <td>[https://t.co/JzPgOzm4Rm]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[BreonnaTaylor]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1259587972728533000</td>\n",
       "      <td>I be like “oh shit my mask” like I’m Batman or...</td>\n",
       "      <td>8994</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[😂, 😂]</td>\n",
       "      <td>[:face_with_tears_of_joy:, :face_with_tears_of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1266251584461090816</td>\n",
       "      <td>Really disappointed by @SAfridiOfficial‘s comm...</td>\n",
       "      <td>8984</td>\n",
       "      <td>[]</td>\n",
       "      <td>[SAfridiOfficial, narendramodi]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[🇮🇳]</td>\n",
       "      <td>[:India:]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1266728243236950018</td>\n",
       "      <td>Let's be clear about what's happening:\\n\\n→ Am...</td>\n",
       "      <td>8974</td>\n",
       "      <td>[]</td>\n",
       "      <td>[realDonaldTrump]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                               text  \\\n",
       "495  1264986843948277760  People who say ‘well, he’s doing the best he c...   \n",
       "496  1260425005483073538  This young woman was killed in her home for no...   \n",
       "497  1259587972728533000  I be like “oh shit my mask” like I’m Batman or...   \n",
       "498  1266251584461090816  Really disappointed by @SAfridiOfficial‘s comm...   \n",
       "499  1266728243236950018  Let's be clear about what's happening:\\n\\n→ Am...   \n",
       "\n",
       "     retweets                       urls                         mentions  \\\n",
       "495      9033  [https://t.co/5POEhfB6vi]                               []   \n",
       "496      9021  [https://t.co/JzPgOzm4Rm]                               []   \n",
       "497      8994                         []                               []   \n",
       "498      8984                         []  [SAfridiOfficial, narendramodi]   \n",
       "499      8974                         []                [realDonaldTrump]   \n",
       "\n",
       "            hashtags   emoji  \\\n",
       "495          [COVID]      []   \n",
       "496  [BreonnaTaylor]      []   \n",
       "497               []  [😂, 😂]   \n",
       "498               []    [🇮🇳]   \n",
       "499               []      []   \n",
       "\n",
       "                                            emoji_text  \n",
       "495                                                 []  \n",
       "496                                                 []  \n",
       "497  [:face_with_tears_of_joy:, :face_with_tears_of...  \n",
       "498                                          [:India:]  \n",
       "499                                                 []  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_emojis(text, return_codes=False):\n",
    "    # first turn emojis into related text code\n",
    "    text_de = emoji.demojize(text)\n",
    "    # second find all emojis text code\n",
    "    emojis_list_de = re.findall(r'(:[!_\\-\\w]+:)', text_de)\n",
    "    # reconvert text code to emojis\n",
    "    list_emoji = [emoji.emojize(x) for x in emojis_list_de]\n",
    "\n",
    "    if return_codes:\n",
    "        return emojis_list_de\n",
    "    else:\n",
    "        return list_emoji\n",
    "\n",
    "tweets_df['emoji'] = tweets_df['text'].apply(extract_emojis)\n",
    "tweets_df['emoji_text'] = tweets_df['text'].apply(extract_emojis, return_codes=True)\n",
    "\n",
    "tweets_df.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the final results from our extraction example and sort values according to mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>urls</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emoji</th>\n",
       "      <th>emoji_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>1258617080430997505</td>\n",
       "      <td>A Black New York State Senator (@zellnor4ny) a...</td>\n",
       "      <td>9151</td>\n",
       "      <td>[https://t.co/NoT8g4uAli]</td>\n",
       "      <td>[zellnor4ny, YourFavoriteASW]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>1266956300908363776</td>\n",
       "      <td>NEW: A volunteer on Kushner's coronavirus resp...</td>\n",
       "      <td>9327</td>\n",
       "      <td>[https://t.co/jvs2h4IfNQ]</td>\n",
       "      <td>[yabutaleb7]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1260559563972960256</td>\n",
       "      <td>Wow! The Front Page @washingtonpost Headline r...</td>\n",
       "      <td>11591</td>\n",
       "      <td>[]</td>\n",
       "      <td>[washingtonpost]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>1262940294305071104</td>\n",
       "      <td>it would appear that @vp was joking about carr...</td>\n",
       "      <td>11196</td>\n",
       "      <td>[https://t.co/hI9cO4lxcX]</td>\n",
       "      <td>[vp]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1261718681882693632</td>\n",
       "      <td>Very happy to present this unseen image of @ta...</td>\n",
       "      <td>10245</td>\n",
       "      <td>[https://t.co/3dzvynlUq3]</td>\n",
       "      <td>[tarak9999, DabbooRatnani]</td>\n",
       "      <td>[HappyBirthdayNTR, StayHomeStaySafe]</td>\n",
       "      <td>[😎, 📸, 🎉, 🙏🏼]</td>\n",
       "      <td>[:smiling_face_with_sunglasses:, :camera_with_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1256717572373913605</td>\n",
       "      <td>Update: Got her permission with a fuck yeah. T...</td>\n",
       "      <td>19289</td>\n",
       "      <td>[https://t.co/MqV0QJ0D8h]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1265624335898869760</td>\n",
       "      <td>Y'all, the mask goes OVER your nose.</td>\n",
       "      <td>19351</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1258599146522464256</td>\n",
       "      <td>Because if its Baghdad its okay for this to ha...</td>\n",
       "      <td>19457</td>\n",
       "      <td>[https://t.co/UdFy61zoT5]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1266343312304324608</td>\n",
       "      <td>I gotta be honest the worst looting I've ever ...</td>\n",
       "      <td>19527</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1258018688231407618</td>\n",
       "      <td>“Oh shit my mask.”</td>\n",
       "      <td>15036</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                               text  \\\n",
       "489  1258617080430997505  A Black New York State Senator (@zellnor4ny) a...   \n",
       "464  1266956300908363776  NEW: A volunteer on Kushner's coronavirus resp...   \n",
       "347  1260559563972960256  Wow! The Front Page @washingtonpost Headline r...   \n",
       "360  1262940294305071104  it would appear that @vp was joking about carr...   \n",
       "412  1261718681882693632  Very happy to present this unseen image of @ta...   \n",
       "..                   ...                                                ...   \n",
       "167  1256717572373913605  Update: Got her permission with a fuck yeah. T...   \n",
       "165  1265624335898869760               Y'all, the mask goes OVER your nose.   \n",
       "164  1258599146522464256  Because if its Baghdad its okay for this to ha...   \n",
       "163  1266343312304324608  I gotta be honest the worst looting I've ever ...   \n",
       "250  1258018688231407618                                 “Oh shit my mask.”   \n",
       "\n",
       "     retweets                       urls                       mentions  \\\n",
       "489      9151  [https://t.co/NoT8g4uAli]  [zellnor4ny, YourFavoriteASW]   \n",
       "464      9327  [https://t.co/jvs2h4IfNQ]                   [yabutaleb7]   \n",
       "347     11591                         []               [washingtonpost]   \n",
       "360     11196  [https://t.co/hI9cO4lxcX]                           [vp]   \n",
       "412     10245  [https://t.co/3dzvynlUq3]     [tarak9999, DabbooRatnani]   \n",
       "..        ...                        ...                            ...   \n",
       "167     19289  [https://t.co/MqV0QJ0D8h]                             []   \n",
       "165     19351                         []                             []   \n",
       "164     19457  [https://t.co/UdFy61zoT5]                             []   \n",
       "163     19527                         []                             []   \n",
       "250     15036                         []                             []   \n",
       "\n",
       "                                 hashtags          emoji  \\\n",
       "489                                    []             []   \n",
       "464                                    []             []   \n",
       "347                                    []             []   \n",
       "360                                    []             []   \n",
       "412  [HappyBirthdayNTR, StayHomeStaySafe]  [😎, 📸, 🎉, 🙏🏼]   \n",
       "..                                    ...            ...   \n",
       "167                                    []             []   \n",
       "165                                    []             []   \n",
       "164                                    []             []   \n",
       "163                                    []             []   \n",
       "250                                    []             []   \n",
       "\n",
       "                                            emoji_text  \n",
       "489                                                 []  \n",
       "464                                                 []  \n",
       "347                                                 []  \n",
       "360                                                 []  \n",
       "412  [:smiling_face_with_sunglasses:, :camera_with_...  \n",
       "..                                                 ...  \n",
       "167                                                 []  \n",
       "165                                                 []  \n",
       "164                                                 []  \n",
       "163                                                 []  \n",
       "250                                                 []  \n",
       "\n",
       "[500 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.sort_values(by='mentions', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final exercise, let's clean text from urls, hashtags, mentions, and emojis for further text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_links(text):\n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], ' ') # remove links\n",
    "    return text\n",
    "\n",
    "def clean_all_entities(text):\n",
    "    entity_prefixes = ['#', '@'] # remove hashtags and mentions\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "            text = emoji.replace_emoji(text, replace=\"!\") # remove emojis\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "tweets_df['cleaned_text'] = tweets_df['text'].apply(lambda x: clean_all_entities(clean_links(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tweet: Very happy to present this unseen image of @tarak9999 .. I hope you all like it 😎\n",
      "\n",
      "📸 By @DabbooRatnani \n",
      "\n",
      "#HappyBirthdayNTR 🎉\n",
      "\n",
      "#StayHomeStaySafe 🙏🏼 https://t.co/3dzvynlUq3\n",
      "\n",
      "\n",
      "Cleaned Tweet: Very happy to present this unseen image of I hope you all like it ! ! By ! !\n"
     ]
    }
   ],
   "source": [
    "print('Original Tweet:', tweets_df.text.values[412])\n",
    "print('\\n\\nCleaned Tweet:', tweets_df.cleaned_text.values[412])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2. Extracting named entities\n",
    "\n",
    "A named entity is a real-life object which can be identified and denoted with a proper name. Named Entities can be a place, person, organization, time, object, or geographic entity. For example, named entities would be Joe Biden, New York city, and congress. Named entities are usually instances of entity instances. For example, Joe Biden is an instance of a politician/person, New York City is an instance of a place, and congress is as instance of an organization. \n",
    "\n",
    "**Named Entity Recognition** (NER) is the process of NLP for identifying and classifying named entities. The raw and structured text are used to find out named entities, which are classified into persons, organizations, places, money, time, etc. NER systems are developed with various linguistic approaches, as well as statistical and machine learning methods. \n",
    "\n",
    "NER model first identifies an entity and then categorizes the entity into the most suitable class. Some of the common types of Named Entities will be as follows and others can be found in the further example of a Wikipedia page text.\n",
    "\n",
    "1. Organisations : NASA, CERN\n",
    "\n",
    "2. Places: Istanbul, Germany\n",
    "\n",
    "3. Money: 1 Billion Dollars, 50 Euros\n",
    "\n",
    "4. Date: 24th January 2023, season 4\n",
    "\n",
    "5. Person: Richard Feynman, George Floyd\n",
    " \n",
    "<img src='images/NER.png' style='height: 500px; float: left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Insight</b></big>\n",
    "\n",
    "    \n",
    "For NLP tasks like NER, POS tagging, dependency parsing, word vectors and more, <a href=\"https://spacy.io/\">spaCy</a> has distinct features that provide clear advantage for processing text data and modeling. It is the most trending and advanced free open-source library for implementing NLP in Python nowadays. \n",
    "    \n",
    "An important thing about NER models is that their ability to understand Named Entities depending on the data they have been trained on. There are many applications of NER. NER can be used for content classification, the various Named Entities of a text can be collected, and based on that data, the content themes can be understood.\n",
    "    \n",
    "We can use spaCy very easily for NER tasks. However, we need to consider training our own data for research, commercial, and business specific needs, the spaCy model generally performs well for all types of text data. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's import necessary libraries and packages and start with a toy example from our tweets dataframe, which is the second line of the text column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike Pence caught on hot mic delivering empty boxes of PPE for a PR stunt\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "# before loading it we need to install this module via: #!python -m spacy download en_core_web_sm\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Print the second tweet of our dataset\n",
    "raw_text = tweets_df.cleaned_text[1]\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we print the data on the Named Entities found in this raw text sample from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike Pence PERSON\n",
      "PPE ORG\n"
     ]
    }
   ],
   "source": [
    "# extract the entities using the spacy objects previously defined in the\n",
    "NER_text = NER(raw_text)\n",
    "\n",
    "# show all the entities extracted from the text\n",
    "for word in NER_text.ents:\n",
    "    print(word.text, word.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Insight</b></big>\n",
    "    \n",
    "Here, PPE is a context specific word to be labeled as organization. In the COVID-19 context like in our example, it stands for \"personal protective equipment\"; which is not an organization. On the other hand, as an abbreviation of the Philosophy, Politics, and Economics Society, PPE can be labeled as an organization.\n",
    "</div>  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run NER on the full dataset and find out the output with Named Entities and who is the most cited Person:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike Pence PERSON\n",
      "PPE ORG\n",
      "3 CARDINAL\n",
      "less than 30 CARDINAL\n",
      "season 2 DATE\n",
      "season 1 DATE\n",
      "2 years ago DATE\n",
      "one day DATE\n",
      "SHE ORG\n",
      "Costco ORG\n",
      "a few weeks ago DATE\n",
      "500 billion dollars MONEY\n",
      "1200 CARDINAL\n",
      "Baghdad GPE\n",
      "Giving GPE\n",
      "SNES Edition Nintendo Switch Just RETWEET WORK_OF_ART\n",
      "24 hours TIME\n",
      "Barclays ORG\n",
      "ER ORG\n",
      "Update FedEx ORG\n",
      "19 CARDINAL\n",
      "Trump ORG\n",
      "That’s Simply Not True RT WORK_OF_ART\n",
      "November DATE\n",
      "one CARDINAL\n",
      "NYPD ORG\n",
      "NYPD ORG\n",
      "Donald Trump PERSON\n",
      "69 CARDINAL\n",
      "Playbook PERSON\n",
      "the Pandemic Preparedness Office ORG\n",
      "75 CARDINAL\n",
      "today DATE\n",
      "Costco ORG\n",
      "World ORG\n",
      "CoronaVirus ORG\n",
      "China GPE\n",
      "American NORP\n",
      "COVID ORG\n",
      "today DATE\n",
      "Manhattan GPE\n",
      "Manhattan GPE\n",
      "911 CARDINAL\n",
      "1 4 DATE\n",
      "10 years ago DATE\n",
      "Mark Zuckerberg PERSON\n",
      "CNN ORG\n",
      "China GPE\n",
      "THIS WEEK DATE\n",
      "HOME Practice ORG\n",
      "Watermelon Sugar Video Out ORG\n",
      "China GPE\n",
      "Joe Biden PERSON\n",
      "the United States GPE\n",
      "decades DATE\n",
      "Candace Owens PERSON\n",
      "Michigan GPE\n",
      "Twitter PERSON\n",
      "today DATE\n",
      "HAIR ORG\n",
      "HAIR ORG\n",
      "Twitter PERSON\n",
      "the 13 year olds DATE\n",
      "Covid PERSON\n",
      "the China Virus Ventilators Testing Medical Supply Distribution ORG\n",
      "Governors NORP\n",
      "Peter Daszak PERSON\n",
      "British NORP\n",
      "American NORP\n",
      "PhD WORK_OF_ART\n",
      "2003 DATE\n",
      "60 Minutes TIME\n",
      "Two weeks ago DATE\n",
      "NIH ORG\n",
      "Mexico GPE\n",
      "CoronaVirus PRODUCT\n",
      "California GPE\n",
      "the Southern Border A Classic ORG\n",
      "Cinco de Mayo PERSON\n",
      "Taco ORG\n",
      "Tuesday DATE\n",
      "Mexican NORP\n",
      "The United States GPE\n",
      "Rigged Election WORK_OF_ART\n",
      "thousands CARDINAL\n",
      "Covid PERSON\n",
      "Scam ORG\n",
      "Today DATE\n",
      "Tulsa GPE\n",
      "May 31st 1921 DATE\n",
      "Tulsa GPE\n",
      "Oklahoma GPE\n",
      "99 years later DATE\n",
      "8 weeks DATE\n",
      "Congress ORG\n",
      "Hong Kong GPE\n",
      "China GPE\n",
      "The United States GPE\n",
      "Hong Kong GPE\n",
      "the United States GPE\n",
      "India GPE\n",
      "India GPE\n",
      "Dallas GPE\n",
      "KEEP ORG\n",
      "Please ORG\n",
      "the Bay Area LOC\n",
      "Trump Tower FAC\n",
      "Chicago GPE\n",
      "the White House ORG\n",
      "Mr Floyd PERSON\n",
      "democrat NORP\n",
      "democrat NORP\n",
      "democrat NORP\n",
      "democrat NORP\n",
      "republican NORP\n",
      "Trump ORG\n",
      "Coronavirus GPE\n",
      "the United States GPE\n",
      "REALLY ORG\n",
      "CoronaVirus ORG\n",
      "19 CARDINAL\n",
      "Wear PERSON\n",
      "new zealand GPE\n",
      "today DATE\n",
      "400 000 to QUANTITY\n",
      "450 CARDINAL\n",
      "HHS Covid ORG\n",
      "Coronavirus Monkeys PERSON\n",
      "19 CARDINAL\n",
      "Twitter PERSON\n",
      "Please ORG\n",
      "Psycho GPE\n",
      "Joe Scarborough PERSON\n",
      "George Floyd’s PERSON\n",
      "Queen’s Guard ORG\n",
      "2 CARDINAL\n",
      "America GPE\n",
      "Coronavirus LOC\n",
      "Federal Governments ORG\n",
      "RT one ORG\n",
      "this week DATE\n",
      "Obama GPE\n",
      "DNC ORG\n",
      "Langley GPE\n",
      "Concast and Fake News ORG\n",
      "GREAT PERSON\n",
      "China GPE\n",
      "Chinese NORP\n",
      "USA GPE\n",
      "China GPE\n",
      "Ukraine GPE\n",
      "Dems NORP\n",
      "LA GPE\n",
      "NYPD ORG\n",
      "a few months ago DATE\n",
      "Corona GPE\n",
      "Flynn PERSON\n",
      "Biden PERSON\n",
      "Family Dollar ORG\n",
      "Michigan GPE\n",
      "american NORP\n",
      "Karl Anthony Towns PERSON\n",
      "19 CARDINAL\n",
      "George Floyd Ca PERSON\n",
      "half CARDINAL\n",
      "the next three days DATE\n",
      "Americans NORP\n",
      "CoronaVirus ORG\n",
      "China GPE\n",
      "California GPE\n",
      "Mike Garcia PERSON\n",
      "Democrats NORP\n",
      "first ORDINAL\n",
      "many years DATE\n",
      "California GPE\n",
      "Republican NORP\n",
      "Tom Tiffany PERSON\n",
      "Democrat NORP\n",
      "BIG ORG\n",
      "Wisconsin GPE\n",
      "Two CARDINAL\n",
      "kdrama PERSON\n",
      "two CARDINAL\n",
      "the last 24 hours TIME\n",
      "Candy PERSON\n",
      "Black health ORG\n",
      "Healthcare ORG\n",
      "one CARDINAL\n",
      "George Floyd PERSON\n",
      "the Barclays Center in Brooklyn Pepper ORG\n",
      "Six feet QUANTITY\n",
      "2040 CARDINAL\n",
      "Donald Trump PERSON\n",
      "Live and Let Die WORK_OF_ART\n",
      "69 CARDINAL\n",
      "Trump PERSON\n",
      "Floridians NORP\n",
      "5 CARDINAL\n",
      "Corona ORG\n",
      "European NORP\n",
      "last weeks DATE\n",
      "half CARDINAL\n",
      "Donald Trump PERSON\n",
      "Breonna Taylor PERSON\n",
      "Louisville GPE\n",
      "Kenny Walker PERSON\n",
      "Kenny PERSON\n",
      "Twitter PRODUCT\n",
      "Adam Schiff PERSON\n",
      "the Russian Witch Hunt Plus Plus Plus ORG\n",
      "China GPE\n",
      "WHO ORG\n",
      "Gayle King PERSON\n",
      "A few months ago DATE\n",
      "Today DATE\n",
      "House ORG\n",
      "Republican NORP\n",
      "Democrats NORP\n",
      "Gov Ron DeSantis PERSON\n",
      "19 CARDINAL\n",
      "Florida GPE\n",
      "Gov Andrew Cuomo PERSON\n",
      "DeSantis LOC\n",
      "Cuomo PERSON\n",
      "Psycho GPE\n",
      "Joe Scarborough PERSON\n",
      "the National Guard ORG\n",
      "CoronaVirus ORG\n",
      "This week DATE\n",
      "Title 32 LAW\n",
      "mid August DATE\n",
      "States GPE\n",
      "TOTALLY UNACCEPTABLE ORG\n",
      "INHUMANE ORG\n",
      "America GPE\n",
      "Governors NORP\n",
      "George Floyd PERSON\n",
      "Tim Walz PERSON\n",
      "American NORP\n",
      "Project ‘Truth Or Dare ORG\n",
      "Corona GPE\n",
      "Trump ORG\n",
      "Fired ORG\n",
      "Stolen PPE PERSON\n",
      "Mail GPE\n",
      "Ballots GPE\n",
      "Ballots Whoever PERSON\n",
      "Likewise Social Media Clean ORG\n",
      "Avenger PERSON\n",
      "COVID ORG\n",
      "China GPE\n",
      "the United States GPE\n",
      "Europe LOC\n",
      "one day DATE\n",
      "Twitter PERSON\n",
      "3 months DATE\n",
      "PPE ORG\n",
      "Flynn PERSON\n",
      "Biden PERSON\n",
      "Trump ORG\n",
      "Obama ORG\n",
      "One CARDINAL\n",
      "Bishan GPE\n",
      "Singapore GPE\n",
      "USA GPE\n",
      "Robocops ORG\n",
      "PPE ORG\n",
      "CNN ORG\n",
      "all day DATE\n",
      "yesterday DATE\n",
      "Rand Paul PERSON\n",
      "Today DATE\n",
      "Greta Thunberg PERSON\n",
      "2nd ORDINAL\n",
      "Mike Pence PERSON\n",
      "PPE ORG\n",
      "3 CARDINAL\n",
      "less than 30 CARDINAL\n",
      "season 2 DATE\n",
      "season 1 DATE\n",
      "2 years ago DATE\n",
      "one day DATE\n",
      "SHE ORG\n",
      "Costco ORG\n",
      "a few weeks ago DATE\n",
      "500 billion dollars MONEY\n",
      "1200 CARDINAL\n",
      "Baghdad GPE\n",
      "Giving GPE\n",
      "SNES Edition Nintendo Switch Just RETWEET WORK_OF_ART\n",
      "24 hours TIME\n",
      "Barclays ORG\n",
      "ER ORG\n",
      "Update FedEx ORG\n",
      "19 CARDINAL\n",
      "Trump ORG\n",
      "That’s Simply Not True RT WORK_OF_ART\n",
      "November DATE\n",
      "one CARDINAL\n",
      "NYPD ORG\n",
      "NYPD ORG\n",
      "Donald Trump PERSON\n",
      "69 CARDINAL\n",
      "Playbook PERSON\n",
      "the Pandemic Preparedness Office ORG\n",
      "75 CARDINAL\n",
      "today DATE\n",
      "Costco ORG\n",
      "World ORG\n",
      "CoronaVirus ORG\n",
      "China GPE\n",
      "American NORP\n",
      "COVID ORG\n",
      "today DATE\n",
      "Manhattan GPE\n",
      "Manhattan GPE\n",
      "911 CARDINAL\n",
      "1 4 DATE\n",
      "10 years ago DATE\n",
      "Mark Zuckerberg PERSON\n",
      "CNN ORG\n",
      "China GPE\n",
      "THIS WEEK DATE\n",
      "HOME Practice ORG\n",
      "Watermelon Sugar Video Out ORG\n",
      "China GPE\n",
      "Joe Biden PERSON\n",
      "the United States GPE\n",
      "decades DATE\n",
      "Candace Owens PERSON\n",
      "Michigan GPE\n",
      "Twitter PERSON\n",
      "today DATE\n",
      "HAIR ORG\n",
      "HAIR ORG\n",
      "Twitter PERSON\n",
      "the 13 year olds DATE\n",
      "Covid PERSON\n",
      "the China Virus Ventilators Testing Medical Supply Distribution ORG\n",
      "Governors NORP\n",
      "Peter Daszak PERSON\n",
      "British NORP\n",
      "American NORP\n",
      "PhD WORK_OF_ART\n",
      "2003 DATE\n",
      "60 Minutes TIME\n",
      "Two weeks ago DATE\n",
      "NIH ORG\n",
      "Mexico GPE\n",
      "CoronaVirus PRODUCT\n",
      "California GPE\n",
      "the Southern Border A Classic ORG\n",
      "Cinco de Mayo PERSON\n",
      "Taco ORG\n",
      "Tuesday DATE\n",
      "Mexican NORP\n",
      "The United States GPE\n",
      "Rigged Election WORK_OF_ART\n",
      "thousands CARDINAL\n",
      "Covid PERSON\n",
      "Scam ORG\n",
      "Today DATE\n",
      "Tulsa GPE\n",
      "May 31st 1921 DATE\n",
      "Tulsa GPE\n",
      "Oklahoma GPE\n",
      "99 years later DATE\n",
      "8 weeks DATE\n",
      "Congress ORG\n",
      "Hong Kong GPE\n",
      "China GPE\n",
      "The United States GPE\n",
      "Hong Kong GPE\n",
      "the United States GPE\n",
      "India GPE\n",
      "India GPE\n",
      "Dallas GPE\n",
      "KEEP ORG\n",
      "Please ORG\n",
      "the Bay Area LOC\n",
      "Trump Tower FAC\n",
      "Chicago GPE\n",
      "the White House ORG\n",
      "Mr Floyd PERSON\n",
      "democrat NORP\n",
      "democrat NORP\n",
      "democrat NORP\n",
      "democrat NORP\n",
      "republican NORP\n",
      "Trump ORG\n",
      "Coronavirus GPE\n",
      "the United States GPE\n",
      "REALLY ORG\n",
      "CoronaVirus ORG\n",
      "19 CARDINAL\n",
      "Wear PERSON\n",
      "new zealand GPE\n",
      "today DATE\n",
      "400 000 to QUANTITY\n",
      "450 CARDINAL\n",
      "HHS Covid ORG\n",
      "Coronavirus Monkeys PERSON\n",
      "19 CARDINAL\n",
      "Twitter PERSON\n",
      "Please ORG\n",
      "Psycho GPE\n",
      "Joe Scarborough PERSON\n",
      "George Floyd’s PERSON\n",
      "Queen’s Guard ORG\n",
      "2 CARDINAL\n",
      "America GPE\n",
      "Coronavirus LOC\n",
      "Federal Governments ORG\n",
      "RT one ORG\n",
      "this week DATE\n",
      "Obama GPE\n",
      "DNC ORG\n",
      "Langley GPE\n",
      "Concast and Fake News ORG\n",
      "GREAT PERSON\n",
      "China GPE\n",
      "Chinese NORP\n",
      "USA GPE\n",
      "China GPE\n",
      "Ukraine GPE\n",
      "Dems NORP\n",
      "LA GPE\n",
      "NYPD ORG\n",
      "a few months ago DATE\n",
      "Corona GPE\n",
      "Flynn PERSON\n",
      "Biden PERSON\n",
      "Family Dollar ORG\n",
      "Michigan GPE\n",
      "american NORP\n",
      "Karl Anthony Towns PERSON\n",
      "19 CARDINAL\n",
      "George Floyd Ca PERSON\n",
      "half CARDINAL\n",
      "the next three days DATE\n",
      "Americans NORP\n",
      "CoronaVirus ORG\n",
      "China GPE\n",
      "California GPE\n",
      "Mike Garcia PERSON\n",
      "Democrats NORP\n",
      "first ORDINAL\n",
      "many years DATE\n",
      "California GPE\n",
      "Republican NORP\n",
      "Tom Tiffany PERSON\n",
      "Democrat NORP\n",
      "BIG ORG\n",
      "Wisconsin GPE\n",
      "Two CARDINAL\n",
      "kdrama PERSON\n",
      "two CARDINAL\n",
      "the last 24 hours TIME\n",
      "Candy PERSON\n",
      "Black health ORG\n",
      "Healthcare ORG\n",
      "one CARDINAL\n",
      "George Floyd PERSON\n",
      "the Barclays Center in Brooklyn Pepper ORG\n",
      "Six feet QUANTITY\n",
      "2040 CARDINAL\n",
      "Donald Trump PERSON\n",
      "Live and Let Die WORK_OF_ART\n",
      "69 CARDINAL\n",
      "Trump PERSON\n",
      "Floridians NORP\n",
      "5 CARDINAL\n",
      "Corona ORG\n",
      "European NORP\n",
      "last weeks DATE\n",
      "half CARDINAL\n",
      "Donald Trump PERSON\n",
      "Breonna Taylor PERSON\n",
      "Louisville GPE\n",
      "Kenny Walker PERSON\n",
      "Kenny PERSON\n",
      "Twitter PRODUCT\n",
      "Adam Schiff PERSON\n",
      "the Russian Witch Hunt Plus Plus Plus ORG\n",
      "China GPE\n",
      "WHO ORG\n",
      "Gayle King PERSON\n",
      "A few months ago DATE\n",
      "Today DATE\n",
      "House ORG\n",
      "Republican NORP\n",
      "Democrats NORP\n",
      "Gov Ron DeSantis PERSON\n",
      "19 CARDINAL\n",
      "Florida GPE\n",
      "Gov Andrew Cuomo PERSON\n",
      "DeSantis LOC\n",
      "Cuomo PERSON\n",
      "Psycho GPE\n",
      "Joe Scarborough PERSON\n",
      "the National Guard ORG\n",
      "CoronaVirus ORG\n",
      "This week DATE\n",
      "Title 32 LAW\n",
      "mid August DATE\n",
      "States GPE\n",
      "TOTALLY UNACCEPTABLE ORG\n",
      "INHUMANE ORG\n",
      "America GPE\n",
      "Governors NORP\n",
      "George Floyd PERSON\n",
      "Tim Walz PERSON\n",
      "American NORP\n",
      "Project ‘Truth Or Dare ORG\n",
      "Corona GPE\n",
      "Trump ORG\n",
      "Fired ORG\n",
      "Stolen PPE PERSON\n",
      "Mail GPE\n",
      "Ballots GPE\n",
      "Ballots Whoever PERSON\n",
      "Likewise Social Media Clean ORG\n",
      "Avenger PERSON\n",
      "COVID ORG\n",
      "China GPE\n",
      "the United States GPE\n",
      "Europe LOC\n",
      "one day DATE\n",
      "Twitter PERSON\n",
      "3 months DATE\n",
      "PPE ORG\n",
      "Flynn PERSON\n",
      "Biden PERSON\n",
      "Trump ORG\n",
      "Obama ORG\n",
      "One CARDINAL\n",
      "Bishan GPE\n",
      "Singapore GPE\n",
      "USA GPE\n",
      "Robocops ORG\n",
      "PPE ORG\n",
      "CNN ORG\n",
      "all day DATE\n",
      "yesterday DATE\n",
      "Rand Paul PERSON\n",
      "Today DATE\n",
      "Greta Thunberg PERSON\n",
      "2nd ORDINAL\n",
      "Twitter PERSON\n",
      "China GPE\n",
      "Left Democrat Party ORG\n",
      "Republicans NORP\n",
      "Conservatives NORP\n",
      "the United States Section GPE\n",
      "230 CARDINAL\n",
      "Congress ORG\n",
      "Summer semester DATE\n",
      "this year Fall DATE\n",
      "thousands CARDINAL\n",
      "CoronaVirus ORG\n",
      "Washington GPE\n",
      "Capital Hill ORG\n",
      "Monday DATE\n",
      "House ORG\n",
      "5 CARDINAL\n",
      "Abbott Test ORG\n",
      "Please ORG\n",
      "Brian P Monahan PERSON\n",
      "republicans NORP\n",
      "Minneapolis GPE\n",
      "Wednesday night TIME\n",
      "Louisville GPE\n",
      "7 CARDINAL\n",
      "last night TIME\n",
      "Memorial Day DATE\n",
      "this year DATE\n",
      "COVID ORG\n",
      "19 CARDINAL\n",
      "the months ahead DATE\n",
      "APD ORG\n",
      "Staunton Virginia GPE\n",
      "Obama PERSON\n",
      "3 CARDINAL\n",
      "Republican NORP\n",
      "10k DATE\n",
      "COVID ORG\n",
      "19 CARDINAL\n",
      "the Department of Justice ORG\n",
      "Flynn PERSON\n",
      "Donald Trump PERSON\n",
      "the weekend DATE\n",
      "The Fake WORK_OF_ART\n",
      "first ORDINAL\n",
      "Jaehyun Apologize For Itaewon Outing Confirm They Tested Negative For COVID ORG\n",
      "Brooklyn GPE\n",
      "21 MILLION CARDINAL\n",
      "Coronavirus PERSON\n",
      "Andrew Cuomo PERSON\n",
      "New York GPE\n",
      "thousands CARDINAL\n",
      "Dr Fauci PERSON\n",
      "Time ORG\n",
      "Keep PERSON\n",
      "Los Angeles GPE\n",
      "COVID ORG\n",
      "19 CARDINAL\n",
      "the Obama Admin LOC\n",
      "Playbook PERSON\n",
      "5 hours TIME\n",
      "LA County GPE\n",
      "the month of July DATE\n",
      "LA GPE\n",
      "Brock Turner PERSON\n",
      "8pm curfew QUANTITY\n",
      "Depression ORG\n",
      "Trump ORG\n",
      "Putin PERSON\n",
      "US GPE\n",
      "year 4 DATE\n",
      "Trump ORG\n",
      "HHS Whistleblower ORG\n",
      "LEAKED ORG\n",
      "American NORP\n",
      "17 CARDINAL\n",
      "Intelligence ORG\n",
      "19 CARDINAL\n",
      "Wuhan GPE\n",
      "Americans NORP\n",
      "two CARDINAL\n",
      "Mark Zuckerberg PERSON\n",
      "31 4 billion MONEY\n",
      "LA GPE\n",
      "Minnesota LOC\n",
      "2 weeks DATE\n",
      "2 months DATE\n",
      "50 years DATE\n",
      "WEAKER on ORG\n",
      "China GPE\n",
      "Joe Biden PERSON\n",
      "Covid PERSON\n",
      "Governors NORP\n",
      "Trump WORK_OF_ART\n",
      "54 billion CARDINAL\n",
      "SCOOP ORG\n",
      "congress ORG\n",
      "Flynn PERSON\n",
      "January 2017 DATE\n",
      "3 CARDINAL\n",
      "NSA ORG\n",
      "BREAKING German NORP\n",
      "January 21 DATE\n",
      "Xi Jinping PERSON\n",
      "WHO ORG\n",
      "Tedros PERSON\n",
      "CBS ORG\n",
      "Trump None ORG\n",
      "100 CARDINAL\n",
      "tens of millions MONEY\n",
      "the Republican Party ORG\n",
      "2020 5 31 DATE\n",
      "00PM TIME\n",
      "KST YouTube WorkpointOfficial ORG\n",
      "Thailand Thailand GPE\n",
      "Texans NORP\n",
      "April 2 DATE\n",
      "19 CARDINAL\n",
      "American NORP\n",
      "South Korea GPE\n",
      "two CARDINAL\n",
      "Americans NORP\n",
      "South Korea GPE\n",
      "Today DATE\n",
      "the White House FAC\n",
      "PPE ORG\n",
      "88 CARDINAL\n",
      "don PERSON\n",
      "Obama Over 11 million ORG\n",
      "First ORDINAL\n",
      "millions CARDINAL\n",
      "This year DATE\n",
      "Democrats NORP\n",
      "three 3 hours TIME\n",
      "minneapolis GPE\n",
      "Chicago GPE\n",
      "Number 10 PRODUCT\n",
      "March DATE\n",
      "Dominic Cummings ORG\n",
      "Trump Tough PERSON\n",
      "Last night TIME\n",
      "first ORDINAL\n",
      "CA House District ORG\n",
      "Republican NORP\n",
      "two decades DATE\n",
      "Flynn PERSON\n",
      "Mike Pence PERSON\n",
      "Ventilators ORG\n",
      "Dallas GPE\n",
      "Assemblywoman Diana Richardson PERSON\n",
      "Barclays GPE\n",
      "1 CARDINAL\n",
      "100 000 CARDINAL\n",
      "15 to 20 CARDINAL\n",
      "China GPE\n",
      "New Zealand GPE\n",
      "Jacinda Ardern PERSON\n",
      "The Effect of Chloroquine Hydroxychloroquine ORG\n",
      "Azithromycin PERSON\n",
      "SARS CoV 2 Infection Circulation Arrhythmia and Electrophysiology ORG\n",
      "Saudi NORP\n",
      "The Trump Administration ORG\n",
      "US GPE\n",
      "month DATE\n",
      "May DATE\n",
      "AG ORG\n",
      "CNN ORG\n",
      "Los Angeles GPE\n",
      "tonight TIME\n",
      "Trump ORG\n",
      "Biden PERSON\n",
      "1200 dollar MONEY\n",
      "Taxpayer PERSON\n",
      "Amnesty ORG\n",
      "The Democrat Party ORG\n",
      "Party ORG\n",
      "South Korea Make GPE\n",
      "300 CARDINAL\n",
      "JUNE next week DATE\n",
      "this year DATE\n",
      "n’t GPE\n",
      "the Marine Corps ORG\n",
      "Iraq GPE\n",
      "the age of 21 DATE\n",
      "100 CARDINAL\n",
      "the age of 25 DATE\n",
      "daily DATE\n",
      "Riot PERSON\n",
      "American NORP\n",
      "a Bill of Rights QUANTITY\n",
      "2020 DATE\n",
      "google ORG\n",
      "1 2 CARDINAL\n",
      "George Floyd PERSON\n",
      "Minneapolis GPE\n",
      "Ahmaud Arbery PERSON\n",
      "Amy Cooper PERSON\n",
      "America GPE\n",
      "Twitter Watch Party ORG\n",
      "next week DATE\n",
      "KINGSMAN ORG\n",
      "BTS ORG\n",
      "Drop me WORK_OF_ART\n",
      "One minute TIME\n",
      "Today DATE\n",
      "tomorrow DATE\n",
      "Dr Fauci PERSON\n",
      "NIH ORG\n",
      "Breakthrough Chloroquine PERSON\n",
      "19 CARDINAL\n",
      "today DATE\n",
      "Michigan GPE\n",
      "Americans NORP\n",
      "five CARDINAL\n",
      "Maine GPE\n",
      "State ORG\n",
      "Minneapolis GPE\n",
      "CBP ORG\n",
      "RESPECT ORG\n",
      "Atlanta GPE\n",
      "Niggas GPE\n",
      "DeathBed ORG\n",
      "STILL ORG\n",
      "NYC ORG\n",
      "Ohio GPE\n",
      "LA GPE\n",
      "Dallas GPE\n",
      "African American NORP\n",
      "Joe Biden PERSON\n",
      "late January DATE\n",
      "China GPE\n",
      "44 DATE\n",
      "American NORP\n",
      "the First Amendment LAW\n",
      "mosques FAC\n",
      "SAFELY ORG\n",
      "Americans NORP\n",
      "Covid PERSON\n",
      "19 CARDINAL\n",
      "England GPE\n",
      "John PERSON\n",
      "TTI ORG\n",
      "the United States GPE\n",
      "97 CARDINAL\n",
      "April 25 26 DATE\n",
      "2 CARDINAL\n",
      "4 CARDINAL\n",
      "Coronavirus PRODUCT\n",
      "NFL ORG\n",
      "George Floyd PERSON\n",
      "CIA ORG\n",
      "Martin Luther King Jr’s PERSON\n",
      "kau sorang PERSON\n",
      "Walaupun ORG\n",
      "Kunci ORG\n",
      "Amy Klobuchar PERSON\n",
      "Trump ORG\n",
      "Trump PERSON\n",
      "Trump ORG\n",
      "Trump ORG\n",
      "Dr Fauci PERSON\n",
      "dis patch ORG\n",
      "97 CARDINAL\n",
      "Korea GPE\n",
      "0 CARDINAL\n",
      "Beyoncé Nicki PERSON\n",
      "COVID ORG\n",
      "19 CARDINAL\n",
      "UK GPE\n",
      "55 000 CARDINAL\n",
      "one CARDINAL\n",
      "Americans NORP\n",
      "Newsnight WORK_OF_ART\n",
      "the Daily Mail ORG\n",
      "The UK Government ORG\n",
      "the Barr led Bureau of Prisons ORG\n",
      "Manafort PERSON\n",
      "COVID ORG\n",
      "Asian Americans NORP\n",
      "China GPE\n",
      "World Chinese NORP\n",
      "Americans NORP\n",
      "CDC ORG\n",
      "WHO ORG\n",
      "The National Institute of Allergy ORG\n",
      "1000s DATE\n",
      "YouTube ORG\n",
      "500 CARDINAL\n",
      "BREAKING Baltimore’s ORG\n",
      "Trump ORG\n",
      "tomorrow DATE\n",
      "Trump ORG\n",
      "Baltimore GPE\n",
      "Trump ORG\n",
      "33k DATE\n",
      "six CARDINAL\n",
      "KUN Mixed ORG\n",
      "KUN URL ORG\n",
      "Manila GPE\n",
      "Metro Manila GPE\n",
      "Please don WORK_OF_ART\n",
      "Linick PERSON\n",
      "Pompeo PERSON\n",
      "Senate ORG\n",
      "The Senate Foreign Relations Committee ORG\n",
      "The White House ORG\n",
      "the U S Secret Service ORG\n",
      "George Floyd A dozen PERSON\n",
      "COVID ORG\n",
      "Ayushman Bharat PERSON\n",
      "Indians NORP\n",
      "13 10 CARDINAL\n",
      "Barack Obama’s PERSON\n",
      "one CARDINAL\n",
      "hugo EVENT\n",
      "Jeff Bezos PERSON\n",
      "first ORDINAL\n",
      "Amazon ORG\n",
      "The Task Force ORG\n",
      "Vaccines NORP\n",
      "American NORP\n",
      "ITV News ORG\n",
      "Covid PERSON\n",
      "19 CARDINAL\n",
      "1 800 CARDINAL\n",
      "NHS ORG\n",
      "90 CARDINAL\n",
      "millions CARDINAL\n",
      "PPE ORG\n",
      "Susan Rice’s PERSON\n",
      "Obama GPE\n",
      "The White House ORG\n",
      "Trump ORG\n",
      "Pence ORG\n",
      "tonight TIME\n",
      "DEMOCRATS NORP\n",
      "Trump ORG\n",
      "Looting ORG\n",
      "less than four years DATE\n",
      "100k PRODUCT\n",
      "Americans NORP\n",
      "This week DATE\n",
      "Health ORG\n",
      "Johnson PERSON\n",
      "Northern Ireland Teachers LOC\n",
      "Twitter PRODUCT\n",
      "Iran GPE\n",
      "China GPE\n",
      "Russia GPE\n",
      "US GPE\n",
      "India GPE\n",
      "China GPE\n",
      "the United States GPE\n",
      "six year old DATE\n",
      "COVID19 PERSON\n",
      "Ventura Health Authorities This SHOCKING VIDEO ORG\n",
      "The Hindutva Supremacist Modi Govt ORG\n",
      "Nazi NORP\n",
      "Lebensraum Living Space WORK_OF_ART\n",
      "India GPE\n",
      "Bangladesh through Citizenship Act ORG\n",
      "Nepal GPE\n",
      "China GPE\n",
      "Pak NORP\n",
      "Michelle Obama PERSON\n",
      "Barack Obama PERSON\n",
      "the United States GPE\n",
      "night TIME\n",
      "Americans NORP\n",
      "every 45 seconds TIME\n",
      "19 CARDINAL\n",
      "RESPECTFUL PERSON\n",
      "2 CARDINAL\n",
      "Trump ORG\n",
      "thousands CARDINAL\n",
      "India GPE\n",
      "Japan GPE\n",
      "Britain GPE\n",
      "Canada GPE\n",
      "New Zealand GPE\n",
      "Indonesia GPE\n",
      "Russia GPE\n",
      "27 CARDINAL\n",
      "EU ORG\n",
      "Australia GPE\n",
      "19 CARDINAL\n",
      "US GPE\n",
      "4 CARDINAL\n",
      "NYPD ORG\n",
      "two CARDINAL\n",
      "May 2 2020 DATE\n",
      "5 30 PM TIME\n",
      "two inches QUANTITY\n",
      "CNBC ORG\n",
      "Roger A PERSON\n",
      "Republicans NORP\n",
      "Social Media Platforms ORG\n",
      "2016 DATE\n",
      "Mississippi GPE\n",
      "2020 CARDINAL\n",
      "Harry Azcrac PERSON\n",
      "Kushner ORG\n",
      "House ORG\n",
      "Oversight W PRODUCT\n",
      "The White House ORG\n",
      "Dr Fauci PERSON\n",
      "Congress ORG\n",
      "next week DATE\n",
      "Trump ORG\n",
      "65 600 CARDINAL\n",
      "6 CARDINAL\n",
      "6 CARDINAL\n",
      "one CARDINAL\n",
      "the Indian Ocean LOC\n",
      "Indian NORP\n",
      "19 CARDINAL\n",
      "Samosas GPE\n",
      "4th ORDINAL\n",
      "100 CARDINAL\n",
      "Mexicans NORP\n",
      "asians NORP\n",
      "summer DATE\n",
      "fed ORG\n",
      "one CARDINAL\n",
      "Healthcare ORG\n",
      "Belgium GPE\n",
      "Belgium GPE\n",
      "one CARDINAL\n",
      "2 months ago DATE\n",
      "One month ago DATE\n",
      "A few days ago DATE\n",
      "Today DATE\n",
      "Biden PERSON\n",
      "the Republican Party ORG\n",
      "1 Switch PRODUCT\n",
      "Coronavirus PRODUCT\n",
      "the Fake News They FAC\n",
      "America GPE\n",
      "House ORG\n",
      "US GPE\n",
      "Chinese Communist Party ORG\n",
      "House ORG\n",
      "Trump PERSON\n",
      "today DATE\n",
      "Democrat NORP\n",
      "China GPE\n",
      "Shame PERSON\n",
      "stephen hillenburg PERSON\n",
      "nickelodeon PERSON\n",
      "Left ORG\n",
      "1 CARDINAL\n",
      "Filipinos NORP\n",
      "one CARDINAL\n",
      "2 CARDINAL\n",
      "More than 11 CARDINAL\n",
      "CBN ORG\n",
      "19 CARDINAL\n",
      "Korea GPE\n",
      "thousands CARDINAL\n",
      "Americans NORP\n",
      "millions CARDINAL\n",
      "Six months from today DATE\n",
      "China GPE\n",
      "GOI ORG\n",
      "India GPE\n",
      "Black New York State ORG\n",
      "Black NYS Assemblywoman NORP\n",
      "Brooklyn GPE\n",
      "today DATE\n",
      "Please ORG\n",
      "Please don WORK_OF_ART\n",
      "don PERSON\n",
      "Sigh Sending PERSON\n",
      "George Floyd’s PERSON\n",
      "George PERSON\n",
      "Americans NORP\n",
      "CRY ORG\n",
      "Apprentice ORG\n",
      "Joe Scarborough PERSON\n",
      "5 years DATE\n",
      "New York GPE\n",
      "Trump ORG\n",
      "Justice ORG\n",
      "I’m Batman PRODUCT\n",
      "our Hon’b PM TIME\n",
      "Indian NORP\n",
      "Jai Hind PERSON\n",
      "America GPE\n",
      "Senate ORG\n",
      "DC GPE\n",
      "Dems NORP\n",
      "RT GPE\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweets_df.cleaned_text:\n",
    "    for word in NER(tweet).ents:\n",
    "        print(word.text, word.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Text representation: Implementing a preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<big><b>Notes from the content document (remove after finalizing it):</b></big>\n",
    "    \n",
    "\n",
    "Text representation: creating the document-term matrix; creating the preprocessing pipeline (using Spacy; for inspiration: https://mahadev001.github.io/Mahadev-Upadhyayula/Sentiment%20Analysis%20via%20NLP/Sentiment%20Analysis%20using%20NLP%20with%20Spacy%20and%20%20SVM.html); output should be a sparse matrix for use in ML\n",
    "Tokenization\n",
    "Stopword removal\n",
    "Stemming vs lemmatization\n",
    "POS tagging\n",
    "N-gram detection (mention that results differ depending on which position in the pipeline it’s performed: before/after stopword removal or lemmatization)\n",
    "tf*idf\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Pouria, can you please add this link to the references?\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1. Word Descriptors\n",
    "\n",
    "To refer to the entire collection of documents/observations, we use the word corpus (plural corpora). The raw text data often referred to as *text corpus* has punctuations, suffices, and stop words that do not give us important information. To have more useful information for NLP tasks, Text Preprocessing involves preparing the text corpus. Let's start with basic terminology of NLP.\n",
    "\n",
    "### Tokens and splitting \n",
    "\n",
    "The set of all the unique terms in our data is called the vocabulary. Each element in this set is called a type. Each occurrence of a type in the data is called a token. \n",
    "\n",
    "Let's practice: Our sentence is\n",
    "\n",
    ">“Today is a great day with learning NLP, such a power tool!”\n",
    "\n",
    "Thi sentece has 14 tokens but only 13 types (namely, 'Today', 'is', 'a', 'great', 'day', 'with', 'learning', 'NLP', ',', 'such', 'a', 'powerful', 'tool', '!'). Note that types can also include punctuation marks and multiword expressions.\n",
    "\n",
    "In other words, the words of a text document/file separated by spaces and punctuation are called as tokens.\n",
    "\n",
    "#### What is a Tokenization?\n",
    "The process of extracting tokens from a text file/document is referred as tokenization. Let's see an example below of a tokenization process using spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Mike Pence caught on hot mic delivering empty boxes of PPE for a PR stunt\n",
      "\n",
      "Tokens in the text:\n",
      "\t Mike\n",
      "\t Pence\n",
      "\t caught\n",
      "\t on\n",
      "\t hot\n",
      "\t mic\n",
      "\t delivering\n",
      "\t empty\n",
      "\t boxes\n",
      "\t of\n",
      "\t PPE\n",
      "\t for\n",
      "\t a\n",
      "\t PR\n",
      "\t stunt\n",
      "\n",
      "Total tokens: 15\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# As a text example we will use a tweet from the previous dataset\n",
    "text = tweets_df.cleaned_text[1]\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print the original and tokenized text\n",
    "print('Original text:', text)\n",
    "print('\\nTokens in the text:',)\n",
    "\n",
    "for token in doc:\n",
    "    print('\\t', token.text)\n",
    "\n",
    "print('\\nTotal tokens:', len(doc))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also push furhter our analysis and extract the vocabulary from the corpus of tweets from the previous dataset. Since the vocabulary of a text corpus is the collection of unique tokens present in that corpus, we will just need to tokenize each single tweet and keep unique occurence of each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 492, 'to': 399, 'a': 332, 'and': 297, 'of': 234, 'in': 212, 'is': 195, 'I': 163, 'you': 141, 'that': 135, 'for': 135, 'on': 124, 'this': 120, 'it': 108, 'with': 99, 'n’t': 96, '’s': 90, 'have': 86, 'are': 82, 'we': 76, 'all': 72, 'was': 71, 'The': 71, '!': 70, 'be': 68, 'they': 67, 'do': 64, 'from': 62, 'about': 60, 'his': 59, '“': 57, '”': 57, 'mask': 56, 'my': 55, 'me': 54, 'pandemic': 52, 'will': 52, 'so': 51, 'amp': 51, 'people': 50, 'not': 49, 'but': 49, 'This': 48, 'has': 48, 'like': 46, 'just': 44, 'their': 44, 'by': 44, 'our': 43, 'out': 42, 'at': 42, 'can': 41, 'A': 41, 'Trump': 41, 's': 40, 'We': 40, 'as': 39, '19': 38, 'an': 38, 'up': 37, 'he': 37, 'when': 36, 'COVID': 36, 'one': 35, 'what': 35, 'happened': 33, 'your': 33, 'who': 33, 'It': 33, 'or': 32, 'China': 32, 'would': 32, 'here': 31, 'happen': 31, 'him': 31, 'them': 31, 'if': 30, 'back': 30, 'her': 29, 'had': 29, 'no': 28, 'make': 27, 'being': 27, 'He': 26, 'than': 26, 't': 26, 'They': 26, 'did': 25, 'because': 25, 'us': 25, 'should': 25, 'If': 24, 'over': 24, 'time': 24, 'been': 24, 'now': 22, 'know': 22, 'home': 21, 'When': 21, '2': 21, 'during': 21, 'made': 21, 'What': 21, 'take': 20, 'off': 20, 'very': 20, 'social': 20, 'think': 20, 'wearing': 19, 'THE': 19, 'Please': 19, 'down': 19, 'need': 19, 'were': 19, 'coronavirus': 19, 'PANDEMIC': 18, 'IS': 18, 'STILL': 18, 'i': 18, 'more': 18, 'never': 18, 'States': 18, 'ca': 18, 'how': 18, 'happening': 18, 'video': 17, 'HAPPENING': 17, 'quarantine': 17, 'only': 17, 'she': 17, 'does': 17, 'And': 17, 'police': 17, 'President': 17, 'happens': 16, '’m': 16, 'called': 16, '’re': 16, 'United': 16, 'House': 16, 'face': 15, 'get': 15, 'everyone': 15, 'still': 15, 'ago': 15, 'wear': 15, 'after': 15, 'going': 15, 'today': 15, 'there': 15, 'into': 15, 'things': 15, 'want': 14, 'way': 14, 'while': 14, 'see': 14, 'even': 14, 'could': 14, 'good': 14, 'shit': 14, 'Floyd': 14, 'Obama': 14, 'let': 14, 'why': 14, 'thing': 13, 'trying': 13, 'Today': 13, 'distancing': 13, 'Biden': 13, 'go': 13, 'Twitter': 13, 'state': 13, 'Covid': 13, 'great': 13, 'George': 13, 'which': 13, 'Americans': 13, 'You': 13, 'country': 13, 'got': 12, 'workers': 12, 'CoronaVirus': 12, 'American': 12, 'where': 12, 'work': 12, 'media': 12, 'In': 12, 'before': 12, 'some': 12, 'many': 12, 'told': 11, 'then': 11, 'PPE': 11, 'bag': 11, '1': 11, 'years': 11, 'day': 11, 'keep': 11, 've': 11, 'few': 11, 'am': 11, 're': 11, 'asked': 11, 'middle': 11, 'look': 11, 'nothing': 11, 'BREAKING': 11, 'world': 11, '000': 11, 'every': 11, 'those': 11, 'help': 11, 'health': 11, 'said': 11, 'care': 11, 'immediately': 10, 'support': 10, 'year': 10, 'seen': 10, 'its': 10, 'protest': 10, 'lives': 10, 'But': 10, 'say': 10, 'black': 10, 'man': 10, 'again': 10, 'yet': 10, 'bad': 10, 'right': 10, 'lot': 10, 'coming': 10, 'People': 10, 'last': 10, 'might': 10, 'Coronavirus': 10, 'please': 10, 'don': 10, 'months': 10, 'job': 10, 'response': 10, 'came': 9, 'watch': 9, 'government': 9, 'weeks': 9, 'left': 9, 'check': 9, 'someone': 9, 'fuck': 9, 'pepper': 9, 'masks': 9, 'stay': 9, 'office': 9, 'woman': 9, 'public': 9, 'YOU': 9, 'Joe': 9, 'healthcare': 9, 'business': 9, 'doing': 9, 'White': 9, 'different': 9, 'also': 9, 'beat': 9, 'went': 9, 'lost': 9, 'tested': 9, 'Dr': 9, 'clear': 8, 'lies': 8, 'protests': 8, 'feel': 8, 'safe': 8, 'show': 8, 'looting': 8, 'making': 8, 'matter': 8, 'any': 8, 'essential': 8, 'covid': 8, 'protesting': 8, 'gas': 8, 'saw': 8, 'better': 8, 'As': 8, 'cases': 8, '4': 8, 'use': 8, 'India': 8, 'My': 8, 'cover': 8, 'death': 8, 'democrat': 8, 'enough': 8, 'news': 8, 'lockdown': 8, 'anyone': 8, 'story': 8, 'America': 8, 'week': 8, 'Flynn': 8, 'Republican': 8, 'testing': 8, 'free': 8, 'president': 8, 'starts': 8, 'Thank': 8, '’': 8, 'Party': 8, 'other': 8, 'live': 7, 'full': 7, 'equipped': 7, '3': 7, 'hope': 7, 'pay': 7, 'Now': 7, 'outside': 7, 'both': 7, 'white': 7, 'without': 7, 'RT': 7, 'w': 7, 'NYPD': 7, 'Donald': 7, 'system': 7, 'Me': 7, 'life': 7, 'protect': 7, 'done': 7, 'until': 7, 'telling': 7, 'Governors': 7, 'killed': 7, 'fire': 7, 'sad': 7, 'pulled': 7, 'under': 7, 'well': 7, 'store': 7, 'read': 7, 'So': 7, 'administration': 7, 'put': 7, 'come': 7, 'long': 7, 'two': 7, 'says': 7, '5': 7, 'crisis': 7, 'These': 7, 'stopped': 7, '‘': 7, 'most': 7, 'much': 7, 'Fauci': 7, 'against': 7, 'Can': 7, 'NOT': 6, 'officers': 6, 'SHE': 6, 'AND': 6, 'GOT': 6, 'school': 6, 'worst': 6, 'place': 6, 'Just': 6, 'hours': 6, 'race': 6, 'especially': 6, 'That': 6, 'able': 6, 'tear': 6, 'phone': 6, 'literally': 6, 'global': 6, 'pregnant': 6, 'vs': 6, 'THIS': 6, 'DO': 6, 'disinformation': 6, 'win': 6, 'continue': 6, 'demanding': 6, 'nt': 6, 'handling': 6, 'Ventilators': 6, 'Testing': 6, 'spent': 6, 'California': 6, 'built': 6, 'Ballots': 6, 'history': 6, 'thousands': 6, 'Also': 6, 'Some': 6, 'city': 6, 'Black': 6, 'friends': 6, 'number': 6, 'Our': 6, 'died': 6, 'new': 6, 'give': 6, 'these': 6, 'patients': 6, 'parents': 6, 'high': 6, 'taken': 6, 'Media': 6, 'LA': 6, 'Corona': 6, 'power': 6, 'shot': 6, 'next': 6, '100': 6, 'must': 6, 'sure': 6, 'dead': 6, 'officials': 6, 'Plus': 6, 'propaganda': 6, 'looking': 6, 'through': 6, 'order': 6, 'top': 6, 'doctors': 6, '—': 6, 'homes': 6, 'cities': 6, '’ve': 6, 'saying': 6, 'open': 6, 'Mike': 5, 'IN': 5, 'end': 5, 'dollars': 5, 'having': 5, 'fun': 5, 'possible': 5, 'hard': 5, 'Barclays': 5, '–': 5, 'evidence': 5, 'Not': 5, 'family': 5, 'members': 5, 'young': 5, 'hands': 5, 'Quarantine': 5, 'Pandemic': 5, 'wanted': 5, 'All': 5, 'World': 5, 'Very': 5, 'CNN': 5, 'TO': 5, 'Michigan': 5, 'question': 5, 'BUT': 5, 'big': 5, 'Mail': 5, 'later': 5, '’ll': 5, 'given': 5, 'facts': 5, 'riots': 5, 'car': 5, 'flipped': 5, 'person': 5, 'rappers': 5, 'An': 5, 'important': 5, 'fast': 5, 'act': 5, 'words': 5, 'Psycho': 5, 'Scarborough': 5, 'hit': 5, 'die': 5, 'bitch': 5, 'News': 5, 'Do': 5, 'General': 5, 'killing': 5, 'due': 5, 'flags': 5, 'National': 5, 'days': 5, 'Trade': 5, 'taking': 5, 'Democrats': 5, 'first': 5, 'Democrat': 5, 'far': 5, 'too': 5, 'become': 5, 'fight': 5, 'states': 5, 'used': 5, 'dropped': 5, 'No': 5, 'information': 5, 'Cuomo': 5, 'orders': 5, 'control': 5, 'real': 5, 'meet': 5, 'body': 5, 'New': 5, 'best': 5, 'gt': 5, 'US': 5, 'Korea': 5, 'm': 5, 'spit': 4, 'threw': 4, 'Pence': 4, 'boxes': 4, 'stunt': 4, 'Wow': 4, 'ok': 4, 'continuing': 4, 'mid': 4, 'takes': 4, 'season': 4, 'remember': 4, 'flatten': 4, 'curve': 4, 'water': 4, 'AN': 4, 'ELECTED': 4, 'OFFICIAL': 4, 'continues': 4, 'active': 4, 'shooter': 4, 'drills': 4, 'Costco': 4, 'honest': 4, 'ever': 4, 'billion': 4, 'money': 4, 'decide': 4, 'Because': 4, 'okay': 4, 'little': 4, 'away': 4, '24': 4, 'Update': 4, 'cop': 4, 'She': 4, 'serious': 4, 'seizure': 4, 'mail': 4, 'peacefully': 4, 'sprays': 4, '69': 4, 'page': 4, 'spend': 4, 'wire': 4, 'miscarriage': 4, 'Manhattan': 4, '10': 4, 'website': 4, 'appearance': 4, 'case': 4, 'FAKE': 4, 'wild': 4, 'massive': 4, 'campaign': 4, 'Sleepy': 4, 'along': 4, 'HAIR': 4, 'allow': 4, 'justice': 4, 'mature': 4, 'internet': 4, 'human': 4, 'Great': 4, 'Two': 4, 'Border': 4, 'Wall': 4, 'falls': 4, 'beer': 4, 'force': 4, 'sign': 4, 'names': 4, 'necessary': 4, 'Tulsa': 4, 'May': 4, 'attacked': 4, 'set': 4, 'known': 4, 'Congress': 4, 'Hong': 4, 'Kong': 4, 'longer': 4, 'stand': 4, 'Dallas': 4, 'large': 4, 'sword': 4, 'charged': 4, 'Wear': 4, 'arrested': 4, 'wo': 4, 'protesters': 4, 'march': 4, 'ready': 4, 'already': 4, 'throughout': 4, 'inside': 4, 'forever': 4, 'sippin': 4, 'fine': 4, 'Plague': 4, 'individual': 4, 'damn': 4, 'rules': 4, 'almost': 4, 'normal': 4, 'citizens': 4, 'Government': 4, 'attacking': 4, 'wrote': 4, 'head': 4, 'Police': 4, 'murder': 4, 'Guard': 4, 'MASK': 4, 'Remember': 4, 'Governor': 4, 'happy': 4, 'Federal': 4, 'kids': 4, 'such': 4, 'Fake': 4, 'Chinese': 4, 'USA': 4, 'name': 4, 'Dems': 4, 'NOW': 4, 'small': 4, 'families': 4, 'fair': 4, 'sprayed': 4, 'heard': 4, 'P': 4, 'Anyone': 4, 'political': 4, 'father': 4, 'half': 4, 'memory': 4, 'Congressional': 4, 'seat': 4, 'mom': 4, 'charge': 4, 'Candy': 4, 'talking': 4, 'Brooklyn': 4, 'several': 4, 'Let': 4, '’d': 4, 'nation': 4, 'broke': 4, 'Kenny': 4, 'others': 4, 'WHO': 4, 'gave': 4, 'positive': 4, 'quarantined': 4, 'serve': 4, 'doubt': 4, 'press': 4, 'Gov': 4, 'DeSantis': 4, 'handled': 4, 'using': 4, 'fighting': 4, 'extend': 4, 'keeping': 4, 'roads': 4, 'traffic': 4, 'soon': 4, 'kind': 4, 'Truth': 4, 'Or': 4, 'Dare': 4, 'virus': 4, 'lied': 4, 'doesn': 4, 'moment': 4, 'corruption': 4, 'How': 4, 'spread': 4, 'disgrace': 4, 'took': 4, 'knew': 4, 'One': 4, 'since': 4, 'qualified': 4, 'mean': 4, 'working': 4, 'There': 4, 'everything': 4, 'night': 4, 'means': 4, 'together': 4, 'For': 4, 'Why': 4, 'nursing': 4, 'stop': 4, 'love': 4, 'list': 4, 'millions': 4, '2020': 4, 'shut': 4, 'fully': 4, 'nurses': 4, 'More': 4, 'illegal': 4, 'lying': 4, 'CAN': 4, 'PM': 4, 'something': 4, 'empty': 3, 'douche': 3, 'simply': 3, 'less': 3, '30': 3, 'watched': 3, 'children': 3, 'corporations': 3, '500': 3, 'else': 3, '1200': 3, 'dollar': 3, 'Y': 3, 'goes': 3, 'Looking': 3, 'Switch': 3, 'pick': 3, 'fired': 3, 'vote': 3, 'officer': 3, 'bullets': 3, 'forgot': 3, 'hospital': 3, 'peace': 3, 'Playbook': 3, 'cut': 3, 'retweet': 3, 'ripped': 3, 'child': 3, 'bored': 3, 'tried': 3, 'call': 3, 'Mark': 3, 'Zuckerberg': 3, 'democracy': 3, 'celebrities': 3, 'twitter': 3, 'NEWS': 3, 'rip': 3, 'decades': 3, 'residents': 3, 'agree': 3, 'Imagine': 3, 'DID': 3, '13': 3, 'science': 3, 'ask': 3, 'credit': 3, 'Most': 3, 'helped': 3, 'NIH': 3, 'funding': 3, 'OK': 3, 'looted': 3, 'suddenly': 3, 'bit': 3, '8': 3, 'allowed': 3, 'war': 3, 'stands': 3, 'proud': 3, 'vaccine': 3, 'attempted': 3, 'shop': 3, 'ran': 3, 'rioters': 3, 'clearly': 3, 'floor': 3, 'bring': 3, 'Chicago': 3, 'point': 3, 'Mr': 3, 'mayor': 3, 'governor': 3, 'Get': 3, 'strongly': 3, 'trending': 3, 'probably': 3, 'deep': 3, 'pretend': 3, 'selfish': 3, 'situation': 3, 'reopen': 3, 'crazy': 3, 'entire': 3, 'Last': 3, 'HHS': 3, 'honor': 3, 'lab': 3, 'rollercoaster': 3, 'colleagues': 3, 'myself': 3, 'share': 3, 'reality': 3, 'Read': 3, 'grades': 3, 'stood': 3, 'guns': 3, 'numbers': 3, 'success': 3, 'GREAT': 3, 'actually': 3, 'scandal': 3, 'ai': 3, 'old': 3, 'boy': 3, 'Mayor': 3, 'Declassified': 3, 'V': 3, 'unmasking': 3, 'Bro': 3, 'three': 3, 'dealing': 3, 'barely': 3, 'Deals': 3, 'innocent': 3, 'Big': 3, 'Oh': 3, 'protested': 3, 'medical': 3, 'insurance': 3, 'Nothing': 3, 'date': 3, 'educated': 3, 'Healthcare': 3, 'spray': 3, 'air': 3, 'Six': 3, 'notes': 3, 'oh': 3, 'narrative': 3, 'gon': 3, 'na': 3, 'responsible': 3, 'economic': 3, 'Louisville': 3, 'anything': 3, 'targeted': 3, 'King': 3, 'post': 3, 'biggest': 3, 'another': 3, 'including': 3, 'Andrew': 3, 'ratings': 3, 'Cold': 3, 'Case': 3, 'knows': 3, 'Nobody': 3, 'stupid': 3, 'men': 3, 'women': 3, 'efforts': 3, 'alive': 3, 'saved': 3, 'shooting': 3, 'grocery': 3, 'Act': 3, 'part': 3, 'thanks': 3, 'scientists': 3, 'leadership': 3, 'Country': 3, 'Whoever': 3, 'Likewise': 3, 'Social': 3, 'professionals': 3, 'may': 3, 'currently': 3, 'instead': 3, 'pain': 3, 'upset': 3, 'calls': 3, 'DOJ': 3, 'declassified': 3, 'gear': 3, 'garbage': 3, 'bed': 3, 'talk': 3, 'low': 3, 'service': 3, 'jobs': 3, 'Left': 3, 'students': 3, 'kill': 3, 'Minneapolis': 3, 'vital': 3, 'Here': 3, 'same': 3, 'once': 3, '21': 3, 'York': 3, 'policies': 3, 'month': 3, 'comes': 3, 'getting': 3, 'January': 3, 'nigga': 3, 'Health': 3, 'worse': 3, '6': 3, 'South': 3, 'countries': 3, 'explain': 3, 'angry': 3, 'million': 3, 'First': 3, 'burning': 3, 'truly': 3, 'save': 3, 'around': 3, 'Pelosi': 3, 'illegals': 3, 'God': 3, 'yourself': 3, 'While': 3, 'facing': 3, '…': 3, 'believe': 3, 'front': 3, 'witnesses': 3, 'plan': 3, 'always': 3, 'received': 3, 'Without': 3, 'govt': 3, 'asking': 3, 'message': 3, 'Senate': 3, 'border': 3, 'demand': 3, 'SO': 3, 'Indian': 3, 'tell': 3, 'respect': 3, '→': 3, 'commenting': 2, 'below': 2, 'street': 2, 'caught': 2, 'hot': 2, 'mic': 2, 'delivering': 2, 'PR': 2, 'tv': 2, 'Mask': 2, 'receipts': 2, 'Attending': 2, 'Whether': 2, 'comfortable': 2, 'ways': 2, 'min': 2, 'priests': 2, 'pistols': 2, 'ATTACKED': 2, 'BY': 2, 'POLICE': 2, 'AGAIN': 2, 'WORKS': 2, 'YHE': 2, 'GOVERNMENT': 2, 'PEPPER': 2, 'SPRAYED': 2, 'DURING': 2, 'PEACEFUL': 2, 'PROTEST': 2, 'someday': 2, 'Quote': 2, 'expect': 2, 'elementary': 2, 'endure': 2, 'trauma': 2, 'freedoms': 2, 'ta': 2, 'collected': 2, 'stimulus': 2, 'food': 2, 'rent': 2, 'Baghdad': 2, 'OVER': 2, 'nose': 2, 'Alright': 2, 'international': 2, 'giveaway': 2, 'Giving': 2, 'customized': 2, 'SNES': 2, 'Edition': 2, 'Nintendo': 2, 'RETWEET': 2, 'enter': 2, 'll': 2, 'random': 2, 'winner': 2, 'Shoutout': 2, 'Got': 2, 'permission': 2, 'yeah': 2, 'pushed': 2, 'flung': 2, 'tiny': 2, 'ER': 2, 'waiting': 2, 'updates': 2, 'wait': 2, 'sister': 2, 'thoughts': 2, 'FedEx': 2, 'reposting': 2, 'disrespect': 2, 'putting': 2, 'jeopardy': 2, 'Voting': 2, 'secure': 2, 'Especially': 2, 'lead': 2, 'voter': 2, 'fraud': 2, 'voting': 2, 'Simply': 2, 'True': 2, 'safely': 2, 'November': 2, 'heartbroken': 2, 'disgusted': 2, 'pulls': 2, 'cc': 2, 'unlimited': 2, 'vans': 2, 'needed': 2, 'gofundme': 2, 'tired': 2, 'Anyhow': 2, 'trump': 2, 'hydrated': 2, 'ignored': 2, 'Preparedness': 2, 'Office': 2, 'abolished': 2, 'monitoring': 2, 'PREDICT': 2, '75': 2, 'didn': 2, 'childhood': 2, 'barbed': 2, 'enclosed': 2, 'internment': 2, 'camps': 2, 'listen': 2, 'grown': 2, 'adults': 2, 'cry': 2, 'oppression': 2, 'protester': 2, 'cropped': 2, 'hijab': 2, 'kicked': 2, 'maced': 2, 'considered': 2, 'heroes': 2, 'gift': 2, 'marches': 2, 'choking': 2, 'parallel': 2, 'universe': 2, 'doppelganger': 2, 'therefore': 2, 'Happened': 2, 'defender': 2, 'decade': 2, 'DA': 2, 'uses': 2, 'hysterical': 2, '911': 2, 'categorical': 2, 'guilt': 2, 'Usually': 2, 'refute': 2, 'created': 2, 'rank': 2, 'female': 2, 'classmates': 2, 'ended': 2, 'destroying': 2, 'wondering': 2, 'consequences': 2, 'toxic': 2, 'masculinity': 2, 'ppl': 2, 'brutality': 2, 'MSDNC': 2, 'HOPE': 2, 'SOMETHING': 2, 'UNEXPECTEDLY': 2, 'GOOD': 2, 'HAPPENS': 2, 'WEEK': 2, 'TRY': 2, 'AT': 2, 'HOME': 2, 'Practice': 2, 'Watermelon': 2, 'Sugar': 2, 'Video': 2, 'Out': 2, 'desperate': 2, 'presidential': 2, 'Candace': 2, 'Owens': 2, 'posting': 2, 'privileges': 2, 'suspended': 2, 'unsuspend': 2, 'ignore': 2, 'Okay': 2, 'stylist': 2, 'prioritizing': 2, 'completely': 2, 'stifling': 2, 'FREE': 2, 'SPEECH': 2, 'DIDN’T': 2, 'THINK': 2, 'THEY’D': 2, 'THROW': 2, 'IT': 2, 'THEY': 2, 'reopened': 2, 'amidst': 2, 'olds': 2, 'app': 2, 'realize': 2, 'emotionally': 2, 'growing': 2, 'Premeds': 2, 'majors': 2, 'stressful': 2, 'kindergartner': 2, 'reviews': 2, 'sometimes': 2, 'referred': 2, 'Virus': 2, 'Medical': 2, 'Supply': 2, 'Distribution': 2, 'importantly': 2, 'Peter': 2, 'Daszak': 2, 'British': 2, 'born': 2, 'PhD': 2, 'career': 2, 'discovering': 2, 'dangerous': 2, 'viruses': 2, 'wildlife': 2, 'bats': 2, '2003': 2, 'warned': 2, '60': 2, 'Minutes': 2, 'virology': 2, 'research': 2, 'Mexico': 2, 'sadly': 2, 'experiencing': 2, 'problems': 2, 'Southern': 2, 'Classic': 2, 'sooo': 2, 'lucky': 2, 'tight': 2, 'rapidly': 2, 'Cinco': 2, 'de': 2, 'Mayo': 2, 'Taco': 2, 'Tuesday': 2, 'named': 2, 'Mexican': 2, 'greatest': 2, 'Rigged': 2, 'Election': 2, 'grab': 2, 'mailboxes': 2, 'print': 2, 'forgeries': 2, 'forge': 2, 'absentee': 2, 'Trying': 2, 'Scam': 2, 'anniversary': 2, 'massacre': 2, '31st': 2, '1921': 2, 'Oklahoma': 2, 'Street': 2, '99': 2, 'hung': 2, 'blink': 2, 'eye': 2, 'norm': 2, 'Cop': 2, 'truck': 2, 'insight': 2, 'training': 2, 'commit': 2, 'crimes': 2, 'reported': 2, 'autonomous': 2, 'ground': 2, 'announce': 2, 'donate': 2, 'ventilators': 2, 'cooperating': 2, 'development': 2, 'Together': 2, 'invisible': 2, 'enemy': 2, 'critically': 2, 'injured': 2, 'appears': 2, 'defend': 2, 'Looters': 2, 'skateboard': 2, 'stoned': 2, 'medium': 2, 'sized': 2, 'rocks': 2, 'Ambulance': 2, 'KEEP': 2, 'SAME': 2, 'ENERGY': 2, 'WHEN': 2, 'VOTE': 2, 'friend': 2, 'tags': 2, 'aggressively': 2, 'stomped': 2, 'stomach': 2, 'causing': 2, 'awareness': 2, 'Bay': 2, 'Area': 2, 'Tower': 2, 'frank': 2, 'senator': 2, 'county': 2, 'prosecutor': 2, 'republican': 2, 'TRANSITION': 2, 'GREATNESS': 2, 'cookies': 2, 'downward': 2, 'exceptions': 2, 'indeed': 2, 'yell': 2, 'breaking': 2, 'knowing': 2, 'alienate': 2, 'shove': 2, 'anger': 2, 'horrifically': 2, 'lean': 2, 'bro': 2, 'Doing': 2, 'REALLY': 2, 'medically': 2, 'solving': 2, 'complain': 2, 'coworkers': 2, 'unhinged': 2, 'desire': 2, 'express': 2, 'liberty': 2, 'zealand': 2, 'strictest': 2, 'puts': 2, 'welfare': 2, 'profit': 2, 'Slogan': 2, 'generator': 2, 'beginning': 2, 'yearly': 2, 'salary': 2, '400': 2, '450': 2, 'relief': 2, 'Monkeys': 2, 'escape': 2, 'samples': 2, 'assistant': 2, 'grandfather': 2, 'grandson': 2, 'backyard': 2, 'definitely': 2, 'content': 2, 'certainly': 2, 'interest': 2, 'marathon': 2, 'runner': 2, 'faint': 2, 'desk': 2, 'affair': 2, 'investigator': 2, 'joke': 2, 'weight': 2, 'facial': 2, 'features': 2, 'insecurities': 2, 'scars': 2, 'misery': 2, 'peaceful': 2, 'goddamn': 2, 'Queen': 2, 'assholes': 2, 'storm': 2, 'capitol': 2, 'HAVING': 2, 'WEAR': 2, 'ur': 2, 'dummy': 2, 'DAY': 2, 'sky': 2, 'approval': 2, 'gotten': 2, 'Governments': 2, 'From': 2, 'absolute': 2, 'mugs': 2, 'carers': 2, 'role': 2, 'protectors': 2, 'DNC': 2, 'acting': 2, 'thes': 2, 'Face': 2, 'ID': 2, 'pissing': 2, 'Yall': 2, 'hispanics': 2, 'Langley': 2, 'park': 2, 'Concast': 2, 'puppets': 2, 'airwaves': 2, 'Enemy': 2, 'appearing': 2, 'NEARLY': 2, 'VINDMAN': 2, 'covered': 2, 'bogus': 2, 'Ukraine': 2, 'transcript': 2, 'constantly': 2, 'yelled': 2, 'impeach': 2, 'admits': 2, 'parts': 2, 'Another': 2, 'hoax': 2, 'exposed': 2, 'fake': 2, 'TRUTH': 2, 'Downtown': 2, 'repeat': 2, 'depend': 2, 'type': 2, 'ofcr': 2, '\\u2066@NYPDShea\\u2069': 2, '\\u2066@BilldeBlasio\\u2069': 2, 'blew': 2, 'gma': 2, 'dramatic': 2, 'coughing': 2, 'grabbed': 2, 'throat': 2, 'started': 2, 'praying': 2, 'girl': 2, 'crying': 2, 'documents': 2, 'reveal': 2, 'ordered': 2, 'private': 2, 'conversation': 2, 'abused': 2, 'opponent': 2, 'FaceID': 2, 'security': 2, 'guard': 2, 'Family': 2, 'Dollar': 2, 'mandated': 2, 'american': 2, 'Karl': 2, 'Anthony': 2, 'Towns': 2, 'mother': 2, 'Ca': 2, 'imagine': 2, 'lowering': 2, 'Buildings': 2, 'Monuments': 2, 'staff': 2, 'expensive': 2, 'Deal': 2, 'ink': 2, 'dry': 2, 'difference': 2, 'Garcia': 2, 'Dem': 2, 'Tom': 2, 'Tiffany': 2, 'rival': 2, 'BIG': 2, 'Wisconsin': 2, 'WINS': 2, 'kdrama': 2, 'stans': 2, 'gone': 2, 'downtown': 2, 'sites': 2, 'test': 2, 'appointment': 2, 'pls': 2, 'lmk': 2, 'process': 2, 'action': 2, 'Too': 2, 'BAEKHYUN': 2, '백현': 2, 'Unwrapped': 2, 'Cam': 2, 'lyrics': 2, '에리': 2, '기가막힌': 2, '케미스트리': 2, 'rapper': 2, 'Cause': 2, 'bout': 2, 'songs': 2, 'physicians': 2, 'looked': 2, 'size': 2, 'fits': 2, 'approach': 2, 'popped': 2, 'Center': 2, 'Pepper': 2, 'batons': 2, 'arrests': 2, 'Man': 2, 'corona': 2, 'BREATHED': 2, 'feet': 2, 'nieces': 2, 'nephews': 2, '2040': 2, 'metaphor': 2, 'presidency': 2, 'factory': 2, 'song': 2, 'Live': 2, 'Die': 2, 'blares': 2, 'background': 2, 'bunch': 2, 'malarkey': 2, 'playbook': 2, 'pandemics': 2, 'pass': 2, 'Uh': 2, 'Commies': 2, 'blue': 2, 'Floridians': 2, 'bars': 2, 'Might': 2, 'destroy': 2, 'bubble': 2, '【': 2, 'challenge': 2, '】': 2, 'European': 2, 'election': 2, 'outgoing': 2, 'target': 2, 'incoming': 2, 'sabotage': 2, 'appalled': 2, 'thinks': 2, 'bear': 2, 'responsibility': 2, 'failing': 2, 'lifetime': 2, 'Breonna': 2, 'Taylor': 2, 'cousin': 2, 'unannounced': 2, 'boyfriend': 2, 'Walker': 2, 'unjustly': 2, 'Neither': 2, 'suspect': 2, 'remotely': 2, 'charges': 2, 'fraudulent': 2, 'statements': 2, 'Adam': 2, 'Schiff': 2, 'Russian': 2, 'Witch': 2, 'Hunt': 2, 'mistakes': 2, 'dispatch': 2, 'heat': 2, 'Gayle': 2, 'dude': 2, 'tote': 2, 'carry': 2, 'bright': 2, 'pink': 2, 'red': 2, 'insisted': 2, 'color': 2, 'RESIGN': 2, 'learned': 2, 'Speaker': 2, 'Members': 2, 'either': 2, 'withheld': 2, 'committees': 2, 'lebron': 2, 'kimbo': 2, 'slice': 2, 'recognize': 2, 'Ron': 2, 'Florida': 2, 'excellent': 2, 'fashion': 2, 'nearly': 2, 'badly': 2, 'humanly': 2, 'polls': 2, 'height': 2, 'popularity': 2, 'stuck': 2, 'hits': 2, 'respiratory': 2, 'rattled': 2, 'opening': 2, 'YOUR': 2, 'breathe': 2, 'Title': 2, '32': 2, 'August': 2, 'succeed': 2, 'recovery': 2, 'Politicking': 2, 'TOTALLY': 2, 'UNACCEPTABLE': 2, 'INHUMANE': 2, 'priorities': 2, 'highway': 2, 'construction': 2, 'begin': 2, 'heavy': 2, 'proved': 2, 'smart': 2, 'fixed': 2, 'periods': 2, 'advertisement': 2, 'ramp': 2, 'THUGS': 2, 'dishonoring': 2, 'spoke': 2, 'Tim': 2, 'Walz': 2, 'Military': 2, 'Any': 2, 'difficulty': 2, 'assume': 2, 'Staten': 2, 'Islanders': 2, 'drive': 2, 'non': 2, 'customer': 2, 'brain': 2, 'inwardly': 2, 'snapped': 2, 'FUCK': 2, 'deemed': 2, 'worker': 2, 'whole': 2, 'helping': 2, 'Project': 2, 'SuperM': 2, 'somebody': 2, 'Denied': 2, 'exists': 2, 'Fired': 2, 'oversight': 2, 'Recommended': 2, 'drink': 2, 'bleach': 2, 'Stolen': 2, 'governors': 2, 'gutless': 2, 'posturing': 2, 'scale': 2, 'root': 2, 'cheating': 2, 'forgery': 2, 'theft': 2, 'cheated': 2, 'Clean': 2, 'outfitted': 2, 'Avenger': 2, 'barrels': 2, 'suspenders': 2, 'hearing': 2, 'authority': 2, 'rampant': 2, 'ranking': 2, 'impacted': 2, 'mum': 2, 'seeking': 2, 'prayers': 2, 'Spokesman': 2, 'speaks': 2, 'stupidly': 2, 'behalf': 2, 'desperately': 2, 'deflect': 2, 'carnage': 2, 'Its': 2, 'attack': 2, 'Europe': 2, 'issue': 2, 'details': 2, 'wiretapped': 2, 'surprising': 2, 'official': 2, 'meeting': 2, 'docs': 2, 'weasels': 2, 'hooting': 2, 'hollering': 2, 'accused': 2, 'tapping': 2, 'creepiest': 2, 'dystopian': 2, 'began': 2, 'terrifying': 2, 'camera': 2, 'remote': 2, 'controlled': 2, 'robot': 2, 'patrols': 2, 'Bishan': 2, 'Ang': 2, 'Mo': 2, 'Kio': 2, 'Park': 2, 'Singapore': 2, 'warn': 2, 'Look': 2, 'fear': 2, 'Story': 2, 'kinda': 2, 'seeing': 2, 'pics': 2, 'paramilitary': 2, 'Robocops': 2, 'hospitals': 2, 'bags': 2, 'yesterday': 2, 'Rand': 2, 'Paul': 2, 'doctor': 2, 'announced': 2, 'Greta': 2, 'Thunberg': 2, 'panel': 2, 'speak': 2, '2nd': 2, 'Wave': 2, 'somewhere': 2, 'stretching': 2, 'outfit': 2, 'laying': 2, 'Ready': 2, 'paid': 2, 'distance': 2, 'class': 2, 'expanded': 2, 'range': 2, 'leisure': 2, 'activities': 2, 'Republicans': 2, 'return': 2, 'Nancy': 2, 'minute': 2, 'Looting': 2, 'On': 2, 'Memorial': 2, 'Day': 2, 'loss': 2, 'ahead': 2, 'daughter': 2, 'TV': 2, 'passengers': 2, 'OF': 2, 'voted': 2, 'twice': 2, 'wins': 2, 'REAL': 2, 'alternate': 2, 'cause': 2, 'account': 2, 'Justice': 2, 'played': 2, 'golf': 2, 'sound': 2, 'Of': 2, 'Jungkook': 2, 'Mingyu': 2, 'Cha': 2, 'Jaehyun': 2, 'Itaewon': 2, 'single': 2, 'send': 2, 'OKAY': 2, 'vaccines': 2, 'move': 2, 'wow': 2, 'Los': 2, 'Angeles': 2, 'decided': 2, 'punish': 2, 'attention': 2, 'access': 2, 'tweeted': 2, 'maybe': 2, 'PAGE': 2, 'NINE': 2, 'MUST': 2, 'start': 2, 'thought': 2, 'd': 2, 'reporter': 2, 'Never': 2, 'tests': 2, 'understand': 2, 'Intelligence': 2, '17': 2, 'release': 2, 'stays': 2, 'dies': 2, '31': 2, 'fantastic': 2, 'unless': 2, 'confident': 2, 'late': 2, '2016': 2, 'reports': 2, 'YouTube': 2, 'Thailand': 2, 'worried': 2, 'jail': 2, 'own': 2, 'April': 2, 'represent': 2, 'each': 2, 'Were': 2, 'Over': 2, '11': 2, 'clean': 2, 'tracers': 2, 'y’': 2, 'play': 2, 'questions': 2, 'CA': 2, 'U': 2, 'S': 2, 'Officials': 2, 'Daily': 2, 'Task': 2, 'Force': 2, 'vast': 2, 'appear': 2, 'Assemblywoman': 2, 'early': 2, 'looks': 2, 'times': 2, 'carrying': 2, 'staged': 2, 'publicity': 2, 'truth': 2, 'Zealand': 2, 'Prime': 2, 'Minister': 2, 'Chloroquine': 2, 'Hydroxychloroquine': 2, 'hydroxychloroquine': 2, 'Pompeo': 2, 'blocking': 2, 'testifying': 2, 'catastrophic': 2, 'investigate': 2, 'beating': 2, 'attempting': 2, 'tonight': 2, 'Taxpayer': 2, 'funded': 2, 'checks': 2, 'Make': 2, 'deaths': 2, 'collectively': 2, 'age': 2, '25': 2, 'mental': 2, 'gives': 2, 'sources': 2, 'Amy': 2, 'battle': 2, 'racism': 2, 'movie': 2, 'BTS': 2, 'Hope': 2, 'tomorrow': 2, 'shown': 2, 'associated': 2, 'arrogant': 2, 'infection': 2, 'rate': 2, 'track': 2, 'Many': 2, 'Maine': 2, 'State': 2, 'won': 2, 'problem': 2, 'leaders': 2, 'blank': 2, 'letting': 2, 'answers': 2, 'find': 2, 'elected': 2, 'community': 2, 'calling': 2, 'threatening': 2, 'threat': 2, 'exactly': 2, 'worship': 2, 'places': 2, 'rates': 2, 'RIOT': 2, 'military': 2, '97': 2, 'liners': 2, 'lock': 2, 'cars': 2, 'closed': 2, 'tax': 2, 'idols': 2, 'drop': 2, 'criticism': 2, 'contract': 2, 'tracing': 2, 'deny': 2, 'To': 2, 'UK': 2, 'present': 2, 'image': 2, 'blame': 2, 'guy': 2, 'recording': 2, 'Baltimore': 2, 'trip': 2, 'six': 2, 'KUN': 2, 'Manila': 2, 'absolutely': 2, 'mass': 2, 'investigation': 2, 'Secretary': 2, 'largest': 2, 'Barack': 2, 'complete': 2, 'showing': 2, 'PA': 2, 'close': 2, 'unpunished': 2, 'Rioters': 2, 'Russia': 2, 'WHAT': 2, 'HE': 2, 'distract': 2, 'silence': 2, 'accept': 2, 'ft': 2, 'current': 2, 'Belgium': 2, 'reason': 2, 'fees': 2, 'ABS': 2, 'CBN': 2, 'Really': 2, 'Radical': 1, 'Conservatives': 1, 'Section': 1, '230': 1, 'revoked': 1, 'Until': 1, 'regulated': 1, 'Summer': 1, 'semester': 1, 'Fall': 1, 'Y’all': 1, 'classes': 1, 'mad': 1, 'gets': 1, 'bigger': 1, 'tremendous': 1, 'capacity': 1, 'Washington': 1, 'Senators': 1, 'returning': 1, 'Capital': 1, 'Hill': 1, 'Monday': 1, 'Crazy': 1, 'Abbott': 1, 'Test': 1, 'inform': 1, 'Brian': 1, 'Monahan': 1, 'faster': 1, 'republicans': 1, 'Ngl': 1, 'rights': 1, 'leads': 1, 'Wednesday': 1, '7': 1, 'expression': 1, 'forms': 1, 'veterans': 1, 'lived': 1, 'roadmap': 1, 'entertain': 1, 'costumes': 1, 'Dads': 1, 'bruh': 1, 'LIVE': 1, 'APD': 1, 'windows': 1, 'slashed': 1, 'tires': 1, 'tazed': 1, 'NOTHING': 1, 'RIPPED': 1, 'THEM': 1, 'OUT': 1, 'THER': 1, 'CAR': 1, 'HAVE': 1, 'HUGE': 1, 'Staunton': 1, 'Virginia': 1, 'incumbent': 1, 'council': 1, 'seats': 1, 'monumental': 1, 'earth': 1, 'shattering': 1, 'Serious': 1, 'guys': 1, 'toll': 1, 'Under': 1, '10k': 1, 'viral': 1, 'technically': 1, 'listed': 1, 'Deactivated': 1, 'bc': 1, 'deadly': 1, 'lack': 1, 'federal': 1, 'implosion': 1, 'Department': 1, 'dismissal': 1, 'brought': 1, 'arm': 1, 'stories': 1, 'fact': 1, 'perhaps': 1, 'exercise': 1, 'weekend': 1, 'Totally': 1, 'Corrupt': 1, 'makes': 1, 'mortal': 1, 'sin': 1, 'Agencies': 1, 'Eun': 1, 'Woo': 1, 'Apologize': 1, 'Outing': 1, 'Confirm': 1, 'Tested': 1, 'Negative': 1, 'Did': 1, 'field': 1, 'constructed': 1, 'MILLION': 1, 'patient': 1, 'elderly': 1, 'infecting': 1, 'needlessly': 1, 'OHHHH': 1, 'huh': 1, 'Time': 1, 'Keep': 1, 'reform': 1, 'closing': 1, 'centers': 1, 'systems': 1, 'Yes': 1, 'Admin': 1, 'specific': 1, 'HCQ': 1, 'LISTEN': 1, 'feeling': 1, 'County': 1, 'likely': 1, 'July': 1, 'couped': 1, 'Brock': 1, 'Turner': 1, 'imprisoned': 1, 'raping': 1, 'unconscious': 1, 'Getting': 1, 'pm': 1, 'curfew': 1, 'alert': 1, 'Depression': 1, 'invites': 1, 'Putin': 1, 'Whistleblower': 1, 'HYDROXYCHLOROQUINE': 1, 'Then': 1, 'emergency': 1, 'authorization': 1, 'shared': 1, 'concerns': 1, 'LEAKED': 1, 'dumb': 1, 'grandstanding': 1, 'Trumper': 1, 'needs': 1, 'Every': 1, 'contact': 1, 'contacts': 1, 'See': 1, 'DOES': 1, 'Senior': 1, 'Source': 1, 'tells': 1, 'agreement': 1, 'among': 1, 'agencies': 1, 'originated': 1, 'Wuhan': 1, 'source': 1, 'stressed': 1, 'believed': 1, 'MISTAKE': 1, 'intentional': 1, 'ride': 1, 'hyper': 1, 'productivity': 1, 'excessive': 1, 'laziness': 1, 'exhausting': 1, 'piece': 1, 'fathom': 1, 'society': 1, 'options': 1, 'conceive': 1, 'illness': 1, 'poverty': 1, 'arrangement': 1, 'Minnesota': 1, 'brave': 1, 'vain': 1, '50': 1, 'WEAKER': 1, 'asleep': 1, 'wheel': 1, 'EVERYTHING': 1, 'Lamestream': 1, 'everybody': 1, 'offer': 1, 'username': 1, 'outbid': 1, '54': 1, 'pharmaceutical': 1, 'company': 1, 'u': 1, 'SCOOP': 1, 'obtains': 1, 'notification': 1, 'congress': 1, 'between': 1, '2017': 1, 'pages': 1, 'provided': 1, 'NSA': 1, 'Forgot': 1, 'crib': 1, 'toilet': 1, 'paper': 1, 'German': 1, 'intelligence': 1, 'Xi': 1, 'Jinping': 1, 'chief': 1, 'Tedros': 1, 'hold': 1, 'transmission': 1, 'delay': 1, 'warning': 1, 'Insider': 1, 'Reveals': 1, 'Staged': 1, 'Line': 1, 'CBS': 1, 'clue': 1, 'PATIENTS': 1, 'RN': 1, 'Cherry': 1, 'None': 1, 'tens': 1, 'unemployed': 1, 'fabric': 1, 'shredded': 1, 'reputation': 1, 'collapsed': 1, 'BAMBAM': 1, 'x': 1, 'UNICEF': 1, 'Online': 1, 'Charity': 1, 'Show': 1, '00PM': 1, 'KST': 1, 'WorkpointOfficial': 1, 'sorry': 1, 'Professor': 1, 'shagging': 1, 'penis': 1, 'double': 1, 'Throwing': 1, 'Texans': 1, 'whose': 1, 'biz': 1, 'fault': 1, 'wrong': 1, 'eliminating': 1, 'violating': 1, 'retroactive': 1, 'superseding': 1, 'local': 1, 'Criminals': 1, 'released': 1, 'prevent': 1, 'owners': 1, 'interesting': 1, 'stark': 1, 'contrast': 1, 'lengths': 1, 'undergone': 1, 'try': 1, '88': 1, 'pairs': 1, 'shoes': 1, 'nurse': 1, 'Yet': 1, 'short': 1, 'Front': 1, 'Page': 1, 'Headline': 1, 'reads': 1, 'BOOST': 1, 'TESTS': 1, 'LACK': 1, 'TAKERS': 1, 'combined': 1, 'Pulling': 1, 'prank': 1, 'mess': 1, 'Despicable': 1, 'pedo': 1, 'burned': 1, 'economy': 1, 'impoverishing': 1, 'lie': 1, 'virulence': 1, 'evil': 1, 'anonymous': 1, 'minneapolis': 1, 'hacked': 1, 'PD': 1, 'radios': 1, 'communicating': 1, 'aired': 1, 'Number': 1, 'acted': 1, 'quickly': 1, 'forcefully': 1, 'March': 1, 'Dominic': 1, 'Cummings': 1, 'Almost': 1, '70': 1, 'upsetting': 1, 'Tough': 1, 'Cry': 1, 'river': 1, 'District': 1, 'flip': 1, 'Have': 1, 'List': 1, 'Who': 1, 'Involved': 1, 'Unmasking': 1, 'Wire': 1, 'headed': 1, 'Vice': 1, 'bringing': 1, 'highly': 1, 'complex': 1, 'resources': 1, 'standard': 1, 'follow': 1, 'future': 1, 'shape': 1, 'HOLY': 1, 'SHIT': 1, 'murdered': 1, 'defending': 1, 'looters': 1, 'pray': 1, 'Diana': 1, 'Richardson': 1, 'plaza': 1, 'hacks': 1, 'Million': 1, 'opposed': 1, 'plus': 1, '15': 1, '20': 1, 'lose': 1, 'entry': 1, 'joking': 1, 'reveals': 1, 'apologies': 1, 'dearly': 1, 'values': 1, 'preference': 1, '⠀': 1, 'earthquake': 1, 'Jacinda': 1, 'Ardern': 1, 'skipped': 1, 'quake': 1, 'struck': 1, 'interview': 1, 'Effect': 1, 'Azithromycin': 1, 'Corrected': 1, 'QT': 1, 'Interval': 1, 'Patients': 1, 'SARS': 1, 'CoV': 1, 'Infection': 1, 'Circulation': 1, 'Arrhythmia': 1, 'Electrophysiology': 1, 'Saudi': 1, 'legs': 1, 'Administration': 1, 'task': 1, 'deputies': 1, 'testimonies': 1, 'imaginable': 1, 'Maybe': 1, 'AG': 1, 'savage': 1, 'senior': 1, 'whining': 1, 'showed': 1, 'crowd': 1, 'detain': 1, 'HISTORIC': 1, 'monkey': 1, 'drag': 1, 'bench': 1, 'platform': 1, 'immigration': 1, 'Free': 1, 'Amnesty': 1, 'officially': 1, 'immigrants': 1, 'gun': 1, 'Guess': 1, '300': 1, 'JUNE': 1, 'chile': 1, 'wtf': 1, 'At': 1, 'joined': 1, 'Marine': 1, 'Corps': 1, 'deployed': 1, 'Iraq': 1, 'VA': 1, 'disabled': 1, 'suffer': 1, 'unimaginable': 1, 'physical': 1, 'daily': 1, 'refuse': 1, 'patriot': 1, 'thank': 1, 'vet': 1, 'frightening': 1, 'scene': 1, 'Riot': 1, 'holding': 1, 'expressing': 1, 'opinion': 1, 'Does': 1, 'Newsome': 1, 'Bill': 1, 'Rights': 1, 'swore': 1, 'uphold': 1, 'offense': 1, 'google': 1, 'countless': 1, 'events': 1, 'personal': 1, 'choice': 1, 'fcking': 1, 'embarrassing': 1, 'Trevor': 1, 'Ahmaud': 1, 'Arbery': 1, 'Cooper': 1, 'Calling': 1, 'twt': 1, 'Watch': 1, 'KINGSMAN': 1, 'invite': 1, 'boys': 1, 'join': 1, 'gang': 1, 'fans': 1, 'Drop': 1, 'line': 1, 'fancy': 1, 'Such': 1, 'counting': 1, 'Almighty': 1, 'reward': 1, 'huge': 1, 'gains': 1, 'despair': 1, 'whatever': 1, 'remove': 1, 'replace': 1, 'Breakthrough': 1, 'phosphate': 1, 'apparent': 1, 'efficacy': 1, 'treatment': 1, 'pneumonia': 1, 'clinical': 1, 'studies': 1, 'outrage': 1, 'affront': 1, 'rule': 1, 'law': 1, 'frightining': 1, 'roots': 1, 'bloody': 1, 'division': 1, 'privilege': 1, 'often': 1, 'majority': 1, 'recoil': 1, 'disgust': 1, 'easily': 1, 'plague': 1, 'casual': 1, 'simple': 1, 'trolley': 1, 'barreling': 1, 'towards': 1, 'five': 1, 'grandmothers': 1, 'pull': 1, 'switch': 1, 'redirect': 1, 'kills': 1, 'buy': 1, 'toaster': 1, 'oven': 1, 'complaints': 1, 'cure': 1, 'itself': 1, 'reporting': 1, 'sending': 1, 'predator': 1, 'drones': 1, 'militarize': 1, 'CBP': 1, 'etc': 1, 'violence': 1, 'unchecked': 1, 'defund': 1, 'Wearing': 1, 'RESPECT': 1, 'Atlanta': 1, 'Niggas': 1, 'DeathBed': 1, 'Pop': 1, 'Shit': 1, '4LIFERS': 1, 'MERCH': 1, 'JUST': 1, 'DROPPED': 1, 'NYC': 1, 'Ohio': 1, 'rubber': 1, 'observing': 1, 'apartment': 1, 'walking': 1, 'militarized': 1, 'terrorize': 1, 'MUCH': 1, 'everywhere': 1, 'progress': 1, 'Saying': 1, 'cops': 1, 'African': 1, 'isn': 1, 'mostly': 1, 'reps': 1, 'banned': 1, 'xenophobic': 1, 'equally': 1, 'nuts': 1, '44': 1, 'apologized': 1, 'Listen': 1, 'stealing': 1, 'employee': 1, 'LMAO': 1, 'eyes': 1, 'criminalize': 1, 'wh': 1, 'te': 1, 'Places': 1, 'ESSENTIAL': 1, 'reaffirmed': 1, 'Amendment': 1, 'announcing': 1, 'guidance': 1, 'churches': 1, 'synagogues': 1, 'mosques': 1, 'SAFELY': 1, 'ALL': 1, 'faiths': 1, 'spreading': 1, 'lift': 1, 'England': 1, 'Agree': 1, 'John': 1, 'advice': 1, 'TTI': 1, 'capable': 1, 'surge': 1, 'locally': 1, 'responsive': 1, 'rapid': 1, 'results': 1, 'lower': 1, 'trusted': 1, 'demonstrate': 1, 'Taking': 1, 'Knee': 1, 'BS': 1, 'Nationwide': 1, 'streets': 1, 'deploy': 1, 'regime': 1, 'change': 1, 'Dispatch': 1, 'ASTRO': 1, 'Eunwoo': 1, 'NCT': 1, 'SEVENTEEN': 1, '26': 1, 'reportedly': 1, 'visited': 1, 'restaurant': 1, 'entertainment': 1, 'negative': 1, 'NFL': 1, 'tweeting': 1, 'equivalent': 1, 'CIA': 1, 'recognizes': 1, 'Martin': 1, 'Luther': 1, 'Jr': 1, 'birthday': 1, 'Loved': 1, 'outta': 1, 'bullshit': 1, 'island': 1, 'pokémon': 1, 'stadium': 1, 'quintessential': 1, 'berry': 1, 'lmfao': 1, 'kalau': 1, 'kau': 1, 'sorang': 1, 'Walaupun': 1, 'pakaian': 1, 'macam': 1, 'tu': 1, 'scared': 1, 'door': 1, 'Kunci': 1, 'dulu': 1, 'baru': 1, 'engine': 1, 'REPORT': 1, 'Klobuchar': 1, 'Admits': 1, 'Saved': 1, 'Her': 1, 'Husband': 1, 'Life': 1, 'MEDIA': 1, 'SILENT': 1, 'summation': 1, 'genius': 1, 'healthy': 1, 'rich': 1, 'returns': 1, 'subpoena': 1, 'Sometimes': 1, 'calm': 1, 'faith': 1, 'naturally': 1, 'dis': 1, 'patch': 1, 'ruin': 1, 'profile': 1, 'careers': 1, '0': 1, 'confirmed': 1, 'WONT': 1, 'celebs': 1, 'affiliated': 1, 'nth': 1, 'room': 1, 'y': 1, 'Lana': 1, 'blatantly': 1, 'ignoring': 1, 'Beyoncé': 1, 'Nicki': 1, 'sexuality': 1, 'sit': 1, 'Commercial': 1, 'exempt': 1, 'misogynistic': 1, 'attacks': 1, 'masked': 1, 'constructive': 1, 'fought': 1, 'nations': 1, 'healthier': 1, 'friendship': 1, 'categorically': 1, 'untrue': 1, 'Care': 1, 'ARE': 1, 'held': 1, 'snark': 1, 'extraordinary': 1, 'willingly': 1, 'undermine': 1, 'messaging': 1, 'midsts': 1, '55': 1, 'rising': 1, 'wretched': 1, 'Explain': 1, 'cameras': 1, 'Is': 1, 'apply': 1, 'peasants': 1, 'proposing': 1, 'Law': 1, 'corporation': 1, 'rehire': 1, 'employees': 1, 'funds': 1, 'unseen': 1, 'By': 1, 'astonishing': 1, 'Hot': 1, 'heels': 1, 'Newsnight': 1, 'publish': 1, 'pro': 1, 'pieces': 1, 'Sponsored': 1, 'Barr': 1, 'led': 1, 'Bureau': 1, 'Prisons': 1, 'frees': 1, 'Manafort': 1, 'REMAINING': 1, 'FOUR': 1, 'YEARS': 1, 'sentence': 1, 'despite': 1, 'prison': 1, 'serving': 1, 'bureau': 1, 'standards': 1, 'Asian': 1, 'VERY': 1, 'CDC': 1, 'Institute': 1, 'Allergy': 1, 'Infectious': 1, 'Diseases': 1, '1000s': 1, 'channel': 1, 'worth': 1, 'equipment': 1, 'lil': 1, 'uzi': 1, 'hopped': 1, 'starbucks': 1, 'counter': 1, 'hide': 1, 'rumble': 1, 'visit': 1, 'rethink': 1, 'sends': 1, 'reasons': 1, 'neighborhood': 1, 'Median': 1, 'income': 1, '33k': 1, 'transit': 1, 'pharmacies': 1, 'banks': 1, 'stations': 1, 'restaurants': 1, 'destroyed': 1, 'closest': 1, 'stores': 1, 'nearest': 1, 'station': 1, 'Play': 1, 'WayV': 1, 'Dream': 1, 'Launch': 1, 'Special': 1, 'Cappella': 1, 'Cover': 1, 'Recorded': 1, 'Mixed': 1, 'URL': 1, 'beg': 1, 'Metro': 1, 'Community': 1, 'Inspector': 1, 'Linick': 1, 'conducting': 1, 'conduct': 1, 'Foreign': 1, 'Relations': 1, 'Committee': 1, 'bottom': 1, 'Secret': 1, 'Service': 1, 'gates': 1, 'dozen': 1, 'reporters': 1, 'included': 1, 'West': 1, 'Wing': 1, 'correct': 1, 'deadass': 1, 'partying': 1, 'immune': 1, 'orrr': 1, 'straight': 1, 'appreciate': 1, 'Ayushman': 1, 'Bharat': 1, 'Their': 1, 'programme': 1, 'initiative': 1, 'trust': 1, 'Indians': 1, 'poor': 1, 'downtrodden': 1, 'dogs': 1, 'scoop': 1, 'oreo': 1, 'ice': 1, 'cream': 1, 'trapped': 1, 'acts': 1, 'uncovered': 1, 'thinking': 1, 'skeleton': 1, 'hugo': 1, 'simberg': 1, 'garden': 1, 'loves': 1, 'flowers': 1, 'Jeff': 1, 'Bezos': 1, 'trillionaire': 1, 'Amazon': 1, 'ending': 1, 'overtime': 1, 'warehouse': 1, 'delivery': 1, 'lines': 1, 'immoral': 1, 'appropriate': 1, 'focused': 1, 'Vaccines': 1, 'Therapeutics': 1, 'predict': 1, 'population': 1, 'shift': 1, 'liberal': 1, 'conservative': 1, 'taught': 1, 'liberals': 1, 'goal': 1, 'EXCLUSIVE': 1, 'ITV': 1, 'obtained': 1, 'discharge': 1, '800': 1, 'beds': 1, 'block': 1, 'booked': 1, 'NHS': 1, 'councils': 1, 'exact': 1, 'purpose': 1, 'discharged': 1, '90': 1, 'priority': 1, 'critical': 1, 'supplies': 1, 'actions': 1, 'Susan': 1, 'Rice': 1, 'memo': 1, 'requested': 1, 'unmaskings': 1, 'transcripts': 1, 'Kislyak': 1, 'flipping': 1, 'tightening': 1, 'restrictions': 1, 'wonder': 1, 'agitated': 1, 'violently': 1, 'rioting': 1, 'run': 1, 'PROGRESSIVE': 1, 'DEMOCRATS': 1, 'Again': 1, 'Shooting': 1, 'four': 1, '100k': 1, 'criminal': 1, 'POTUS': 1, 'stokes': 1, 'flames': 1, 'threatens': 1, 'minister': 1, 'promoted': 1, 'Johnson': 1, 'customs': 1, 'Northern': 1, 'Ireland': 1, 'Teachers': 1, 'maligned': 1, 'chaos': 1, 'escalated': 1, 'R': 1, 'organize': 1, 'domestic': 1, 'terrorism': 1, 'consequence': 1, 'detained': 1, 'Iran': 1, 'spied': 1, 'followed': 1, 'harassed': 1, 'handcuffed': 1, 'recommend': 1, 'providers': 1, 'plans': 1, 'coverage': 1, 'participation': 1, 'civil': 1, 'unrest': 1, 'informed': 1, 'willing': 1, 'mediate': 1, 'arbitrate': 1, 'raging': 1, 'dispute': 1, 'WOULD': 1, 'son': 1, 'COVID19': 1, 'center': 1, 'Ventura': 1, 'Authorities': 1, 'SHOCKING': 1, 'VIDEO': 1, 'demands': 1, 'miles': 1, 'armed': 1, 'protestors': 1, 'lockdowns': 1, 'Hindutva': 1, 'Supremacist': 1, 'Modi': 1, 'Govt': 1, 'expansionist': 1, 'akin': 1, 'Nazi': 1, 'Lebensraum': 1, 'Living': 1, 'Space': 1, 'becoming': 1, 'neighbours': 1, 'Bangladesh': 1, 'Citizenship': 1, 'disputes': 1, 'Nepal': 1, 'Pak': 1, 'threatened': 1, 'false': 1, 'flag': 1, 'operation': 1, 'Michelle': 1, 'golfing': 1, 'hypocrisy': 1, 'Your': 1, 'shipped': 1, 'guess': 1, 'keeps': 1, 'dying': 1, '45': 1, 'seconds': 1, 'WHERE': 1, 'BIGHIT': 1, 'FIND': 1, 'MIN': 1, 'YOONGI': 1, 'DANCER': 1, 'SINGER': 1, 'RAPPER': 1, 'PRODUCER': 1, 'MODEL': 1, 'ART': 1, 'COOK': 1, 'CLEANING': 1, 'GENUINE': 1, 'DEDICATED': 1, 'RESPECTFUL': 1, 'MEAN': 1, 'T': 1, 'BELIEVE': 1, '’S': 1, 'SEEMS': 1, 'PERFECT': 1, 'smear': 1, 'campaigns': 1, 'unmoored': 1, 'deranged': 1, 'indecent': 1, 'seem': 1, 'designed': 1, 'least': 1, 'horrific': 1, 'caused': 1, 'impacts': 1, 'focus': 1, 'favor': 1, 'hashtag': 1, 'reaction': 1, 'transparency': 1, 'sent': 1, 'apoplexy': 1, 'prophylaxis': 1, 'outset': 1, 'Japan': 1, 'Britain': 1, 'Canada': 1, 'Indonesia': 1, '27': 1, 'EU': 1, 'member': 1, 'backing': 1, 'Australia': 1, 'push': 1, 'probe': 1, 'approaches': 1, 'launch': 1, 'hrs': 1, 'TAKE': 1, 'ME': 1, 'LITTLE': 1, 'MONSTERS': 1, 'ITS': 1, 'enforces': 1, 'tale': 1, 'approx': 1, 'Luckily': 1, 'average': 1, 'inches': 1, 'wide': 1, 'CNBC': 1, 'POLL': 1, 'Poll': 1, 'Roger': 1, 'watching': 1, 'beautiful': 1, 'creation': 1, 'Platforms': 1, 'totally': 1, 'conservatives': 1, 'voices': 1, 'regulate': 1, 'failed': 1, 'sophisticated': 1, 'version': 1, 'psych': 1, 'ward': 1, 'house': 1, 'tweets': 1, 'radical': 1, 'censored': 1, 'Be': 1, 'careful': 1, 'Still': 1, 'digging': 1, 'team': 1, 'determine': 1, 'NY': 1, 'NJ': 1, 'MI': 1, 'ordering': 1, 'infected': 1, 'Mississippi': 1, 'HS': 1, 'graduates': 1, 'virtual': 1, 'graduation': 1, 'slipped': 1, 'Harry': 1, 'Azcrac': 1, 'NEW': 1, 'volunteer': 1, 'Kushner': 1, 'effort': 1, 'filed': 1, 'complaint': 1, 'Oversight': 1, 'W': 1, 'blocked': 1, 'leading': 1, 'sick': 1, '65': 1, '600': 1, 'pathetic': 1, 'obstruction': 1, 'works': 1, 'asks': 1, 'note': 1, 'pad': 1, 'pen': 1, 'attentive': 1, 'Baby': 1, 'incredible': 1, 'Doctor': 1, 'Normal': 1, 'paint': 1, 'Connected': 1, 'Ocean': 1, 'united': 1, 'Samosa': 1, 'Looks': 1, 'delicious': 1, 'Once': 1, 'achieve': 1, 'decisive': 1, 'victory': 1, 'enjoy': 1, 'Samosas': 1, 'forward': 1, '4th': 1, 'reached': 1, 'milestone': 1, 'reaching': 1, 'passed': 1, 'heartfelt': 1, 'sympathy': 1, 'aLl': 1, 'CoPS': 1, 'ArE': 1, 'baD': 1, 'criminals': 1, 'Mexicans': 1, 'asians': 1, 'Sucks': 1, 'marginalized': 1, 'summer': 1, 'fed': 1, 'frontline': 1, 'homeschooling': 1, 'ends': 1, 'survived': 1, 'worthy': 1, 'sunshine': 1, 'powerful': 1, 'turning': 1, 'backs': 1, 'arrives': 1, 'demonstration': 1, 'higest': 1, 'multiple': 1, 'Essentially': 1, 'suing': 1, 'easier': 1, 'safer': 1, 'stance': 1, 'online': 1, 'learning': 1, 'However': 1, 'easy': 1, 'distracted': 1, 'avoid': 1, 'procrastination': 1, 'gadgets': 1, 'Set': 1, 'screen': 1, 'limit': 1, 'tablets': 1, 'except': 1, 'OUTRAGE': 1, 'Communist': 1, 'labs': 1, 'Instead': 1, 'directing': 1, 'rather': 1, 'harassing': 1, 'Shame': 1, 'neighbors': 1, 'forget': 1, 'stephen': 1, 'hillenburg': 1, 'explicitly': 1, 'spongebob': 1, 'spin': 1, 'offs': 1, 'instant': 1, 'nickelodeon': 1, 'production': 1, 'deliberate': 1, 'failures': 1, 'governments': 1, 'contributed': 1, 'Tory': 1, 'ministers': 1, 'owe': 1, 'schools': 1, 'youth': 1, 'centre': 1, 'services': 1, 'underfunded': 1, 'secondary': 1, 'stripped': 1, 'meals': 1, 'peers': 1, 'tripled': 1, 'tuition': 1, 'demonised': 1, 'climate': 1, 'strikers': 1, 'glad': 1, 'scamdemic': 1, 'cancelled': 1, 'accommodate': 1, 'nationwide': 1, 'anarchist': 1, 'parks': 1, 'GRANDMA': 1, 'KILLERS': 1, 'across': 1, 'SHUTDOWN': 1, 'MEANS': 1, 'Filipinos': 1, 'battles': 1, 'themselves': 1, 'jobless': 1, 'faces': 1, 'unemployment': 1, 'living': 1, 'add': 1, 'notice': 1, 'dine': 1, 'policy': 1, 'lifted': 1, 'light': 1, 'decreasing': 1, 'talentless': 1, 'pill': 1, 'poppers': 1, 'DAMN': 1, 'faves': 1, 'reflect': 1, 'midst': 1, 'suffering': 1, 'claim': 1, 'victim': 1, 'unifying': 1, 'former': 1, 'summon': 1, 'fueling': 1, 'speculation': 1, 'uncertainty': 1, 'GOI': 1, 'Senator': 1, 'NYS': 1, 'communities': 1, 'Completely': 1, 'utterly': 1, 'unacceptable': 1, 'Everyone': 1, 'practice': 1, 'wash': 1, 'sanitize': 1, 'packages': 1, 'granted': 1, 'Sigh': 1, 'Sending': 1, 'independent': 1, 'autopsy': 1, 'festering': 1, 'rot': 1, 'baby': 1, 'found': 1, 'annoying': 1, 'smile': 1, 'sadder': 1, 'flunky': 1, 'CRY': 1, 'whipped': 1, 'mind': 1, 'FORCED': 1, 'apologize': 1, 'Apprentice': 1, 'Even': 1, 'unmercifully': 1, 'Find': 1, 'tapes': 1, 'insane': 1, 'rally': 1, 'sociopath': 1, 'promoting': 1, 'gatherings': 1, 'Seems': 1, 'honestly': 1, 'Batman': 1, 'disappointed': 1, 'comments': 1, 'Hon’b': 1, 'ji': 1, 'appeal': 1, 'behest': 1, 'sake': 1, 'humanity': 1, 'Jai': 1, 'Hind': 1, 'Essential': 1, 'DC': 1, 'sitting': 1, 'GET': 1, 'BACK': 1, 'WORK': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Process all tweets with spaCy and extract all tokens\n",
    "tokens = []\n",
    "for text in tweets_df.cleaned_text:\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        tokens.append(token.text)\n",
    "\n",
    "# Count the occurrences of each token and create a vocabulary of unique tokens\n",
    "vocabulary = Counter(tokens)\n",
    "\n",
    "# Print the extracted vocabulary\n",
    "print(vocabulary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "When we look up a word in a dictionary, we usually just look for the base form. This dictionary base form is called the **lemma**.\n",
    "For instance, we might see forms like “go”, “goes”, “went”, “gone”, or “going” and we look up dictionary in a lemmatized form, such as \"go\" (Hovy, 2020). These words have clearly different meaning, in some contexts it is not fundamental to distinguish them. On the contrary, it is much more convenient to trace them back to their lemma. Indeed, this may simplify some analysis and allow easier extraction of relevant information from the text. Let's see an example of lemmatization applied to the corpus of tweets using spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike -> Mike\n",
      "Pence -> Pence\n",
      "caught -> catch\n",
      "on -> on\n",
      "hot -> hot\n",
      "mic -> mic\n",
      "delivering -> deliver\n",
      "empty -> empty\n",
      "boxes -> box\n",
      "of -> of\n",
      "PPE -> PPE\n",
      "for -> for\n",
      "a -> a\n",
      "PR -> pr\n",
      "stunt -> stunt\n",
      "\n",
      "\n",
      "Original text: Mike Pence caught on hot mic delivering empty boxes of PPE for a PR stunt\n",
      "Lemmatized text: Mike Pence catch on hot mic deliver empty box of PPE for a pr stunt\n"
     ]
    }
   ],
   "source": [
    "# Load the small English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# As a text example we will use a tweet from the previous dataset\n",
    "text = tweets_df.cleaned_text[1]\n",
    "\n",
    "# Process the text with spaCy and perform lemmatization\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print words and extractes lemmas\n",
    "for token in doc:\n",
    "    print(\"{0} -> {1}\".format(token.text, token.lemma_))\n",
    "\n",
    "# Finally we can recover the text of the tweet after lemmatization\n",
    "print('\\n\\nOriginal text:', text)\n",
    "lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "print('Lemmatized text:', lemmatized_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming \n",
    "\n",
    "Another strategy to reduce different forms of a word to a common base or root form is stemming. Stemming involves removing the suffixes of words to create a simplified form of the word. For example, the stem of the words \"running,\" \"runner,\" and \"run\" is \"run.\" This can be achieved using several algorithms like the one developed by Porter (1980). This algorithm defines a number of suffixes and the order in which they should be removed or replaced. These actions are then applied iteratively untill a word is reduced to its stem.\n",
    "\n",
    "Note how, although similar, stemming and lemmatization are different and give different results. Generally speaking, lemmatization tends to produce more accurate and meaningful results with respect to stemming. Nonethelss, stemming is often faster and simpler to implement, which makes it useful for tasks that require real-time processing or have limited computational resources.\n",
    "\n",
    "An implementation of the Porter stemmer is available in the Python library NLTK. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to install NLTK\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download popular NLTK data\n",
    "# !python -m nltk.downloader popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike -> mike\n",
      "Pence -> penc\n",
      "caught -> caught\n",
      "on -> on\n",
      "hot -> hot\n",
      "mic -> mic\n",
      "delivering -> deliv\n",
      "empty -> empti\n",
      "boxes -> box\n",
      "of -> of\n",
      "PPE -> ppe\n",
      "for -> for\n",
      "a -> a\n",
      "PR -> pr\n",
      "stunt -> stunt\n",
      "\n",
      "\n",
      "Original text: Mike Pence caught on hot mic delivering empty boxes of PPE for a PR stunt\n",
      "Stemmed text: mike penc caught on hot mic deliv empti box of ppe for a pr stunt\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# As a text example we will use a tweet from the previous dataset\n",
    "text = tweets_df.cleaned_text[1]\n",
    "\n",
    "# This performs tokenization on the text (NLTK equivalalent of what we did with spaCy)\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Create a PorterStemmer object\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to each word in the text\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "# Let's see results \n",
    "for token, stem in zip(tokens, stemmed_tokens):\n",
    "    print(\"{0} -> {1}\".format(token, stem))\n",
    "\n",
    "# Finally we can recover the text of the tweet after lemmatization\n",
    "print('\\n\\nOriginal text:', text)\n",
    "stemmed_text = \" \".join(stemmed_tokens)\n",
    "print('Stemmed text:', stemmed_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams\n",
    "\n",
    "In natural language processing (NLP), **N-grams** are contiguous sequences of n elements from a given text sample, where an element can be a word, a character, or part of speech. In most cases, n-grams are created from a text by dragging a window of size n over the text and extracting the sequences of n elements that fall within that window.\n",
    "\n",
    "N-grams are used in a variety of NLP tasks such as language modeling, machine translation, and text classification. By extracting n-grams from a text, it is possible to capture the local context of a word or word sequence, which can help improve the accuracy of many NLP tasks.\n",
    "\n",
    "For example, a bigram (n=2) is \"natural language\", a trigram (n=3) is \"natural language processing\", and a 4-gram (n=4) is \"natural language processing task\". By examining the frequency of different n-grams in a text or corpus, it is possible to gain insight into the distribution of words and their relationships.\n",
    "\n",
    "N-grams can also be used to generate new texts through techniques such as n-gram language modeling. In this approach, the probabilities of different N-grams in a text are used to generate a new text that is similar in style and content to the original text.\n",
    "\n",
    "However, it should be noted that n-grams can be constrained by the sparsity problem, especially for larger values of n. That is, as the value of n increases, the number of unique n-grams in a text can increase rapidly, making it difficult to capture meaningful patterns or relationships. Therefore, choosing an appropriate value of n is an important consideration in many NLP tasks.\n",
    "\n",
    "Let's see an example of  N-grams extraction applied to the corpus of tweets using spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: ['Mike', 'Pence', 'caught', 'on', 'hot', 'mic', 'delivering', 'empty', 'boxes', 'of', 'PPE', 'for', 'a', 'PR', 'stunt']\n",
      "Bigrams: ['Mike Pence', 'Pence caught', 'caught on', 'on hot', 'hot mic', 'mic delivering', 'delivering empty', 'empty boxes', 'boxes of', 'of PPE', 'PPE for', 'for a', 'a PR', 'PR stunt']\n",
      "Trigrams: ['Mike Pence caught', 'Pence caught on', 'caught on hot', 'on hot mic', 'hot mic delivering', 'mic delivering empty', 'delivering empty boxes', 'empty boxes of', 'boxes of PPE', 'of PPE for', 'PPE for a', 'for a PR', 'a PR stunt']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# As a text example we will use a tweet from the previous dataset\n",
    "text = tweets_df.cleaned_text[1]\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Define the function to extract n-grams\n",
    "def extract_ngrams(doc, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(doc) - n + 1):\n",
    "        ngram = \" \".join([doc[j].text for j in range(i, i + n)])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "# Extract unigrams, bigrams, and trigrams from the text\n",
    "unigrams = extract_ngrams(doc, 1)\n",
    "bigrams = extract_ngrams(doc, 2)\n",
    "trigrams = extract_ngrams(doc, 3)\n",
    "\n",
    "# Print the extracted n-grams\n",
    "print(\"Unigrams:\", unigrams)\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Trigrams:\", trigrams)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2. Stopwords\n",
    "\n",
    "In natural language processing (NLP), stop words refer to words that are frequently used in a language but usually do not have much meaning or semantic value when used in context. Examples of stop words in English are \"the\", \"a\", \"an\", \"and\", \"in\", \"on\", \"is\", \"are\", \"for\", \"with\", and so on.\n",
    "\n",
    "Stop words are usually removed from text during preprocessing in NLP tasks such as text classification, sentiment analysis, and information retrieval. The reason is that they do not contribute much to the overall meaning or topic of a text and can potentially degrade algorithm performance by adding noise to the data. Removing stop words can also help reduce the size of vocabulary and improve the efficiency of text processing algorithms.\n",
    "\n",
    "However, there are certain cases where the inclusion of stop words in the analysis may be useful or even necessary. For example, stopwords can be useful in tasks such as authorship attribution, to identify common themes, or writing styles. In such cases, it is important to carefully consider the use of stop words and their potential impact on the analysis\n",
    "\n",
    "We will now see a simple example on how to remove Stop Words from a text using spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens:  ['Mike', 'Pence', 'caught', 'on', 'hot', 'mic', 'delivering', 'empty', 'boxes', 'of', 'PPE', 'for', 'a', 'PR', 'stunt']\n",
      "Filtered tokens: ['Mike', 'Pence', 'caught', 'hot', 'mic', 'delivering', 'boxes', 'PPE', 'PR', 'stunt']\n",
      "\n",
      "Stop words removed:  ['on', 'empty', 'of', 'for', 'a']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# As a text example we will use a tweet from the previous dataset\n",
    "text = tweets_df.cleaned_text[1]\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Define the list of stop words\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Remove stop words from the text\n",
    "filtered_text = [token.text for token in doc if not token.is_stop]\n",
    "stop_words_removed = [token.text for token in doc if token.is_stop]\n",
    "\n",
    "# Print the original and filtered text, and the stop words removed\n",
    "print(\"Original tokens: \", [token.text for token in doc])\n",
    "print(\"Filtered tokens:\", filtered_text)\n",
    "print(\"\\nStop words removed: \", stop_words_removed)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.3. Parts of Speech\n",
    "\n",
    "**Part of speech tagging** (POS) is the process of assigning a part of speech to each word in a sentence, such as noun, verb, adjective, or adverb. POS Tagging is an important step in many NLP applications, such as named entity recognition, sentiment analysis, and machine translation.\n",
    "\n",
    "The goal of POS tagging is to identify the grammatical structure of a sentence by labelling each word with its corresponding part of speech. This information can then be used to extract meaning and context from the text. For example, knowing whether a word is a noun or a verb can help determine the subject and predicate of a sentence.\n",
    "\n",
    "POS tagging is typically performed using machine learning algorithms, such as hidden Markov models, conditional random fields, or neural networks. These algorithms are trained on annotated text corpora in which each word is labelled with the corresponding word type. After training, the algorithm can then predict the word type for a new unseen text.\n",
    "\n",
    "POS tagging is not always an easy task, as some words may have multiple possible word types depending on the context. For example, \"run\" can be a verb (\"I run every morning\") or a noun (\"I went for a run\"). In these cases, the algorithm must use contextual clues to determine the most likely part of speech for the word.\n",
    "\n",
    "Overall, POS tagging is an important technique in NLP that helps extract meaning and context from texts by identifying the grammatical structure of sentences.\n",
    "\n",
    "English has 9 main categories:\n",
    "\n",
    "- verb — Expresses an action or a state of being. E.g. jump, is, write, become\n",
    "- noun — identifies a person, a place or a thing or names of particular of one of these (pronoun). E.g. man, house, happiness\n",
    "- pronoun — can replace a noun or noun phrase. E.g. she, we, they, it\n",
    "- determiner — Is placed in front of a noun to express a quantity or clarify what the noun refers to — briefly a noun introducer. E.g. my, that, the, many\n",
    "- adjective — modifies a noun or a pronoun. E.g. pretty, old, blue, smart\n",
    "- adverb — modifies a verb, an adjective, or another adverb. E.g. gently, extremely, carefully, well\n",
    "- preposition — Connect a noun/pronoun to other parts of the sentence. E.g. by, with, about, until\n",
    "- conjunction — glue words, clauses, and sentences together. E.g. and, but, or, while, because\n",
    "- interjection — Expresses emotion in a sudden or exclamatory way. E.g. oh!, wow!, oops!\n",
    "\n",
    "\n",
    "<img src='images/POS.png' style='height: 200px; float: center'>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We will now see a simple example on how to perform POS on a text using spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike PROPN\n",
      "Pence PROPN\n",
      "caught VERB\n",
      "on ADP\n",
      "hot ADJ\n",
      "mic NOUN\n",
      "delivering VERB\n",
      "empty ADJ\n",
      "boxes NOUN\n",
      "of ADP\n",
      "PPE PROPN\n",
      "for ADP\n",
      "a DET\n",
      "PR NOUN\n",
      "stunt NOUN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load the English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# As a text example we will use a tweet from the previous dataset\n",
    "text = tweets_df.cleaned_text[1]\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# iterate over each token in the doc and print its text and POS tag\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the meaning of a POS tag is not clear to us, we ask spaCy to explain it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"PROPN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how spaCy POS tagging works on more tricky examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "run VERB\n",
      "every DET\n",
      "morning NOUN\n",
      "\n",
      "\n",
      "I PRON\n",
      "went VERB\n",
      "for ADP\n",
      "a DET\n",
      "run NOUN\n"
     ]
    }
   ],
   "source": [
    "# here the word run is used as verb\n",
    "text1 = \"I run every morning\"\n",
    "\n",
    "# here the word run is used as a noun\n",
    "text2 = \"I went for a run\"\n",
    "\n",
    "# POS tagging of sentence 1\n",
    "for token in nlp(text1):\n",
    "    print(token.text, token.pos_)\n",
    "\n",
    "print(\"\\n\")\n",
    "# POS tagging of sentence 2\n",
    "for token in nlp(text2):\n",
    "    print(token.text, token.pos_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that spaCy correctly tag the word \"run\" differently in these two examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Insight</b></big>\n",
    "    \n",
    "<a href=\"https://docs.python.org/3/library/string.html\">Python String</a> module contains some constants, utility function, and classes for string manipulation. It is a built-in module and we have to import it before using any of its constants and classes.\n",
    "\n",
    "<a href=\"https://spacy.io/\">spaCy</a> is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more that we will practice in the following sections.\n",
    "</div>\n",
    "\n",
    "<img src='images/spacy_pipeline.svg' style='height: 200px; float: right'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage, we will use spaCy to lemmatize and remove stop words from our tweets dataset. We will also list the stop words to observe: We can keep some of them depdending on our research questions (e.g., 'us' vs. 'them') or remove them fully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "#nlp = spacy.load('en')\n",
    "\n",
    "# To build a list of stop words for filtering\n",
    "stopwords = list(STOP_WORDS)\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will learn how to tokenize words to be lemmatized and filtered for pronouns, stopwords and punctuations by the 'tokenize_text' function. A text can be converted into nlp object of spaCy. First, we should convert raw text into a spaCy object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = string.punctuation\n",
    "# Creating a Spacy Parser\n",
    "from spacy.lang.en import English\n",
    "# parser = English()\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "def lemmatize_text(text):\n",
    "    tokens = nlp(text)\n",
    "    tokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens ]\n",
    "    tokens = [ word for word in tokens if word not in stopwords and word not in punctuations ]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following line, we apply 'tokenize_text' function for removing stop words and lemmatizing tokens to ...\n",
    "\n",
    "INTRODUCE TEXT CORPUS NOW (AMAZON REVIEWS?) -- DON'T USE TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['lemmatized_text'] = df['cleaned_text'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df['lemmatized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.3. Parts of Speech\n",
    "The **Part of speech tagging** (POS tagging) is the process of marking a word in the text to a particular part of speech based on both its context and definition. In simple language, we can say that POS tagging is the process of identifying a word as nouns, pronouns, verbs, adjectives, etc.\n",
    "\n",
    "English has 9 main categories:\n",
    "\n",
    "- verb — Expresses an action or a state of being. E.g. jump, is, write, become\n",
    "- noun — identifies a person, a place or a thing or names of particular of one of these (pronoun). E.g. man, house, happiness\n",
    "- pronoun — can replace a noun or noun phrase. E.g. she, we, they, it\n",
    "- determiner — Is placed in front of a noun to express a quantity or clarify what the noun refers to — briefly a noun introducer. E.g. my, that, the, many\n",
    "- adjective — modifies a noun or a pronoun. E.g. pretty, old, blue, smart\n",
    "- adverb — modifies a verb, an adjective, or another adverb. E.g. gently, extremely, carefully, well\n",
    "- preposition — Connect a noun/pronoun to other parts of the sentence. E.g. by, with, about, until\n",
    "- conjunction — glue words, clauses, and sentences together. E.g. and, but, or, while, because\n",
    "- interjection — Expresses emotion in a sudden or exclamatory way. E.g. oh!, wow!, oops!\n",
    "\n",
    "\n",
    "<img src='images/POS.png' style='height: 200px; float: right'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "    \n",
    "Pouria, can you please check other images for POS tagging that you find it beautiful and comprehensive that shows all the tags? I found this now but we can change. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply POS tagging on our dataset, first, let's tag the tokens. First, to build the dataframe with POS tags, we will have columns as \"text\", \"pos\", \"tag\", \"explain_tag\", then we will write rows from the tokenized text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"text\", \"pos\", \"tag\", \"explain_tag\"]\n",
    "rows = []\n",
    "\n",
    "for text in df['text']:\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        row = token.text, token.pos_, token.tag_, spacy.explain(token.tag_)\n",
    "        rows.append(row)\n",
    "    \n",
    "pos_df = pd.DataFrame(rows, columns = cols)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4. n-gram extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Insight</b></big>\n",
    "    \n",
    "<a href=\"https://textacy.readthedocs.io/en/0.12.0/api_reference/extract.html\">textacy</a> is a Python library for performing a variety of natural language processing (NLP) tasks, built on the high-performance spaCy library. With the fundamentals — tokenization, part-of-speech tagging, dependency parsing, etc. — delegated to another library, textacy focuses primarily on the tasks that come before and follow after. We will use textacy for extracting basic ngrams. If you have not installed yet this library, please do so before moving to the following exercise.\n",
    "     \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "\n",
    "ngrams_list = []\n",
    "\n",
    "for text in df['cleaned_text']:\n",
    "    doc = nlp(text)\n",
    "    # you can change the n grams to 3, 4 depending on your quesiton\n",
    "    ngrams = list(textacy.extract.basics.ngrams(doc, 3, min_freq=2))\n",
    "    if ngrams != []:\n",
    "        ngrams_list.append(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "    \n",
    "Or, we can introduce gensim here??... another option is shown as well. we can discuss. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, pprint \n",
    "\n",
    "# tokenize documents with gensim's tokenize() function\n",
    "tokens = [list(gensim.utils.tokenize(doc, lower=True)) for doc in df['text']]\n",
    "\n",
    "# build bigram model\n",
    "bigram_mdl = gensim.models.phrases.Phrases(tokens, min_count=1, threshold=2)\n",
    "\n",
    "# do more pre-processing on tokens (remove stopwords, stemming etc.)\n",
    "# NOTE: this can be done better\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, stem_text\n",
    "CUSTOM_FILTERS = [remove_stopwords, stem_text]\n",
    "tokens = [preprocess_string(\" \".join(doc), CUSTOM_FILTERS) for doc in tokens]\n",
    "\n",
    "# apply bigram model on tokens\n",
    "bigrams = bigram_mdl[tokens]\n",
    "\n",
    "pprint.pprint(list(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in bigrams:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in bigrams]\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(dictionary.token2id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing: tf*idf, systematic pipeline, save document-term matrix at each step to TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Pouria, can you add here visualizations for the word count after we talk with Haiko about preprocessing texts if we need more cleaning before plotting?\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Understanding the meaning: Similarity of words and documents \n",
    "\n",
    "<div class='alert alert-block alert-danger'>\n",
    "<big><b>Notes from the content document(remove after finalizing it):</b></big>\n",
    "    \n",
    "(we need a large corpus for this; https://ai.googleblog.com/2017/01/a-large-corpus-for-supervised-word.html proposes disambiguation of “stock”: pick the largest pages from https://en.wikipedia.org/wiki/Stock_(disambiguation))\n",
    "Cosine\n",
    "Word2vec (not pretrained embeddings); using gensim (Hovy)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some suggested inspiration:\n",
    "\n",
    "Measurement of Text Similarity A Survey: https://www.mdpi.com/2078-2489/11/9/421\n",
    "\n",
    "https://medium.com/@adriensieg/text-similarities-da019229c894\n",
    "\n",
    "https://newscatcherapi.com/blog/ultimate-guide-to-text-similarity-with-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<big><b>Gizem's note:</b></big>\n",
    "    \n",
    "From this point on, old notebook notes are here that I didn't want to remove in case we would like to reuse some of the information below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfg6soCx1CND"
   },
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "<b>Additional resources</b>\n",
    "\n",
    "#### NLTK (Natural Language Toolkit)\n",
    "\n",
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum. For more details, check out <a href=\"https://www.nltk.org/\">NLTK</a>'s webpage.\n",
    "    \n",
    "#### gensim\n",
    "\n",
    "Gensim is a free open-source Python library for representing documents as semantic vectors, as efficiently (computer-wise) as possible. It was developed for topic modelling, which supports the NLP tasks like Word Embedding, text summarization and many others, such as <a href=\"https://radimrehurek.com/gensim/models/ldamodel.html\">LDA Topic Modeling</a> and <a href=\"https://radimrehurek.com/gensim/models/phrases.html\">Bigrams/Trigrams</a>. For more details, check out  <a href=\"https://radimrehurek.com/gensim/auto_examples/index.html#documentation\">Gensim</a>'s webpage.\n",
    "    \n",
    "#### Hugging Face Transformers\n",
    "\n",
    "Transformers was developed by <a href=\"https://huggingface.co/\">Hugging Face</a> and provides state of the art models. It is an advanced library known for the transformer modules with high-level NLP tasks. Hugging Face is one of the most widely used libraries in NLP community. It provides native support for PyTorch and Tensorflow-based models, increasing its applicability in the deep learning community. <a href=\"https://arxiv.org/abs/1810.04805\">BERT</a>  and <a href=\"https://arxiv.org/abs/1907.11692\">RoBERTa</a> are two of the most valuable models supplied by the Hugging Face library, which is used for machine translation, question/answer activities, and many other applications. \n",
    "\n",
    "Hugging Face pipeline provides a rapid and simple approach to perform a range of NLP operations, and the Hugging Face library also supports GPUs for training. As a result, processing speeds are multiplied by a factor of ten. Check out their <a href=\"https://huggingface.co/docs/transformers/main_classes/pipelines\">Pipelines</a> for what 10+ tasks we can perform as one-liners basically. Their model repository is vast.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Haiko</b></big>\n",
    "\n",
    "At this point it occurrs to me that just providing the commands how things can be done is not enough. We also want to teach how users can proproces their corpus and save intermediate steps like \"corpus.txt\" > \"corpus_stemmed.txt\" > \"corpus_stemmed_nostopwords.txt\" > ...\n",
    "\n",
    "In general, do we need spacy, nltk, and gensim to work along this pipeline? Even if not, we should tell in the session why we introduce all packages.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency analysis\n",
    "\n",
    "<div class='alert-info'>\n",
    "<big><b>Haiko</b></big>\n",
    "\n",
    "Yes, I think it should be here. Hovy has it in the \"text representation\" section. It is then a statistic of the document-term matrix for example.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "\n",
    "To session 10! Huggingface’s transformers: State-of-the-art NLP\n",
    "Currently, Hugging face is supported by Pytorch and tensorflow 2.0. We can use transformers of Hugging Face to implement Summarization, Text Generation, Language Translation, ChatBot...\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Hovy, D. (2020). Text analysis in Python for social scientists: Discovery and exploration. Cambridge University Press.\n",
    "\n",
    "Hovy, D. (2021). Text Analysis in Python for Social Scientists: Prediction and Classification. Cambridge University Press.\n",
    "\n",
    "Vajjala, S., Majumder, B., Gupta, A., & Surana, H. (2020). Practical natural language processing: a comprehensive guide to building real-world NLP systems. O'Reilly Media.\n",
    "\n",
    "Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
    "\n",
    "Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\n",
    "\n",
    "He, R., & McAuley, J. (2016, April). Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In proceedings of the 25th international conference on world wide web (pp. 507-517).: http://jmcauley.ucsd.edu/data/amazon/index_2014.html\n",
    "\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/natural-language-processing-guide/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Credits\n",
    "\n",
    "[1] Vajjala, S., Majumder, B., Gupta, A., & Surana, H. (2020). Practical natural language processing: a comprehensive guide to building real-world NLP systems. O'Reilly Media. Chapter 1: https://www.oreilly.com/library/view/practical-natural-language/9781492054047/ch01.html\n",
    "\n",
    "POS image credit: https://machinelearningknowledge.ai/tutorial-on-spacy-part-of-speech-pos-tagging/\n",
    "spacy pipeline image credit: https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "<b>Document information</b>\n",
    "\n",
    "Contact and main author: N. Gizem Bacaksizlar Turbic & ..?\n",
    "\n",
    "Contributors: Haiko Lietz & Pouria Mirelmi & ..?\n",
    "\n",
    "Acknowledgements: ...\n",
    "\n",
    "Version date: XX. February 2023\n",
    "\n",
    "License: ...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes to be removed before publication\n",
    "\n",
    "Reviewers: Indira, Olya, or Veronika?\n",
    "\n",
    "Review intro\n",
    "\n",
    "Review and finish red boxes\n",
    "\n",
    "Add insight boxes more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Z0-GgEFeNJbn"
   ],
   "name": "Day 1: Introduction to Jupyter Notebook and Text Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
