{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d446f365",
   "metadata": {},
   "source": [
    "<img src='images/gesis.png' style='height: 50px; float: left'>\n",
    "<img src='images/social_comquant.png' style='height: 50px; float: left; margin-left: 40px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944308be",
   "metadata": {},
   "source": [
    "## Introduction to Computational Social Science methods with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9daed7e",
   "metadata": {},
   "source": [
    "# Session B2: Data parsing and static web scraping\n",
    "\n",
    "The previous [Session B1: API harvesting](1_api_harvesting.ipynb) has introduced Application Programming Interfaces (APIs) to collect data from online platforms. To refresh, APIs are pieces of software that help you control how your computer retrieves data from the servers of online platform providers with fairly easy commands. When you want to collect data from platforms or websites that do not offer an API, you will have to use **web scraping**. The idea is that all the content visible when visiting a website is collectable in principle (Bosse *et al.*, 2022). Data obtained via web scraping can be as diverse as on hotels (Han & Anderson, 2021), food prices (Hillen, 2019), and the weather (Fatmasari *et al.*, 2018).\n",
    "\n",
    "While manually collecting the information from a web page is possible (*e.g.*, downloading images, copy-pasteing text, saving the entire HTML site), most scraping procedures are automated with the help of stand-alone software or through custom programmed scrapers. Given that most websited do not offer APIs, web scraping is a major method in Computational Social Science and Machine Learning, and there are many stand-alone and programming tools to scrape webpages (Diouf *et al.*, 2019).\n",
    "\n",
    "<img src=\"./images/webscrape.png\"  width=\"350\" height = \"350\" align=\"center\"/>\n",
    "\n",
    "In most cases, not all the information on the webpage is of interest, and only some aspects are important to be collected. The content of webpages is stored in [HTML](https://en.wikipedia.org/wiki/HTML), a semi-structured and hierarchical data format (see [Session A2: Data management and relational databases](../a_introduction/2_data_management_and_relational_databases.ipynb)). Some websites help you access new content via [RSS](https://en.wikipedia.org/wiki/RSS) feeds. Retrieving just the content of semi-structured data requires **parsing**, [that is](https://en.wikipedia.org/wiki/Parsing), the process of analyzing data structures conforming to the rules of a formal grammar. For example, if the intention is to collect product reviews, the parser must be given a rule where the review texts can be found exactly. Such a rule can be: collect all listed items under the headline \"Reviews\". Building a web scraper requires understanding a webpage's HTML structure, which must be inspected before beginning to build a scraper. Building a scraper (constructing the grammar) requires manual work, and hence, web scraping is a semi-automated data collection method. In most cases, you should use an API if it is offered because API harvesting is fully automated and will save you a lot of work. But if you must build your own scraper, [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/) is the package you will want to start with (Patel, 2020; McLevey, 2022, ch. 5).\n",
    "\n",
    "Content on websites can be generated statically or dynamically. Content generated statically is present when the requested URL is loaded (*i.e.*, the content is displayed when visiting the website without interacting with it). Content that is generated dynamically means that the website displays new content due to interactions with it (*e.g.*, clicking, scrolling). For example, a social media newsfeed which allows for endless scrolling is dynamically generated. A reloading of the website is not required. Parsing statically generated content is called **static web scraping**.\n",
    "\n",
    "There are **ethical and legal issues**. It is possible to scrape every webpage simply because it is displayed in your browser, but some websites may prohibit scraping in their Terms of Services (ToS). Even when scraping is not prohibited, it may be fair not to do so (*e.g.*, sensitive content). Every scraping process is a load on the computer serving the webpage, so you may also want to reduce this traffic to not slow down a service or even crash it. Sometimes scraping limits are enforced by a website. Just make sure to always check the ToS of webpages you scrape. \"A general rule to follow here is that if websites want to provide access to data at scale, they will set up an API to provide it. If they haven't, err on the side of not collecting data at scale, and limit your data collection efforts to what you need rather than gathering all of it and filtering later.\" (McLevey, 2022, ch. 5.6)\n",
    "\n",
    "<div class='alert alert-block alert-success'>\n",
    "<b>In this session</b>, \n",
    "\n",
    "you will learn how to do web scraping with Python from scratch. In subsession **B2.1**, we will convey the basic knowledge you need for building a web scraper by taking a deep look at how HTML is structured. In subsession **B2.2**, we will introduce the Beautiful Soup package. You will learn its basic functions and how you can scrape a news site, taking Aljazeera as an example. We close, in subsession **B2.3**, with a demonstration how you can parse an RSS feed.\n",
    "</div>\n",
    "\n",
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Caution</b>\n",
    "\n",
    "This Jupyter Notebook demonstrates a workflow that consists of a **sequence of processing steps**. The notebook must be executed from top to bottom. Going back up from a certain code cell and trying to execute a cell that precedes it may not work.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e895ec3",
   "metadata": {},
   "source": [
    "## B2.1. Parsing HTML content\n",
    "\n",
    "Building a web scraper requires understanding the webpage structure (i.e., HTML structure), which should be inspected before beginning to build a scraper. Understanding the webpage structure is necessary to specify where the information that should be collected is located for the scraper. By right-clicking with the mouse anywhere on a webpage and selecting \"inspect\", the web page's structure can be visible through a menu, as seen in Figure 2. By using the \"inspector\" function, it is possible to hover over any web element (i.e., title text, price, vendor name, etc.) and locate the information within the HTML structure. You can find more resources about HTML structures and how websites are built here:\n",
    "\n",
    "\n",
    "Programming a web scraper follows a structure, which can change depending on whether parsing is performed before or after downloading the web content. However, a few steps are essential, such as (i) creating a list of websites that are intended to be scraped, (ii) looping through and downloading the website's content (with or without parsing), and (iii) saving the downloaded content in a database or data frame (e.g., SQL, CSV) if you want to analyze the data further .\n",
    "\n",
    "Creating a list of websites can be done manually by copy-pasting the website's URLs into your programming script. However, such an approach is time-consuming and should be automated if possible. Alternatively, only the base URL should be specified (e.g., ebay.com), and the remaining URLs should be generated automatically. Similar to inspecting the website's HTML structure, understanding the URL structure is also important. In most cases, the URL follows a clear pattern for pagination and other common operations (e.g., selecting the size and colour of products on a shopping platform). Thus, after some testing (i.e., clicking through the webpage), a pattern most often emerges that can be utilized to generate the URLs automatically (e.g., looping through pages of search results). Another practice is to automatically find all links on the website of the current URL and add them to the list of URLs to be scraped. That way, the entire webpage can be collected, but managing duplicates becomes important to avoid unnecessary scraping.\n",
    "\n",
    "After determining how to obtain the necessary URLs, the scraper needs to collect the content of interest (including parsing the website) or the entire page. In either case, the data should be saved in a local folder structure, allowing easy (automated) access to the data in later stages of the project. Locally saving the data includes unparsed (e.g., HTML files) and parsed (e.g., CSV or Excel files) data. The data should be continuously saved while scraping the content from the website to prevent unnecessary computer memory usage and data loss if the scraping procedure unexpectedly stops (e.g., due to errors in the programming scripts, internet, or server issues). Once the web data collection is completed, the data can be parsed into a data frame or merged if already parsed.\n",
    "\n",
    "To get started on how to program a web scraper see the following guides:\n",
    "-\tList of guides/tutorials showing how to program web scrapers.\n",
    "\n",
    "___\n",
    "\n",
    "POURIA: Please add these to the references and organize the reference list according to the APA like in other sessions. Thank you!\n",
    "    \n",
    "https://realpython.com/beautiful-soup-web-scraper-python/#reasons-for-web-scraping\n",
    "    \n",
    "https://medium.com/pythoneers/the-fundamentals-of-web-scraping-using-python-its-libraries-6f146b91efb4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9573370",
   "metadata": {},
   "source": [
    "<img src='images/html.png' style='height: 90px; float: right; margin-left: 50px' >\n",
    "\n",
    "The **HyperText Markup Language** or **HTML** is the standard markup language for documents designed to be \n",
    "displayed in a web browser. It can be assisted by technologies such as Cascading Style Sheets (CSS) and scripting languages such as JavaScript. While the main content of the web pages are in the form of HTML, CSS add styling to the pages to make them look nicer and JavaScript files add interactivity to them.\n",
    "\n",
    "\n",
    "HTML code consists of a series of **elements**, and these elements tell the browser how to display the content. For collecting data from HTML web pages, it's necessary to have an idea of how this element syntax works.\n",
    "\n",
    "#### HTML Element Syntax [<a href='#destination2'>4</a>] <a id='destination2_'></a>\n",
    "\n",
    "HTML language can be applied to pieces of text to give them different meanings in a document (Is it a paragraph? Is it a bulleted list? Is it part of a table?), structure a document into logical sections (Does it have a header? Three columns of content? A navigation menu?), and embed content such as images and videos into a page. In this section we will introduce the first two of these, together with the fundamental concepts and syntax you need to know to understand HTML.\n",
    "\n",
    "To get started, we will begin with defining elements, attributes, and some other important terms. We will also explain where these fit into HTML. You will learn how HTML elements are structured, how a typical HTML page is structured, and other important basic language features.\n",
    "\n",
    "As already mentioned, HTML is a markup language that tells web browsers how to structure the web pages you visit. It can be as complicated or as simple as the web developer wants it to be. HTML consists of a series of elements, which you use to enclose, wrap, or mark up different parts of content to make it appear or act in a certain way. The enclosing tags can make content into a hyperlink to connect to another page, italicize words, and so on. For example, consider the following line of text:\n",
    "\n",
    "`My cat is very grumpy`\n",
    "\n",
    "If we wanted the text to stand by itself, we could specify that it is a paragraph by enclosing it in a paragraph (`<p>`) element:\n",
    "\n",
    "`<p>My cat is very grumpy</p>`\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "<b>Insight</b>\n",
    "\n",
    "Tags in HTML are not case-sensitive, but it's better to write all of them in lower case for the sake of consistency and readability.\n",
    "    \n",
    "</div>\n",
    "\n",
    "####  Anatomy of an HTML element\n",
    "\n",
    "Let's further explore our paragraph element mentioned above:\n",
    "\n",
    "<img src='images/html4.png' width=\"500\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "The anatomy of our element is:\n",
    "\n",
    "- **The opening tag**: This consists of the name of the element (in this example, p for paragraph), wrapped in opening and closing angle brackets. This opening tag marks where the element begins or starts to take effect. In this example, it precedes the start of the paragraph text.\n",
    "\n",
    "\n",
    "- **The content**: This is the content of the element. In this example, it is the paragraph text.\n",
    "\n",
    "\n",
    "- **The closing tag**: This is the same as the opening tag, except that it includes a forward slash before the element name. This marks where the element ends. Failing to include a closing tag is a common beginner error that can produce peculiar results.\n",
    "\n",
    "So, *the element* is the opening tag, followed by content, followed by the closing tag.\n",
    "\n",
    "**Create your first HTML element:**  Edit the `html` string below (it contains an HTML code) and get the actual rendered HTML output from `HTML()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba5ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"<p>My cat is very grumpy.</p>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40aaac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43959db",
   "metadata": {},
   "source": [
    "#### Nesting elements\n",
    "\n",
    "Elements can be placed within other elements. This is called *nesting*. If we wanted to state that our cat is **very** grumpy, we could wrap the word \"very\" in a `<strong>` element, which means that the word is to have strong(er) text formatting:\n",
    "\n",
    "`<p>My cat is <strong>very</strong> grumpy.</p>`\n",
    "\n",
    "There is a right and wrong way to do nesting. In the example above, we opened the `p` element first, then opened the `strong` element. For proper nesting, we should close the `strong` element first, before closing the `p`.\n",
    "The following is an example of the *wrong* way to do nesting:\n",
    "\n",
    "`<p>My cat is <strong>very grumpy.</p></strong>`\n",
    "\n",
    "<u>The tags have to open and close in a way that they are inside or outside one another.</u> With the kind of overlap in the example above, the browser has to guess at your intent. This kind of guessing can lead to unexpected results.\n",
    "\n",
    "#### Block versus inline elements\n",
    "\n",
    "There are two important categories of elements to know in HTML: block-level elements and inline elements.\n",
    "\n",
    "- Block-level elements form a visible block on a page. A block-level element appears on a new line following the content that precedes it. Any content that follows a block-level element also appears on a new line. Block-level elements are usually structural elements on the page. For example, a block-level element might represent headings, paragraphs, lists, navigation menus, or footers. A block-level element wouldn't be nested inside an inline element, but it might be nested inside another block-level element.\n",
    "\n",
    "\n",
    "- Inline elements are contained within block-level elements, and surround only small parts of the document's content (not entire paragraphs or groupings of content). An inline element will not cause a new line to appear in the document. It is typically used with text, for example an `<a>` element creates a hyperlink, and elements such as `<em>` or `<strong>` create emphasis.\n",
    "\n",
    "Consider the following example:\n",
    "\n",
    "`<em>first</em><em>second</em><em>third</em>`\n",
    "\n",
    "`<p>fourth</p><p>fifth</p><p>sixth</p>`\n",
    "\n",
    "`<em>` is an inline element. As you can see below, the first three elements sit on the same line, with no space in between. On the other hand, `<p>` is a block-level element. Each p element appears on a new line, with space above and below. (The spacing is due to default CSS styling that the browser applies to paragraphs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd75709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"<em>first</em><em>second</em><em>third</em>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71509fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"<p>fourth</p><p>fifth</p><p>sixth</p>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8083558",
   "metadata": {},
   "source": [
    "#### Empty elements\n",
    "\n",
    "Not all elements follow the pattern of an opening tag, content, and a closing tag. Some elements consist of a single tag, which is typically used to insert/embed something in the document. For example, the `<img>` element embeds an image file onto a page:\n",
    "\n",
    "`<img src=\"https://raw.githubusercontent.com/mdn/beginner-html-site/gh-pages/images/firefox-icon.png\">`\n",
    "\n",
    "This would output the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40499b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<img src=\"https://raw.githubusercontent.com/mdn/beginner-html-site/gh-pages/images/firefox-icon.png\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4eeba7",
   "metadata": {},
   "source": [
    "#### Attributes\n",
    "\n",
    "Elements can also have attributes. Attributes look like this:\n",
    "\n",
    "<img src='images/html5.png' width=\"800\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "Attributes contain extra information about the element that won't appear in the content. In this example, the `class` attribute is an identifying name used to target the element with style information.\n",
    "\n",
    "An attribute should have:\n",
    "\n",
    "- A space between it and the element name. (For an element with more than one attribute, the attributes should be separated by spaces too.)\n",
    "- The attribute name, followed by an equal sign.\n",
    "- An attribute value, wrapped with opening and closing quote marks.\n",
    "\n",
    "**Adding attributes to an element**: Another example of an element is `<a>`. This stands for *anchor*. An anchor can make the text it encloses into a hyperlink. Anchors can take a number of attributes, but several are as follows:\n",
    "\n",
    "- `href`: This attribute's value specifies the web address for the link. For example: `href=\"https://www.mozilla.org/\"`\n",
    "- `title`: The `title` attribute specifies extra information about the link, such as a description of the page that is being linked to. For example, `title=\"The Mozilla homepage\"`. This appears as a tooltip when a cursor hovers over the element.\n",
    "- `target`: The `target` attribute specifies the browsing context used to display the link. For example, `target=\"_blank\"` will display the link in a new tab. If you want to display the linked content in the current tab, just omit this attribute.\n",
    "\n",
    "You can edit the `html` string below to turn it into a link to your favorite website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5087c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '<p>A link to my <a href=\"https://www.mozilla.org/\" title=\"The Mozilla homepage\" target=\"_blank\">favorite website</a>.</p>'\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9894d8",
   "metadata": {},
   "source": [
    "#### Anatomy of an HTML document\n",
    "\n",
    "Individual HTML elements aren't very useful on their own. Next, let's examine how individual elements combine to form an entire HTML page:\n",
    "\n",
    "```\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en-US\">\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>My test page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <p>This is my page</p>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "Here we have:\n",
    "\n",
    "1. `<!DOCTYPE html>`: The doctype. When HTML was young (1991-1992), doctypes were meant to act as links to a set of rules that the HTML page had to follow to be considered good HTML. More recently, the doctype is a historical artifact that needs to be included for everything else to work right. `<!DOCTYPE html>` is the shortest string of characters that counts as a valid doctype. That is all you need to know!\n",
    "\n",
    "\n",
    "2. `<html></html>`: The `<html>` element. This element wraps all the content on the page. It is sometimes known as the root element.\n",
    "\n",
    "\n",
    "3. `<head></head>`: The `<head>` element. This element acts as a container for everything you want to include on the HTML page, **that isn't the content** the page will show to viewers. This includes keywords and a page description that would appear in search results, CSS to style content, character set declarations, and more. You will learn more about this in the next article of the series.\n",
    "\n",
    "\n",
    "4. `<meta charset=\"utf-8\">`: The `<meta>` element. This element represents metadata that cannot be represented by other HTML meta-related elements, like `<base>`, `<link>`, `<script>`, `<style>` or `<title>`. The charset attributes sets the character set for your document to UTF-8, which includes most characters from the vast majority of human written languages. With this setting, the page can now handle any textual content it might contain. There is no reason not to set this, and it can help avoid some problems later.\n",
    "\n",
    "\n",
    "5. `<title></title>`: The `<title>` element. This sets the title of the page, which is the title that appears in the browser tab the page is loaded in. The page title is also used to describe the page when it is bookmarked.\n",
    "\n",
    "\n",
    "6. `<body></body>`: The `<body>` element. This contains all the content that displays on the page, including text, images, videos, games, playable audio tracks, or whatever else.\n",
    "\n",
    "Later in this notebook, you will get to explore HTML codes in more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b8192c",
   "metadata": {},
   "source": [
    "#### HTML Tree Structure [<a href='#destination3'>5</a>] <a id='destination3_'></a>\n",
    " \n",
    "Each HTML document can actually be referred to as a document tree. We describe the elements in the tree like we would describe a family tree. There are ancestors, descendants, parents, children and siblings.\n",
    "\n",
    "Use the sample HTML document below for the following examples. The `<head>` section of the document is omitted for brevity.\n",
    "\n",
    "```\n",
    "<body>\n",
    "\n",
    "  <div id=\"content\">\n",
    "    <h1>Heading here</h1>\n",
    "    <p>Lorem ipsum dolor sit amet.</p>\n",
    "    <p>Lorem ipsum dolor <em>sit</em> amet.</p>\n",
    "    <hr>\n",
    "  </div>\n",
    "  \n",
    "  <div id=\"nav\">\n",
    "    <ul>\n",
    "      <li>item 1</li>\n",
    "      <li>item 2</li>\n",
    "      <li>item 3</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "\n",
    "</body>\n",
    "```\n",
    "\n",
    "A diagram of the above HTML document tree would look like this:\n",
    "\n",
    "<img src='images/tree1.gif' width=\"435\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "##### Ancestor\n",
    "\n",
    "An ancestor refers to any element that is connected but further up the document tree - no matter how many levels higher.\n",
    "\n",
    "In the diagram below, the `<body>` element is the ancestor of all other elements on the page.\n",
    "\n",
    "<img src='images/tree_ancestor.gif' width=\"435\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "##### Descendant\n",
    "\n",
    "A descendant refers to any element that is connected but lower down the document tree - no matter how many levels lower.\n",
    "In the diagram below, all elements that are connected below the `<div>` element are descendants of that `<div>`.\n",
    "\n",
    "<img src='images/tree_descendant.gif' width=\"435\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "##### Parent and Child\n",
    "\n",
    "A parent is an element that is directly above and connected to an element in the document tree. In the diagram below, the `<div>` is a parent to the `<ul>`.\n",
    "\n",
    "A child is an element that is directly below and connected to an element in the document tree. In the diagram above, the `<ul>` is a child to the `<div>`.\n",
    "\n",
    "<img src='images/tree_parent.gif' width=\"435\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "##### Sibling\n",
    "\n",
    "A sibling is an element that shares the same parent with another element.\n",
    "\n",
    "In the diagram below, the `<li>`s are siblings as they all share the same parent - the `<ul>`.\n",
    "\n",
    "<img src='images/tree_siblings.gif' width=\"435\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4aaec9",
   "metadata": {},
   "source": [
    "## B2.2. Static web scraping with Beautiful Soup\n",
    "\n",
    "<img src='images/bs.png' style='height: 150px; float: right; margin-left: 0px' >\n",
    "\n",
    "[Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is a Python library that makes it easy to scrape information from web pages. It sits atop (an interactive monitor to view the load on a Linux system) an HTML or XML parser, providing Pythonic idioms for iterating, searching, and modifying the parse tree.\n",
    "\n",
    "Now that you have an idea of how HTML webpages are structured, we can start working with Beautiful Soup. We will go through some of the most important methods of it, and then you will get to write your first scraping project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9063b51",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Caution</b>\n",
    "    \n",
    "If you need to the package installed on your system, use `pip` (check out [Session 1](https://github.com/gesiscss/css_methods_python/blob/main/a_introduction/1_computing_environment.ipynb) for installing packages), and then import the neccessary packages as the upcoming cell shows.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7412cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd6ec3",
   "metadata": {},
   "source": [
    "### B2.2.1. Basic functions\n",
    "\n",
    "[<a href='#destination4'>6, 7</a>] <a id='destination4_'></a>\n",
    "\n",
    "We will begin with an example page at http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html.\n",
    "\n",
    "The HTML source code of the page is stored in the `content` string as follows:\n",
    "\n",
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Pouria, please change the reference styling to APA and let's not have numbers for citations on the title.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caee8a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"<html>\n",
    "<head>\n",
    "<title>A simple example page</title>\n",
    "</head>\n",
    "<body>\n",
    "<div>\n",
    "<p class=\"inner-text first-item\" id=\"first\">\n",
    "                First paragraph.\n",
    "            </p>\n",
    "<p class=\"inner-text\">\n",
    "                Second paragraph.\n",
    "            </p>\n",
    "</div>\n",
    "<p class=\"outer-text first-item\" id=\"second\">\n",
    "<b>\n",
    "                First outer paragraph.\n",
    "            </b>\n",
    "</p>\n",
    "<p class=\"outer-text\">\n",
    "<b>\n",
    "                Second outer paragraph.\n",
    "            </b>\n",
    "</p>\n",
    "</body>\n",
    "</html>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a2e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML (content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aaa001",
   "metadata": {},
   "source": [
    "#### requests \n",
    "\n",
    "You can get the same content by fetching the page through `requests`. It is a simple and useful HTTP library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97637b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "page = requests.get(\"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\")\n",
    "content = page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d17b44",
   "metadata": {},
   "source": [
    "By printing `page`, you can check to see if fetching the contents has been successful. The status code of \"200\" means you are good to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179026d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page)\n",
    "# print(page.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cebf352",
   "metadata": {},
   "source": [
    "#### html parser\n",
    "\n",
    "By using its HTML parser, Beautiful Soup transforms a complex HTML document into a tree of python objects, so we can manage working with it easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97422f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b7bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0abe54",
   "metadata": {},
   "source": [
    "Using `soup.pretiffy()`, we can have a better tree overview of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a698b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b3646e",
   "metadata": {},
   "source": [
    "Each tag can now be viewed as an object. We can also access all children objects of a tag using dots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8308c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(soup.html.body.children)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27271a57",
   "metadata": {},
   "source": [
    "#### find() & find_all()\n",
    "\n",
    "Two  of the most important methods of Beautiful Soup are its `find` and `find_all()` methods.\n",
    "\n",
    "`find()` method finds the first occurence of a certain tag matching the given criteria. Its first argument is the tag name, so if we pass `p` as a string to it, it will return the first occurence of the `p` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9961befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d80db",
   "metadata": {},
   "source": [
    "As you can see, the output is the same as when we use a dot for accessing the `p` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84036703",
   "metadata": {},
   "source": [
    "With the `find_all()` method, we can get a list of all of the occurences of a certain tag matching the given criteria. Again, if we pass the \"p\" string to it, it will return all the occurences of the `p` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c90af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00426dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09040b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b47361",
   "metadata": {},
   "source": [
    "We can also specify attribute values and pass them to the method. The following line of code returns the list of all the `p` tags whose values for the `class` attribute is `\"outer-text\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fafee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p', {'class': \"outer-text\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd531a9c",
   "metadata": {},
   "source": [
    "This one returns the list of all tags whose `id` attributes equal `\"first\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(id=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2071c01",
   "metadata": {},
   "source": [
    "#### select()\n",
    "\n",
    "Beautiful Soup has a `select()` method which uses the [SoupSieve](https://facelessuser.github.io/soupsieve/) package to run a CSS selector against a parsed document and return all the matching elements.\n",
    "\n",
    "The SoupSieve documentation lists all the currently supported CSS selectors, but here are some of the basics;\n",
    "\n",
    "You can find tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fccfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select(\"p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709bfa5",
   "metadata": {},
   "source": [
    "You can find tags beneath other tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c801eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select(\"div p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59b867",
   "metadata": {},
   "source": [
    "You can find tags with specific classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select(\"p.first-item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6634f3b4",
   "metadata": {},
   "source": [
    "You can find tags by id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3a322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select(\"#second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3252a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select(\"p#second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4f458a",
   "metadata": {},
   "source": [
    "And you can also find tags by a combination of the above-mentioned criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dad313",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select(\"div p.first-item#first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf29d4b0",
   "metadata": {},
   "source": [
    "#### get_text()\n",
    "\n",
    "If you only want the human-readable text inside a document or tag, you can use the get_text() method. It returns all the text in a document or beneath a tag, as a single Unicode string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0541ec",
   "metadata": {},
   "source": [
    "You can tell Beautiful Soup to strip whitespace from the beginning and end of each bit of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b09d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.get_text(strip = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa593e22",
   "metadata": {},
   "source": [
    "You can also specify a string to be used to join the bits of text together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d45aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.get_text(\"|\", strip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf983fe",
   "metadata": {},
   "source": [
    "But at that point you might want to use the `stripped_strings` generator instead, and process the text yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d829e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[text for text in soup.stripped_strings]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f715206",
   "metadata": {},
   "source": [
    "### B2.2.2. Static web scraping of Aljazeera\n",
    "\n",
    "Depending on our research projects, we might need data from different sources. For example, if we want to investigate the news exposure in specific countries and compare the topics of news, the first step will be collecting news articles from news websites. Or, if our project is related to understanding the subjects discussed at the European Union meetings, we first should have those meeting minutes at hand. These two project examples' data collection processes are explained in detail in this section. You can always think of another project, which might use and combine similar techniques that are introduced here. \n",
    "\n",
    "[<a href='#destination5'>8</a>] <a id='destination5_'></a>\n",
    "\n",
    "<img src='images/aljazeera.png' style='height: 150px; float: right; margin-left: 50px' >\n",
    "\n",
    "Now that you are familiar with the basics of Beautiful Soup, we can do a more practical scraping project for the news exposure in Turkey on a particular news website [aljazeera.com](https://www.aljazeera.com), and we will get to practice what you have learnt so far.\n",
    "\n",
    "To have a better idea of what exactly we are going to do, go the [Aljazeera website](https://www.aljazeera.com), use the search bar and search \"Turkey\". In the new page, sort the retrieved news articles by date. As you can see, the 10 most recent news articles related to Turkey are now displayed. We are going to scrape and store these articles and their relavent information (e.g., title, text, or url) in a pandas dataframe.\n",
    "\n",
    "First, we need to make sure we have all the necessary packages available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9aa2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these libraries if you have not done so\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e37ca3",
   "metadata": {},
   "source": [
    "Then we construct the right URL from `address` and `searchterm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764c3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "address = \"https://www.aljazeera.com/search/\"\n",
    "searchterm = \"Turkey\"\n",
    "parameters = \"?sort=date\"\n",
    "url = address + searchterm + parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a8a5fe",
   "metadata": {},
   "source": [
    "The resulting URL is the same as that of the page you explored at the first stage. Now we fetch it using `requests`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b87ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(url) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce151ebd",
   "metadata": {},
   "source": [
    "Then we parse the webpage with Beautiful Soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a109acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8218e6a",
   "metadata": {},
   "source": [
    "Now that we have the page parsed, we need to select the right elements of it to extract our desired information from. In the simple webpage that we investigated in the Beautiful Soup Basics section, it was easy to pick the right elements to investigate from the few lines of code. In real HTML web pages it's a bit different.\n",
    "\n",
    "In order to find the right elemets, right-click somewhere on the page and click on *inspect*. Then press Ctrl+Shift+C. Now you should be able to inspect the page and see the HTML code for each part of the page you hover the mouse. Equivalently, by hovering the mouse on certain lines of HTML code you can see what that code actually creates on the page.\n",
    "\n",
    "On Google Chrome it would look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f825da",
   "metadata": {},
   "source": [
    "<img src='images/inspect.png' style='height: 550px; float: right; margin-left: 50px' >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca068b0",
   "metadata": {},
   "source": [
    "It turns out that the elements that we would like to work on are the ones with the `article` tags. We'll select them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = soup.select('article')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b192b5",
   "metadata": {},
   "source": [
    "Next, we will scrape different information from the articles. We do that by putting every article's title, text and URL in a corresponding dictionary, and will add all the dictionaries to the `results` list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433b3106",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bbfd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty list for results\n",
    "results = []\n",
    "\n",
    "for article in articles: \n",
    "    \n",
    "    # Initialize empty dictionary\n",
    "    # Extract title, text and URL of articles \n",
    "    item = {}\n",
    "    item['title'] = article.select_one('span').text.strip()\n",
    "    item['text'] = article.select_one('p').text.strip()    \n",
    "    item['url'] = article.select_one('a').get('href')\n",
    "    item['date'] = articles[0].select('span')[2].text\n",
    "    # You can also get the URLs with article.select_one('a')['href']\n",
    "    \n",
    "    # Append items to result-list\n",
    "    results.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfab398",
   "metadata": {},
   "source": [
    "At last, we convert the results list to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2825893",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9400d",
   "metadata": {},
   "source": [
    "We can save the resulting dataframe in a csv file. You can access the file in the `outputs` folder in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8413263",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/aljazeera.csv', mode = 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d5a9cb",
   "metadata": {},
   "source": [
    "BOX: Scraping many pages (McLevey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f43cb0",
   "metadata": {},
   "source": [
    "### B2.3. Parsing RSS feeds [<a href='#destination1'>1, 2, 3</a>] <a id='destination1_'></a>\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Web_feed), a web feed (or news feed) is a data format used for providing users with frequently updated content. Content distributors syndicate a web feed, thereby allowing users to subscribe a channel to it by adding the feed resource address to a news aggregator client (also called a feed reader or a news reader). Users typically subscribe to a feed by manually entering the URL of a feed or clicking a link in a web browser or by dragging the link from the web browser to the aggregator, thus \"RSS and Atom files provide news updates from a website in a simple form for your computer.\"\n",
    "\n",
    "Here we introduce [feedparser](https://pypi.org/project/feedparser/), a powerful python package for parsing RSS feeds. By providing the RSS feed link, we can get structured information in the form of python lists and dictionaries, which could then be used to extract the desired information in a simple and efficient way.\n",
    "\n",
    "\n",
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Pouria, please change the citations to APA format, below Feed Parsing also has three references in a number format. We should integrate these references into the text not on the title. Does that sound clear? Thanks!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d4f99",
   "metadata": {},
   "source": [
    "#### Getting started with feedparser\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Hint:</b> \n",
    "    \n",
    "Before importing the libraries, we need to have the neccessary software packages and libraries installed. You can always go back to [Session 1: Setting up the computing environment](https://github.com/gesiscss/css_methods_python/blob/main/a_introduction/1_computing_environment.ipynb) to learn about how to install software packages and libraries that you need for this session.\n",
    "    \n",
    "</div>\n",
    "\n",
    "As usual, we need to import the package in the first place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeff60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9394ab",
   "metadata": {},
   "source": [
    "#### Parsing an RSS feed URL\n",
    "To parse an RSS feed link, you can simply use the **parse()** method from the feedparser package. It takes a string as argument, which could be a URL or the address to the file locally saved on the computer. Here we use CNN RSS as an example URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab913e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed = feedparser.parse(\"https://www.voanews.com/api/zgvmqye_o_qv\")\n",
    "\n",
    "# You can try other news websites as well:\n",
    "\n",
    "# feed = feedparser.parse(\"https://www.aljazeera.com/xml/rss/all.xml\")\n",
    "# feed = feedparser.parse(\"http://rss.cnn.com/rss/edition_europe.rss\")\n",
    "\n",
    "feed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e90b94",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Hint:</b> \n",
    "You can try the following ways in order to get a website's RSS feed:\n",
    "\n",
    "- If the website is powered by Wordpress, you can do it by adding /feed/ at the end of its URL. Trying /rss/ is another option.\n",
    "<img src='images/rss_logo.png' style='height: 50px; float: right; margin-left: 50px' >\n",
    "- If you see the standard orange RSS logo, by simply clicking on it you will be taken to the website's RSS feed.\n",
    "- You can also use the page source: right click on the page and choose page source. In the new window, use ctrl+f and type in RSS. You’ll find the feed’s URL between the quotes after **href=**.\n",
    "\n",
    "The parse method fetches the feed from the provided URL, extracts the information in a systematic way and stores each piece in a structured format. At the high level, it returns a python dictionary with multiple keys and values, in which each value may contain python lists or other dictionaries. You can access the keys using the **keys()** method:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d7224",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75073c74",
   "metadata": {},
   "source": [
    "Using these keys, we can access the more specific information that we want. The most common keys that can be used for extracting information are **entries** and **feed**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e6c4e0",
   "metadata": {},
   "source": [
    "#### Extracting the contents from the feed\n",
    "We will start with the **entries** key. We can get the list of all the posts/podcasts/entries or any other form of content the feed is serving for, from the **entries** key in the dictionary. More information on other possible keys in the returned dictionary can be found [here](https://feedparser.readthedocs.io/en/latest/reference.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd78dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = feed['entries']\n",
    "entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7514602d",
   "metadata": {},
   "source": [
    "We can get the number of articles/entries using the **len()** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc57a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1d29e4",
   "metadata": {},
   "source": [
    "#### Getting details of the entries\n",
    "We can iterate over the items of the entries list and print them to get more details on each article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dc5f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in entries:\n",
    "    print (entry)\n",
    "    print (\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fcb394",
   "metadata": {},
   "source": [
    "As we can see, each entry in the list is a dictionary again, which has different key-value pairs like **title**, **summary**, **link**, etc. We can again use the **keys()** method in order to explore the keys of the new dictionary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbba0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2c8329",
   "metadata": {},
   "source": [
    "Now that we have all the keys associated with the entries, we can extract the specific information like title, author, and actual contents of the feed.\n",
    "Though this might not be the same for all RSS feeds, it might be very similar and a matter of using the right keyword for the associated keys in the list of dictionaries.\n",
    "\n",
    "Let's say, we want to print out the titles of all the entries in the feed, and save them to the `titles` list. We can do that by iterating over the entries list and fetching the title from the iterator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab5e523",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "\n",
    "for entry in entries:\n",
    "    titles.append(entry.title)\n",
    "    print (entry.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4804d",
   "metadata": {},
   "source": [
    "Similarly, we can get the links, summaries, publishing dates and tags of the entries using the corresponding keys in the dictionary. We will save them in lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5cf57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "summaries = []\n",
    "published = []\n",
    "tags = []\n",
    "\n",
    "for entry in entries:\n",
    "    links.append(entry.link)\n",
    "    summaries.append(entry.summary)\n",
    "    published.append(entry.published)\n",
    "    tags.append(entry.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6386bc",
   "metadata": {},
   "source": [
    "Now we put all of the results in a pandas dataframe and then save it to a csv file. You can find it in the `outputs` folder, which is located in our current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2269a2f0",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Caution</b>\n",
    "\n",
    "Pandas dataframes are used for keeping data in a well-structured manner. If you are not familiar with the basics of dataframes, check out [session 2: data handling and visualization](https://github.com/gesiscss/css_methods_python/blob/main/a_introduction/2_data_handling_and_visualization.ipynb).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b38cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "feeds_df = pd.DataFrame([titles, published, summaries, links, tags]).transpose()\n",
    "feeds_df.columns = ['title', 'published', 'summary', 'link', 'tags']\n",
    "\n",
    "feeds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68b07fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feeds_df.to_csv('./results/feeds_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b2d48",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Recommended readings\n",
    "\n",
    "Bosse, S., Dahlhaus, L., & Engel, U. (2022) \"Web data mining: Collecting textual data from web pages using R.\" In: Engel, U. & Quan-Haase, A. (eds), *Handbook of Computational Social\n",
    "Science*, vol. 2, p. 46–70. Abingdon: Routledge. https://doi.org/10.4324/9781003025245.\n",
    "\n",
    "<a id='mclevey_doing_2022'></a>\n",
    "McLevey, J. (2022). *Doing Computational Social Science: A Practical Introduction*. SAGE. https://us.sagepub.com/en-us/nam/doing-computational-social-science/book266031. *A rather complete introduction to the field with well-structured and insightful chapters also on using Pandas. The [website](https://github.com/UWNETLAB/dcss_supplementary) offers the code used in the book.*\n",
    "\n",
    "### Complementary readings\n",
    "\n",
    "Diouf, R., Sarr, E. N., Sall, O., Birregah, B., Bousso, M., & Mbaye, S. N. (2019). \"Web scraping:\n",
    "State-of-the-art and areas of application.\" In: *2019 IEEE International Conference\n",
    "on Big Data*, p. 6040–6042. IEEE, Los Angeles, CA. https://doi.org/10.1109/BigData47090.2019.9005594.\n",
    "\n",
    "Fatmasari, Kunang, Y. N., and Purnamasari, S. D. (2018). \"Web scraping techniques to collect\n",
    "weather data in South Sumatera.\" In: *2018 International Conference on Electrical\n",
    "Engineering and Computer Science (ICECOS)*, p. 385–390. Pangkal, Indonesia. IEEE. https://doi.org/10.1109/ICECOS.2018.8605202.\n",
    "\n",
    "Han, S., & Anderson, C. K. (2021). \"Web scraping for hospitality research: Overview, opportunities, and implications.\" *Cornell Hospitality Quarterly* 62:89–104. https://doi.org/10.1177/1938965520973587.\n",
    "\n",
    "Hillen, J. (2019). \"Web scraping for food price research.\" *British Food Journal* 121:3350–3361. https://doi.org/10.1108/BFJ-02-2019-0081.\n",
    "\n",
    "Patel, J. M. (2020). \"Web scraping in Python using Beautiful Soup library.\" In: *Getting Structured Data from the Internet*, p. 31–84. Apress, Berkeley, CA. https://doi.org/10.1007/978-1-4842-6576-5_2.\n",
    "\n",
    "___\n",
    "\n",
    "[<a href='#destination1_'>1</a>] https://pypi.org/project/feedparser/ <a id='destination1'></a>\n",
    "\n",
    "[<a href='#destination1_'>2</a>] https://rss.com/blog/find-rss-feed/#:~:text=Right%20click%20on%20the%20website's,between%20the%20quotes%20after%20href%3D\n",
    "\n",
    "[<a href='#destination1_'>3</a>] https://dev.to/mr_destructive/feedparser-python-package-for-reading-rss-feeds-5fnc\n",
    "\n",
    "[<a href='#destination2_'>4</a>] https://developer.mozilla.org/en-US/docs/Learn/HTML/Introduction_to_HTML/Getting_started <a id='destination2'></a>\n",
    "\n",
    "[<a href='#destination3_'>5</a>] http://web.simmons.edu/~grabiner/comm244/weekfour/document-tree.html <a id='destination3'></a>\n",
    "\n",
    "[<a href='#destination4_'>6</a>] https://www.crummy.com/software/BeautifulSoup/bs4/doc/ <a id='destination4'></a>\n",
    "\n",
    "[<a href='#destination4_'>7</a>] Fabian's notebook from GESIS fall seminar 2021: https://colab.research.google.com/drive/1uKxOc8mXTE2b05uUq-YlijJYzOTgi5DZ#scrollTo=ao_sLGiOSu7Y\n",
    "\n",
    "[<a href='#destination5_'>8</a>] The Social Comquant Workshop 10 at https://github.com/strohne/autocol <a id='destination5'></a>\n",
    "\n",
    "___\n",
    "\n",
    "https://realpython.com/beautiful-soup-web-scraper-python/#reasons-for-web-scraping\n",
    "\n",
    "https://medium.com/pythoneers/the-fundamentals-of-web-scraping-using-python-its-libraries-6f146b91efb4\n",
    "\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "https://www.dataquest.io/blog/web-scraping-python-using-beautiful-soup/#tve-jump-1788432a71d\n",
    "\n",
    "https://developer.mozilla.org/en-US/docs/Web/HTML/Element\n",
    "\n",
    "https://medium.com/geekculture/web-scraping-cheat-sheet-2021-python-for-web-scraping-cad1540ce21c#b81d\n",
    "\n",
    "https://trends.google.com/trends/yis/2021/DE/\n",
    "\n",
    "https://blog.google/products/search/15-tips-getting-most-out-google-trends/\n",
    "\n",
    "https://limeproxies.netlify.app/blog/selenium-vs-beautifulsoup\n",
    "\n",
    "https://github.com/strohne/autocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e75646",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "<b>Document information</b>\n",
    "\n",
    "Contact and main author: Pouria Mirelmi & N. Gizem Bacaksizlar Turbic\n",
    "\n",
    "Contributors: Felix Beck-Soldner & Haiko Lietz\n",
    "\n",
    "Acknowledgements: Fabian Flöck\n",
    "\n",
    "Version date: 18 August 2023\n",
    "\n",
    "License: Creative Commons Attribution 4.0 International (CC BY 4.0)\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
