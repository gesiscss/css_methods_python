{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/gesis.png' style='height: 50px; float: left'>\n",
    "<img src='images/social_comquant.png' style='height: 50px; float: left; margin-left: 40px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Computational Social Science methods with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6: Natural Language Processing\n",
    "The field of study that focuses on the interactions between human language and computers is called natural language processing (NLP).\n",
    "\n",
    "NLP is a field of artificial intelligence in which computers analyze, understand, and derive meaning from human language in a smart and useful way. NLP systems are used exploiting the signals in our language used to predict all of the aforementioned features: people’s age (Nguyen et al., 2011; Rosenthal & McKeown, 2011), gender (Alowibdi et al., 2013; Ciot et al., 2013; Liu & Ruths, 2013), personality (Park et al., 2015), job title (Preoţiuc-Pietro et al., 2015a), income (Preoţiuc-Pietro et al., 2015b), and much more (Volkova et al., 2014, 2015).\n",
    "\n",
    "In NLP, word embeddings have been at the forefront of this progress, which has expanded to include flexible model architectures (Hovy, 2021). The most publicly visible example of this shift is probably the translation quality of services like Google Translate (Wu et al., 2016).\n",
    "\n",
    "A collection of fundamental tasks appear frequently across various NLP projects (Vajjala et al., 2020). Let’s briefly introduce them (Figure 1):\n",
    "\n",
    "<img src='images/nlp_tasks.png' style='height: 400px; float: right'>\n",
    "\n",
    "*Language modeling* is the task of predicting what the next word in a sentence will be based on the history of previous words. The goal of this task is to learn the probability of a sequence of words appearing in a given language. Language modeling is useful for building solutions for a wide variety of problems, such as **speech recognition, optical character recognition, handwriting recognition, machine translation, and spelling correction**.\n",
    "\n",
    "*Text classification* is the task of bucketing the text into a known set of categories based on its content. Text classification is by far the most popular task in NLP and is used in a variety of tools, from **email spam identification** to **sentiment analysis**.\n",
    "\n",
    "*Information extraction* is the task of extracting relevant information from text, such as **calendar events from emails** or the **names of people mentioned** in a social media post.\n",
    "\n",
    "*Information retrieval* is the task of finding documents relevant to a user query from a large collection. Applications like **Google Search** are well-known use cases of information retrieval.\n",
    "\n",
    "*Conversational agent* is the task of building dialogue systems that can converse in human languages. **Alexa** and **Siri** are some common applications of this task.\n",
    "\n",
    "*Text summarization* aims to create short summaries of longer documents while retaining the **core content** and preserving the **overall meaning** of the text.\n",
    "\n",
    "*Question answering* is the task of building a system that can automatically answer questions posed in natural language.\n",
    "\n",
    "*Machine translation* is the task of converting a piece of text from one language to another. Tools like **Google Translate** are common applications of this task.\n",
    "\n",
    "*Topic modeling* is the task of uncovering the topical structure of a large collection of documents. Topic modeling is a common text-mining tool and is used in a wide range of domains, from **literature** to **bioinformatics**.\n",
    "\n",
    "\n",
    "\n",
    "Understanding human language is considered as a difficult task due to its complexity. For example, there is an infinite number of different ways to arrange words in a sentence. \n",
    "\n",
    "Also, words can have several meanings and contextual information is necessary to correctly interpret sentences as every language is unique and ambiguous. The ambiguity can be in lexical and syntactic forms.\n",
    "\n",
    "- In lexical ambiguity, a single word has two or more possible meanings. For example, \"I saw bats\".\n",
    "- In syntactic ambiguity, a single sentence or a sequence of words have multiple possible meanings. For example, \"The chicken is ready to eat\".\n",
    "\n",
    "This session will help you understand the basic and advanced NLP concepts and show you how to implement using the most advanced and popular NLP libraries, such as  <a href=\"https://spacy.io/\">spaCy</a> and <a href=\"https://radimrehurek.com/gensim/\">Gensim</a>.\n",
    "\n",
    "<!-- – <a href=\"https://www.nltk.org/\">NLTK</a>, <a href=\"https://spacy.io/\">spaCy</a>, <a href=\"https://radimrehurek.com/gensim/\">Gensim</a>, and <a href=\"https://huggingface.co/\">Hugging Face</a>. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "<b>In this session</b>, \n",
    "\n",
    "you will learn about basics for the Natural Language Processing. In subsession **6.1**, we will extract useful information / facts (communication symbols) from tweets. In **6.2**, we will show how to implement a text preprocessing pipeline using XXX data at the end of which stands the document-term matrix that is ready for analysis (such as topic modeling). In **6.3**, we will deal with word and document similarities using similarity metrics and word/document embeddings (not the pretrained ones); also Zipf's Law.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<big><b>Reminder</b></big>\n",
    "    \n",
    "ONLY use pip unless there is no Conda option. Please make sure that ALL packages we need are installed. If you need further information how to install and import packages and libraries, please check out \n",
    "<a href=\"https://github.com/gesiscss/css_methods_python/blob/main/a_introduction/1_computing_environment.ipynb\"> Session 1: Computing environment </a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Extracting entities from tweet texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<big><b>Notes from the content document (remove after finalizing it)</b></big>\n",
    "    \n",
    "Extracting information from tweets; start with using regular expressions (Hovy has section; Ali has slides in summer school 2022) to extract tweets with certain properties (e.g., tweets with hashtags, tweets with URLs); demonstrate that it is important to remove punctuation at a certain step in the extraction process (certainly after URL extraction but before hashtag extraction); sensitize users about the order of operations; data: table of tweet texts of the 100 most retweeted tweets in the TweetsCOV19 dataset from session 2\n",
    "mentioned users\n",
    "hashtags\n",
    "urls\n",
    "named entities (with Spacy: https://www.analyticsvidhya.com/blog/2021/06/nlp-application-named-entity-recognition-ner-in-python-with-spacy/; or does gensim have anything?)\n",
    "lexicon-based sentiment (using spacy of pysenti? Or gensim?)\n",
    "emojis\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1. Extracting facts using regular expressions\n",
    "\n",
    "<a href=\"https://docs.python.org/3/howto/regex.html\">Regular expressions</a> (called regex, regexes, regex pattern, regexp, or REs) specify search patterns. Typical examples of regular expressions are the patterns for matching email addresses, phone numbers, and credit card numbers.\n",
    "\n",
    "Regular expressions are essentially a specialized programming language embedded in Python, and you can interact with regular expressions via the built-in `re` module in Python, which has some functions that match a string for a pattern:\n",
    "\n",
    "- `match()`\n",
    "- `search()`\n",
    "- `findall()`\n",
    "- `finditer()`\n",
    "\n",
    "Pattern... character set..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Pouria, can you please add here the image about regex functions, a nicer one maybe you can find online and cite it of course??? Add logos for the libraries, such as pandas, regex, spacy, gensim and if I forget anything, pls check out the notebook and add those logos like we did in other sessions. Thank you!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the top 500 retweeted tweets from the TweetsCOV19 dataset, which was introduced in [Session 2: Data handling and visualization](https://github.com/gesiscss/css_methods_python/blob/main/a_introduction/2_data_handling_and_visualization.ipynb). To read and practice with this data, we need to import neccessary libraries below. If you have some difficulties with importing/installing, please check out the [Session 1: Computing environment](https://github.com/gesiscss/css_methods_python/blob/main/a_introduction/1_computing_environment.ipynb) for further installation information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('../data/TweetsCOV19/top_500_retweeted_tweets.csv', encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start using `findall` function from regex to extract **urls, mentions, and hashtags** in tweets (i.e., the column of text in our dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['urls'] = tweets_df['text'].apply(lambda x: re.findall(\"http[s]*\\S+\", x))\n",
    "\n",
    "tweets_df['mentions'] = tweets_df['text'].apply(lambda x: re.findall(\"@([a-zA-Z0-9_]{1,50})\", x))\n",
    "# if you want to keep mention sign with mentioned string, use this following code\n",
    "#tweets_df['mentions'] = tweets_df['text'].apply(lambda x: re.findall(r'@\\w+ ?', x))\n",
    "\n",
    "tweets_df['hashtags'] = tweets_df['text'].apply(lambda x: re.findall(\"#([a-zA-Z0-9_]{1,50})\", x))\n",
    "# if you want to keep hashtag sign hashtag's string, use this following code\n",
    "#tweets_df['hashtags'] = tweets_df['text'].apply(lambda x: re.findall(r'#\\w+ ?', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For emoji extraction, in addition to regex, we will use the library called emoji (if not installed before, please install it before running the following cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "    text_de= emoji.demojize(text)\n",
    "    emojis_list_de= re.findall(r'(:[!_\\-\\w]+:)', text_de)\n",
    "    list_emoji= [emoji.emojize(x) for x in emojis_list_de]\n",
    "    return(list_emoji)\n",
    "\n",
    "tweets_df['emojis'] = tweets_df['text'].apply(extract_emojis)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results from our entity extraction example and sort values according to mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_df.sort_values(by = 'mentions', ascending =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean text from urls, hashtags, mentions, and emojis for further text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_links(text):\n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], ' ') # remove links\n",
    "    return text\n",
    "\n",
    "def clean_all_entities(text):\n",
    "    entity_prefixes = ['#', '@'] # remove hashtags and mentions\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "            text = emoji.replace_emoji(text, replace=\"!\") # remove emojis\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "tweets_df['cleaned_text'] = tweets_df['text'].apply(lambda x: clean_all_entities(clean_links(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.cleaned_text.values[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2. Extracting named entities\n",
    "\n",
    "A named entity is a real-life object which can be identified and denoted with a proper name. Named Entities can be a place, person, organization, time, object, or geographic entity. For example, named entities would be Joe Biden, New York city, and congress. Named entities are usually instances of entity instances. For example, Joe Biden is an instance of a politician/person, New York City is an instance of a place, and congress is as instance of an organization. \n",
    "\n",
    "**Named Entity Recognition** (NER) is the process of NLP for identifying and classifying named entities. The raw and structured text are used to find out named entities, which are classified into persons, organizations, places, money, time, etc. NER systems are developed with various linguistic approaches, as well as statistical and machine learning methods. \n",
    "\n",
    "NER model first identifies an entity and then categorizes the entity into the most suitable class. Some of the common types of Named Entities will be as follows and others can be found in the further example of a Wikipedia page text.\n",
    "\n",
    "1. Organisations : NASA, CERN\n",
    "\n",
    "2. Places: Istanbul, Germany\n",
    "\n",
    "3. Money: 1 Billion Dollars, 50 Euros\n",
    "\n",
    "4. Date: 24th January 2023, season 4\n",
    "\n",
    "5. Person: Richard Feynman, George Floyd\n",
    " \n",
    "<img src='images/NER.png' style='height: 500px; float: left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Insight</b></big>\n",
    "\n",
    "    \n",
    "For NLP tasks like NER, POS tagging, dependency parsing, word vectors and more, <a href=\"https://spacy.io/\">spaCy</a> has distinct features that provide clear advantage for processing text data and modeling. It is the most trending and advanced free open-source library for implementing NLP in Python nowadays. \n",
    "    \n",
    "An important thing about NER models is that their ability to understand Named Entities depending on the data they have been trained on. There are many applications of NER. NER can be used for content classification, the various Named Entities of a text can be collected, and based on that data, the content themes can be understood.\n",
    "    \n",
    "We can use spaCy very easily for NER tasks. However, we need to consider training our own data for research, commercial, and business specific needs, the spaCy model generally performs well for all types of text data. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's import necessary libraries and packages and start with a toy example from our tweets dataframe, which is the second line of the text column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "raw_text = tweets_df.text[1]\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we print the data on the Named Entities found in this raw text sample from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_text = NER(raw_text)\n",
    "\n",
    "for word in NER_text.ents:\n",
    "    print(word.text, word.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Insight</b></big>\n",
    "    \n",
    "Here, PEP is a context specific word to be labeled as organization or not. In the COVID19 case like in our example, it stands for \"personal protective equipment\"; which is not an organization. On the other hand, as an abbreviation of the Philosophy, Politics, and Economics Society, PEP can be labeled as an organization.\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run NER on the full dataset and check out the output with Named Entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tweet in tweets_df.text:\n",
    "    for word in NER(tweet).ents:\n",
    "        print(word.text, word.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Text representation: Implementing a preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<big><b>Notes from the content document (remove after finalizing it):</b></big>\n",
    "    \n",
    "\n",
    "Text representation: creating the document-term matrix; creating the preprocessing pipeline (using Spacy; for inspiration: https://mahadev001.github.io/Mahadev-Upadhyayula/Sentiment%20Analysis%20via%20NLP/Sentiment%20Analysis%20using%20NLP%20with%20Spacy%20and%20%20SVM.html); output should be a sparse matrix for use in ML\n",
    "Tokenization\n",
    "Stopword removal\n",
    "Stemming vs lemmatization\n",
    "POS tagging\n",
    "N-gram detection (mention that results differ depending on which position in the pipeline it’s performed: before/after stopword removal or lemmatization)\n",
    "tf*idf\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Pouria, can you please add this link to the references?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1. Word Descriptors\n",
    "\n",
    "To refer to the entire collection of documents/observations, we use the word corpus (plural corpora). The raw text data often referred to as *text corpus* has punctuations, suffices, and stop words that do not give us important information. To have more useful information for NLP tasks, Text Preprocessing involves preparing the text corpus. Let's start with basic terminology of NLP.\n",
    "\n",
    "### Tokens and splitting \n",
    "\n",
    "The set of all the unique terms in our data is called the vocabulary. Each element in this set is called a type. Each occurrence of a type in the data is called a token. \n",
    "\n",
    "Let's practice: Our sentence “Today is a great day with learning NLP, such a power tool!”, has 14 tokens but only 13 types (namely, 'Today', 'is', 'a', 'great', 'day', 'with', 'learning', 'NLP', ',', 'such', 'a', 'powerful', 'tool', '!'). Note that types can also include punctuation marks and multiword expressions.\n",
    "\n",
    "In other words, the words of a text document/file separated by spaces and punctuation are called as tokens.\n",
    "\n",
    "#### What is a Tokenization?\n",
    "The process of extracting tokens from a text file/document is referred as tokenization.\n",
    "\n",
    "### Lemmatization\n",
    "When we look up a word in a dictionary, we usually just look for the base form. This dictionary base form is called the **lemma**.\n",
    "For instance, we might see forms like “go,” “goes,” “went,”, “gone,” or “going” and we look up dictionary in a lemmatized form, such as \"go\" (Hovy, 2020).\n",
    "\n",
    "### Stemming \n",
    "\n",
    "From Hovy's book - we should rewrite and summarize: \"Rather than reducing a word to the lemma, we strip away everything but the irreducible morphological core (the stem). For example, for a word like “anticonstitutionalism”, which can be analyzed as “anti+constitut+ion+al+ism,” we remove everything but “constitut.” The most famous and commonly used stemming tool is based on the algorithm developed by Porter (1980). For each language, it defines a number of suffixes (i.e., word endings) and the order in which they should be removed or replaced. By repeatedly applying these actions, we reduce all words to their stems. In our example, all words derive from the stem “constitut–” by attaching different endings. Again, a version of the Porter stemmer is already available in Python, in the nltk library (Loper & Bird, 2002), but we have to specify the language.\"\n",
    "\n",
    "### n-grams\n",
    "**N-grams** are continuous sequences of words or symbols or tokens in a document. In technical terms, they can be defined as the neighbouring sequences of items in a document. They come into play when we deal with text data in NLP tasks. 'n' in n-grams refers to positive integer values including 1,2,3 and so on. The term becomes unigram (n=1), bigram (n=2), trigram (n=3), .. to n-gram (n=n). The different types of n-grams are suitable for different types of applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2. Stopwords\n",
    "\n",
    "Stop words are a set of commonly used words in any language. For example, in English, “the”, “is”, and “and” would easily qualify as stop words. In NLP and text mining applications, stop words are used to eliminate unimportant words, allowing applications to focus on the important words instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Insight</b></big>\n",
    "    \n",
    "<a href=\"https://docs.python.org/3/library/string.html\">Python String</a> module contains some constants, utility function, and classes for string manipulation. It is a built-in module and we have to import it before using any of its constants and classes.\n",
    "\n",
    "<a href=\"https://spacy.io/\">spaCy</a> is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more that we will practice in the following sections.\n",
    "</div>\n",
    "\n",
    "<img src='images/spacy_pipeline.svg' style='height: 200px; float: right'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage, we will use spaCy to lemmatize and remove stop words from our tweets dataset. We will also list the stop words to observe: We can keep some of them depdending on our research questions (e.g., 'us' vs. 'them') or remove them fully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "#nlp = spacy.load('en')\n",
    "\n",
    "# To build a list of stop words for filtering\n",
    "stopwords = list(STOP_WORDS)\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will learn how to tokenize words to be lemmatized and filtered for pronouns, stopwords and punctuations by the 'tokenize_text' function. A text can be converted into nlp object of spaCy. First, we should convert raw text into a spaCy object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = string.punctuation\n",
    "# Creating a Spacy Parser\n",
    "from spacy.lang.en import English\n",
    "# parser = English()\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "def lemmatize_text(text):\n",
    "    tokens = nlp(text)\n",
    "    tokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens ]\n",
    "    tokens = [ word for word in tokens if word not in stopwords and word not in punctuations ]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following line, we apply 'tokenize_text' function for removing stop words and lemmatizing tokens to ...\n",
    "\n",
    "INTRODUCE TEXT CORPUS NOW (AMAZON REVIEWS?) -- DON'T USE TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['lemmatized_text'] = df['cleaned_text'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df['lemmatized_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.3. Parts of Speech\n",
    "The **Part of speech tagging** (POS tagging) is the process of marking a word in the text to a particular part of speech based on both its context and definition. In simple language, we can say that POS tagging is the process of identifying a word as nouns, pronouns, verbs, adjectives, etc.\n",
    "\n",
    "English has 9 main categories:\n",
    "\n",
    "verb — Expresses an action or a state of being. E.g. jump, is, write, become\n",
    "noun — identifies a person, a place or a thing or names of particular of one of these (pronoun). E.g. man, house, happiness\n",
    "pronoun — can replace a noun or noun phrase. E.g. she, we, they, it\n",
    "determiner — Is placed in front of a noun to express a quantity or clarify what the noun refers to — briefly a noun introducer. E.g. my, that, the, many\n",
    "adjective — modifies a noun or a pronoun. E.g. pretty, old, blue, smart\n",
    "adverb — modifies a verb, an adjective, or another adverb. E.g. gently, extremely, carefully, well\n",
    "preposition — Connect a noun/pronoun to other parts of the sentence. E.g. by, with, about, until\n",
    "conjunction — glue words, clauses, and sentences together. E.g. and, but, or, while, because\n",
    "interjection — Expresses emotion in a sudden or exclamatory way. E.g. oh!, wow!, oops!\n",
    "\n",
    "\n",
    "<img src='images/POS.png' style='height: 200px; float: right'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "    \n",
    "Pouria, can you please check other images for POS tagging that you find it beautiful and comprehensive that shows all the tags? I found this now but we can change. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply POS tagging on our dataset, first, let's tag the tokens. First, to build the dataframe with POS tags, we will have columns as \"text\", \"pos\", \"tag\", \"explain_tag\", then we will write rows from the tokenized text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"text\", \"pos\", \"tag\", \"explain_tag\"]\n",
    "rows = []\n",
    "\n",
    "for text in df['text']:\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        row = token.text, token.pos_, token.tag_, spacy.explain(token.tag_)\n",
    "        rows.append(row)\n",
    "    \n",
    "pos_df = pd.DataFrame(rows, columns = cols)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4. n-gram extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Insight</b></big>\n",
    "    \n",
    "<a href=\"https://textacy.readthedocs.io/en/0.12.0/api_reference/extract.html\">textacy</a> is a Python library for performing a variety of natural language processing (NLP) tasks, built on the high-performance spaCy library. With the fundamentals — tokenization, part-of-speech tagging, dependency parsing, etc. — delegated to another library, textacy focuses primarily on the tasks that come before and follow after. We will use textacy for extracting basic ngrams. If you have not installed yet this library, please do so before moving to the following exercise.\n",
    "     \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "\n",
    "ngrams_list = []\n",
    "\n",
    "for text in df['cleaned_text']:\n",
    "    doc = nlp(text)\n",
    "    # you can change the n grams to 3, 4 depending on your quesiton\n",
    "    ngrams = list(textacy.extract.basics.ngrams(doc, 3, min_freq=2))\n",
    "    if ngrams != []:\n",
    "        ngrams_list.append(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "    \n",
    "Or, we can introduce gensim here??... another option is shown as well. we can discuss. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, pprint \n",
    "\n",
    "# tokenize documents with gensim's tokenize() function\n",
    "tokens = [list(gensim.utils.tokenize(doc, lower=True)) for doc in df['text']]\n",
    "\n",
    "# build bigram model\n",
    "bigram_mdl = gensim.models.phrases.Phrases(tokens, min_count=1, threshold=2)\n",
    "\n",
    "# do more pre-processing on tokens (remove stopwords, stemming etc.)\n",
    "# NOTE: this can be done better\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, stem_text\n",
    "CUSTOM_FILTERS = [remove_stopwords, stem_text]\n",
    "tokens = [preprocess_string(\" \".join(doc), CUSTOM_FILTERS) for doc in tokens]\n",
    "\n",
    "# apply bigram model on tokens\n",
    "bigrams = bigram_mdl[tokens]\n",
    "\n",
    "pprint.pprint(list(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in bigrams:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in bigrams]\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(dictionary.token2id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing: tf*idf, systematic pipeline, save document-term matrix at each step to TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "Pouria, can you add here visualizations for the word count after we talk with Haiko about preprocessing texts if we need more cleaning before plotting?\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Understanding the meaning: Similarity of words and documents \n",
    "\n",
    "<div class='alert alert-block alert-danger'>\n",
    "<big><b>Notes from the content document(remove after finalizing it):</b></big>\n",
    "    \n",
    "(we need a large corpus for this; https://ai.googleblog.com/2017/01/a-large-corpus-for-supervised-word.html proposes disambiguation of “stock”: pick the largest pages from https://en.wikipedia.org/wiki/Stock_(disambiguation))\n",
    "Cosine\n",
    "Word2vec (not pretrained embeddings); using gensim (Hovy)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some suggested inspiration:\n",
    "\n",
    "Measurement of Text Similarity A Survey: https://www.mdpi.com/2078-2489/11/9/421\n",
    "\n",
    "https://medium.com/@adriensieg/text-similarities-da019229c894\n",
    "\n",
    "https://newscatcherapi.com/blog/ultimate-guide-to-text-similarity-with-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<big><b>Gizem's note:</b></big>\n",
    "    \n",
    "From this point on, old notebook notes are here that I didn't want to remove in case we would like to reuse some of the information below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfg6soCx1CND"
   },
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "<b>Additional resources</b>\n",
    "\n",
    "#### NLTK (Natural Language Toolkit)\n",
    "\n",
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum. For more details, check out <a href=\"https://www.nltk.org/\">NLTK</a>'s webpage.\n",
    "    \n",
    "#### gensim\n",
    "\n",
    "Gensim is a free open-source Python library for representing documents as semantic vectors, as efficiently (computer-wise) as possible. It was developed for topic modelling, which supports the NLP tasks like Word Embedding, text summarization and many others, such as <a href=\"https://radimrehurek.com/gensim/models/ldamodel.html\">LDA Topic Modeling</a> and <a href=\"https://radimrehurek.com/gensim/models/phrases.html\">Bigrams/Trigrams</a>. For more details, check out  <a href=\"https://radimrehurek.com/gensim/auto_examples/index.html#documentation\">Gensim</a>'s webpage.\n",
    "    \n",
    "#### Hugging Face Transformers\n",
    "\n",
    "Transformers was developed by <a href=\"https://huggingface.co/\">Hugging Face</a> and provides state of the art models. It is an advanced library known for the transformer modules with high-level NLP tasks. Hugging Face is one of the most widely used libraries in NLP community. It provides native support for PyTorch and Tensorflow-based models, increasing its applicability in the deep learning community. <a href=\"https://arxiv.org/abs/1810.04805\">BERT</a>  and <a href=\"https://arxiv.org/abs/1907.11692\">RoBERTa</a> are two of the most valuable models supplied by the Hugging Face library, which is used for machine translation, question/answer activities, and many other applications. \n",
    "\n",
    "Hugging Face pipeline provides a rapid and simple approach to perform a range of NLP operations, and the Hugging Face library also supports GPUs for training. As a result, processing speeds are multiplied by a factor of ten. Check out their <a href=\"https://huggingface.co/docs/transformers/main_classes/pipelines\">Pipelines</a> for what 10+ tasks we can perform as one-liners basically. Their model repository is vast.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Haiko</b></big>\n",
    "\n",
    "At this point it occurrs to me that just providing the commands how things can be done is not enough. We also want to teach how users can proproces their corpus and save intermediate steps like \"corpus.txt\" > \"corpus_stemmed.txt\" > \"corpus_stemmed_nostopwords.txt\" > ...\n",
    "\n",
    "In general, do we need spacy, nltk, and gensim to work along this pipeline? Even if not, we should tell in the session why we introduce all packages.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency analysis\n",
    "\n",
    "<div class='alert-info'>\n",
    "<big><b>Haiko</b></big>\n",
    "\n",
    "Yes, I think it should be here. Hovy has it in the \"text representation\" section. It is then a statistic of the document-term matrix for example.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "<b>Gizem's note:</b>\n",
    "\n",
    "To session 10! Huggingface’s transformers: State-of-the-art NLP\n",
    "Currently, Hugging face is supported by Pytorch and tensorflow 2.0. We can use transformers of Hugging Face to implement Summarization, Text Generation, Language Translation, ChatBot...\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Hovy, D. (2020). Text analysis in Python for social scientists: Discovery and exploration. Cambridge University Press.\n",
    "\n",
    "Hovy, D. (2021). Text Analysis in Python for Social Scientists: Prediction and Classification. Cambridge University Press.\n",
    "\n",
    "Vajjala, S., Majumder, B., Gupta, A., & Surana, H. (2020). Practical natural language processing: a comprehensive guide to building real-world NLP systems. O'Reilly Media.\n",
    "\n",
    "Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
    "\n",
    "Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\n",
    "\n",
    "He, R., & McAuley, J. (2016, April). Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In proceedings of the 25th international conference on world wide web (pp. 507-517).: http://jmcauley.ucsd.edu/data/amazon/index_2014.html\n",
    "\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/natural-language-processing-guide/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Credits\n",
    "\n",
    "[1] Vajjala, S., Majumder, B., Gupta, A., & Surana, H. (2020). Practical natural language processing: a comprehensive guide to building real-world NLP systems. O'Reilly Media. Chapter 1: https://www.oreilly.com/library/view/practical-natural-language/9781492054047/ch01.html\n",
    "\n",
    "POS image credit: https://machinelearningknowledge.ai/tutorial-on-spacy-part-of-speech-pos-tagging/\n",
    "spacy pipeline image credit: https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "<b>Document information</b>\n",
    "\n",
    "Contact and main author: N. Gizem Bacaksizlar Turbic & ..?\n",
    "\n",
    "Contributors: Haiko Lietz & Pouria Mirelmi & ..?\n",
    "\n",
    "Acknowledgements: ...\n",
    "\n",
    "Version date: XX. February 2023\n",
    "\n",
    "License: ...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes to be removed before publication\n",
    "\n",
    "Reviewers: Indira, Olya, or Veronika?\n",
    "\n",
    "Review intro\n",
    "\n",
    "Review and finish red boxes\n",
    "\n",
    "Add insight boxes more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Z0-GgEFeNJbn"
   ],
   "name": "Day 1: Introduction to Jupyter Notebook and Text Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
